Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{MaxKuhnContributionsfromJedWing2017,
author = {{Max Kuhn Contributions from Jed Wing}, Author and Weston, Steve and Williams, Andre and Keefer, Chris and Engelhardt, Allan and Cooper, Tony and Mayer, Zachary and Benesty, Michael and Lescarbeau, Reynald and Ziem, Andrew and Scrucca, Luca and Tang, Yuan and Candan, Can and Hunt, Tyler and {Max Kuhn}, Maintainer},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Max Kuhn Contributions from Jed Wing et al. - 2017 - Package 'caret' Classification and Regression Training Description Misc functions f.pdf:pdf},
title = {{Package 'caret' Classification and Regression Training Description Misc functions for training and plotting classification and regression models}},
url = {https://cran.r-project.org/web/packages/caret/caret.pdf},
year = {2017}
}
@misc{Kottmann2016,
author = {Kottmann, J{\"{o}}rn and Ingersoll, Grant and Drost, Isabel and Kosin, James and Baldridge, Jason and Morton, Thomas and Silva, William and Agerri, Rodrigo and Autayeu, Aliaksandr and Galitsky, Boris and Giaconia, Mark and Teofili, Tommaso and Khuc, Vinh and Beylerian, Anthony and Bouazizi, Mondher and Mattmann, Chris and Mensikova, Anastasija},
title = {{openNLP: Apache OpenNLP Tools Interface}},
url = {https://opennlp.apache.org/},
year = {2016}
}
@article{Sichel1986,
author = {Sichel, H S},
doi = {10.1007/BF01599746},
issn = {1572-9486},
journal = {Czechoslovak Journal of Physics B},
number = {1},
pages = {133--137},
title = {{The GIGP distribution model with applications to physics literature}},
url = {http://dx.doi.org/10.1007/BF01599746},
volume = {36},
year = {1986}
}
@misc{DeCock,
author = {{De Cock}, Dean},
title = {{House Prices: Advanced Regression Techniques | Kaggle}},
url = {https://www.kaggle.com/c/house-prices-advanced-regression-techniques{\#}description https://www.kaggle.com/jimthompson/house-prices-advanced-regression-techniques/ensemble-model-stacked-model-example/notebook},
urldate = {2017-09-16}
}
@article{Trnka2008a,
abstract = {We present the development and tuning of a topic-adapted language model for word prediction, which improves keystroke savings over a comparable baseline. We outline our plans to develop and integrate style adaptations, building on our experience in topic modeling to dynamically tune the model to both topically and stylistically relevant texts.},
author = {Trnka, Keith},
doi = {10.3115/1564154.1564167},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka - 2008 - Adaptive language modeling for word prediction.pdf:pdf},
journal = {HLT-SRWS '08 Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies},
number = {June},
pages = {61--66},
title = {{Adaptive language modeling for word prediction}},
url = {http://dl.acm.org/citation.cfm?id=1564167},
year = {2008}
}
@article{sabanésbové2011,
author = {{Saban{\'{e}}s Bov{\'{e}}}, Daniel and Held, Leonhard},
doi = {10.1214/11-BA615},
journal = {Bayesian Anal.},
number = {3},
pages = {387--410},
publisher = {International Society for Bayesian Analysis},
title = {{Hyper-{\$}g{\$} priors for generalized linear models}},
url = {https://doi.org/10.1214/11-BA615},
volume = {6},
year = {2011}
}
@article{Sundarkantham2011,
abstract = {Language is a unique phenomenon that distinguishes man from other animals. It is our primary method of communication with each other, yet very little is understood about how language is acquired when we are infants. A greater understanding in this area would have the potential to improve man machine communication The problem that is attempted to be solved in this paper is that of programming a computer to play the Shannon Game. To play the Shannon game, one must predict which words are most likely to follow a given segment of English Text. Word Prediction would be most useful for writers with physical disabilities and severe spelling problems. The aim of this paper is to improve on existing results by writing a program that is capable of automatically inferring a grammar from a Natural Language Corpus, and applying this to the Shannon Game. To play the Shannon Game, a stochastic Grammar for an approximation to the target language must be inferred from a text sample, and as the quality of this grammar improves so too does the quality of the predictor that uses the inferred grammar. The proposed algorithm in the paper uses Support Vector Machine to perform the part of speech tagging which produces 97.6{\%} correct predictions.},
author = {Sundarkantham, K and Shalinie, S Mercy and Pushparathi, S.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sundarkantham, Shalinie, Pushparathi - 2011 - Word predictor using natural language grammar induction technique.pdf:pdf},
issn = {18234690},
journal = {Journal of Engineering Science and Technology},
keywords = {K-means clustering,Natural language grammatical inference,Support vector machines},
number = {2},
pages = {204--215},
title = {{Word predictor using natural language grammar induction technique}},
url = {www.jatit.org},
volume = {6},
year = {2011}
}
@article{Kremers,
author = {Kremers, Jerome},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kremers - Unknown - TEXT MESSAGING ON RELATIONAL INTIMACY The Effects of Text Messaging on Relational Intimacy(2).pdf:pdf},
title = {{TEXT MESSAGING ON RELATIONAL INTIMACY The Effects of Text Messaging on Relational Intimacy}}
}
@article{Crystal2009,
abstract = {The article reviews the book “Txtng: the gr8 db8," by David Crystal.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Crystal, David},
doi = {10.1177/0094306110361589k},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crystal et al. - Unknown - Txting The gr8 db8(2).pdf:pdf},
isbn = {019156267X},
issn = {0046208X},
journal = {English in Australia},
number = {1},
pages = {81--83},
pmid = {25246403},
title = {{Txtng: the Gr8 Db8}},
url = {http://books.google.es/books?id=Nxp3PgAACAAJ},
volume = {44},
year = {2009}
}
@article{Matiasek2002a,
abstract = {Communication and information exchange is a vital factor in human society. Communication disorders severely influence the quality of life. Whereas experienced typists will produce some 300 keystrokes per minute, persons with motor impairments achieve only much lower rates. Predictive typing systems for English speaking areas have proven useful and efficient, but for all other European languages there exist no predic-tive typing programs powerful enough to substantially improve the com-munication rate and the IT access for disabled persons. FASTY aims at offering a communication support system significantly increasing typing speed, adaptable to users with different language and strongly varying needs. In this way the large group of non-English-speaking disabled citi-zens will be supported in living a more independent and self determined life.},
author = {Matiasek, Johannes and Baroni, Marco and Trost, Harald},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matiasek, Baroni, Trost - 2002 - FASTY—A multi-lingual approach to text prediction(2).pdf:pdf},
isbn = {3-540-43904-8},
journal = {Computers Helping People with Special {\ldots}},
number = {c},
pages = {243--250},
title = {{FASTY—A multi-lingual approach to text prediction}},
url = {http://link.springer.com/chapter/10.1007/3-540-45491-8{\_}51},
volume = {2398},
year = {2002}
}
@article{Begleiter2004,
abstract = {This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a “decomposed ” CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems. 1.},
author = {Begleiter, Ron and El-Yaniv, Ran and Yona, Golan},
doi = {10.1613/jair.1491},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Begleiter, El-Yaniv, Yona - 2004 - On prediction using variable order Markov models(2).pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {385--421},
title = {{On prediction using variable order Markov models}},
volume = {22},
year = {2004}
}
@book{zipf32selective,
author = {Zipf, G K},
publisher = {Harvard University Press},
title = {{Selective Studies and the Principle of Relative Frequency in Language}},
year = {1932}
}
@article{Biber1993,
abstract = {The present paper addresses a number of issues related to achieving 'representativeness' in linguistic corpus design, including: discussion of what it means to 'represent' a lan-guage, definition of the target population, stratified versus proportional sampling of a language, sampling within texts, and issues relating to the required sample size (number of texts) of a corpus. The paper distinguishes among various ways that linguistic features can be distributed within and across texts; it analyses the distributions of several particular features, and it discusses the implications of these distribu-tions for corpus design. The paper argues that theoretical research should be prior in corpus design, to identify the situational parameters that distinguish among texts in a speech community, and to identify the types of linguistic features that will be analysed in the corpus. These theoretical considerations should be complemented by empirical investigations of linguistic variation in a pilot corpus of texts, as a basis for specific sampling decisions. The actual construction of a corpus would then proceed in cycles: the original design based on theoretical and pilot-study analyses, followed by collection of texts, followed by further empirical investigations of lin-guistic variation and revision of the design.},
author = {Biber, Douglas},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Biber - 1993 - representativeness in corpus design.pdf(2).pdf:pdf},
title = {representativeness in corpus design.pdf},
year = {1993}
}
@inproceedings{Powers1998,
abstract = {Recently I have been intrigued by the reappearance of an old friend, George Kingsley Zipf, in a number of not entirely expected places. The law named for him is ubiquitous, but Zipf did not actually discover the law so much as provide a plausible explanation. Others have proposed modifications to Zipf's Law, and closer examination uncovers systematic deviations from its normative form. We demonstrate how Zipf's analysis can be extended to include some of these phenomena.},
author = {Powers, David M W},
booktitle = {NeMLaP3/CoNLL '98 Proceedings of the Joint Conferences on New Methods in Language Processing and Computational Natural Language Learning},
doi = {10.3115/1603899.1603924},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Powers - 1998 - Applications and Explanations of Zipf's Law.pdf:pdf},
isbn = {0-7258-0634-6},
pages = {151--160},
title = {{Applications and Explanations of Zipf's Law}},
year = {1998}
}
@article{Sichel1986,
author = {Sichel, H},
journal = {Mathematical Scientist},
pages = {45--72},
title = {{Word frequency distributions and type-token characteristics}},
volume = {11},
year = {1986}
}
@article{Wickham2016a,
author = {Wickham, Hadley and Maintainer, ]},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham, Maintainer - 2016 - Package ‘reshape'(2).pdf:pdf},
title = {{Package ‘reshape'}},
url = {http://had.co.nz/reshape},
year = {2016}
}
@article{Box1982,
abstract = {In the analysis of data it is often assumed that observations y,, y,,...,y, are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters 0. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples. 1.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Box, G. E. P. and Cox, D. R.},
doi = {10.2307/2287791},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Box, Cox - 1982 - An Analysis of Transformations Revisited, Rebutted.pdf:pdf},
isbn = {EE000178 00359246 DI993152 99P02493},
issn = {01621459},
journal = {Journal of the American Statistical Association},
number = {377},
pages = {209},
pmid = {16703375},
title = {{An Analysis of Transformations Revisited, Rebutted}},
url = {http://www.econ.uiuc.edu/{~}econ508/Papers/boxcox64.pdf http://www.jstor.org/stable/2287791?origin=crossref},
volume = {77},
year = {1982}
}
@article{Markov1906,
author = {Markov, A},
journal = {Izvestiya Fiziko-matematicheskogo obschestva pri Kazanskom universitete},
number = {tom 15},
pages = {135--156},
title = {{Rasprostranenie zakona bol'shih chisel na velichiny, zavisyaschie drug ot druga}},
volume = {2-ya seriy},
year = {1906}
}
@article{Ghayoomi2008,
author = {Ghayoomi, Masood},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Daroodi - Unknown - A POS-based Word Prediction System for the Persian Language(2).pdf:pdf},
keywords = {pos tagging,statistical language modeling,word prediction},
pages = {5221--5221},
title = {{A POS-Based Word Prediction System for}},
year = {2008}
}
@article{Kelly2012,
author = {Kelly, Lynne and Keaten, James A. and Becker, Bonnie and Cole, Jodi and Littleford, Lea and Rothe, Barrett},
doi = {10.1080/17459435.2012.719203},
issn = {1745-9435},
journal = {Qualitative Research Reports in Communication},
month = {jan},
number = {1},
pages = {1--9},
title = {{“It's the American Lifestyle!”: An Investigation of Text Messaging by College Students}},
url = {http://www.tandfonline.com/doi/abs/10.1080/17459435.2012.719203},
volume = {13},
year = {2012}
}
@misc{Kennon2015,
author = {Kennon, Joshua},
title = {{Blog Demographics 2015 Edition: If Life Were a Game, You All Would Be Champions}},
url = {http://www.joshuakennon.com/blog-demographics-2015-edition-if-life-were-a-game-you-would-be-champions/},
urldate = {2016-12-13},
year = {2015}
}
@article{Communication2010a,
author = {Communication, Alternative and Yarmohammadi, Mahsa A},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Communication, Yarmohammadi - 2010 - Language Modeling and Word Prediction Papers for Today(2).pdf:pdf},
title = {{Language Modeling and Word Prediction Papers for Today}},
year = {2010}
}
@article{Owoputi2013,
abstract = {We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90{\%} to 93{\%} accuracy (more than 3{\%} absolute). Quali- tative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at: http://www.ark.cs.cmu.edu/TweetNLP This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data},
author = {Owoputi, Olutobi and O'Connor, Brendan and Dyer, Chris and Gimpel, Kevin and Schneider, Nathan and Smith, Noah A},
doi = {10.1177/001316446002000104},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Owoputi et al. - 2013 - Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters(3).pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT 2013},
number = {June},
pages = {380--390},
title = {{Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters}},
year = {2013}
}
@techreport{BLS2016,
author = {{Bureau of Labor Statistics}},
title = {{Employed persons by detailed occupation and sex , 2015 annual averages}},
url = {https://www.bls.gov/cps/cpsaat11b.htm},
year = {2016}
}
@article{Even-zohar1995a,
abstract = {The eventual goal of a language model is to accu-rately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics pred-icates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to gener-ate expressive context representations, along with a learning method capable of handling the large num-ber of features generated this way that can, poten-tially, contribute to each prediction. Second, since the number of words "competing" for each predic-tion is large, there is a need to "focus the attention" on a smaller subset of these. We exhibit the contri-bution of a "focus of attention" mechanism to the performance of the word predictor. Finally, we de-scribe a large scale experimental study in which the approach presented is shown to yield significant im-provements in word prediction tasks.},
annote = {Not relevant to current task. Considers features of sentense and context that is not available to the mobile phone prediction problem.},
author = {Even-zohar, Yair},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Even-zohar - 1995 - to Word Prediction(2).pdf:pdf},
pages = {124--131},
title = {{to Word Prediction *}},
year = {1995}
}
@article{Agarwal2007a,
abstract = {The use of digital mobile phones has led to a tremendous increase in communication using SMS. On a phone keypad, multiple words are mapped to same numeric code. We propose a Context Based Word Prediction system for SMS messaging in which context is used to predict the most appropriate word for a given code. We extend this system to allow informal words (short forms for proper English words). The mapping from informal word to its proper English words is done using Double Metaphone Encoding based on their phonetic similarity. The results show 31{\%} improvement over the traditional frequency based word estimation. Introduction The growth of wireless technology has provided us with many new ways of communication such as SMS (Short Message Service). SMS messaging can also be used to interact with automated systems or participating in contests. With tremendous increase in Mobile Text Messaging, there is a need for an efficient text input system. With limited keys on the mobile phone, multiple letters are mapped to same number (8 keys, 2 to 9, for 26 alphabets). The many to one mapping of alphabets to numbers gives us same numeric code for multiple words.},
author = {Agarwal, Sachin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal - 2007 - Context based word prediction for texting language.pdf:pdf},
journal = {Large Scale Semantic Access to Content (Text,},
pages = {360--368},
title = {{Context based word prediction for texting language}},
url = {http://dl.acm.org/citation.cfm?id=1931426},
year = {2007}
}
@article{10.2307/2334208,
author = {Tiao, George C and Zellner, Arnold},
issn = {00063444},
journal = {Biometrika},
number = {1/2},
pages = {219--230},
publisher = {[Oxford University Press, Biometrika Trust]},
title = {{Bayes's Theorem and the Use of Prior Knowledge in Regression Analysis}},
url = {http://www.jstor.org/stable/2334208},
volume = {51},
year = {1964}
}
@incollection{Biber1993a,
address = {New York},
author = {Biber, Douglas and Finegan, E.},
booktitle = {Sociolinguistic Perspectives on Register},
chapter = {2},
pages = {31--58},
publisher = {Oxford University Press},
title = {{An Analytical Framework for Register Studies}},
year = {1993}
}
@article{Gh2001,
author = {Gh, Derudwrulr and Gh, Hfqrorjtdv and Gh, S W R and Gh, Rolwpfqlfd and Navarro, Santiago Aguilera},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gh et al. - 2001 - Contribution to Word Prediction in Spanish and its Integration in Technial Aids for People with Physical Disabilities.pdf:pdf},
title = {{Contribution to Word Prediction in Spanish and its Integration in Technial Aids for People with Physical Disabilities}},
url = {http://www.geintra-uah.org/system/files/PhDThesisEnglishSummary-SiraPalazuelos.pdf},
year = {2001}
}
@article{Pauls2011,
abstract = {N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25{\%} of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300{\%}.},
author = {Pauls, Adam and Klein, Dan},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pauls, Klein - 2011 - Faster and Smaller N-Gram Language Models.pdf:pdf},
isbn = {978-1-932432-87-9},
journal = {Computational Linguistics},
pages = {258--267},
title = {{Faster and Smaller N-Gram Language Models}},
url = {http://www.aclweb.org/anthology/P11-1027},
year = {2011}
}
@article{Jelinek1999,
author = {Jelinek, Frederick and Helba, Ciprian},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jelinek, Helba - 1999 - Putting Language into Language Modeling(2).pdf:pdf},
number = {June 1999},
title = {{Putting Language into Language Modeling}},
volume = {1},
year = {1999}
}
@article{Piantadosi2014,
abstract = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf 's law. This paper first shows that human language has highly complex, reliable structure in the frequency distribution over and above this classic law, though prior data visualization methods obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf's law, and are then used to evaluate many of the theoretical explanations of Zipf's law in language. No prior account straightforwardly explains all the basic facts, nor is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf's law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
author = {Piantadosi, Steven T},
doi = {10.3758/s13423-014-0585-6},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Piantadosi - 2015 - Zipf's word frequency law in natural language a critical review and future directions(2).pdf:pdf},
isbn = {1342301405856},
issn = {1531-5320},
journal = {Psychonomic bulletin $\backslash${\&} review},
keywords = {frequency f,frequent word has a,known as zipf,language,obeying a power law,r,s law,statistics,that scales according to,the r th most,zipf},
month = {oct},
number = {5},
pages = {1112----1130},
pmid = {24664880},
title = {{Zipf's word frequency law in natural language: a critical review and future directions}},
url = {http://link.springer.com/10.3758/s13423-014-0585-6 http://www.ncbi.nlm.nih.gov/pubmed/24664880 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4176592},
volume = {21},
year = {2014}
}
@article{Wickham2015a,
abstract = {A Grammar of Data Manipulation},
author = {Wickham, Hadley and Francois, Romain and RStudio},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham, Francois, RStudio - 2015 - Package ‘dplyr'.pdf:pdf},
pages = {71},
title = {{Package ‘dplyr'}},
url = {https://cran.r-project.org/web/packages/dplyr/dplyr.pdf},
year = {2015}
}
@article{Carlberger1997,
abstract = {Word prediction is the problem of guessing which words are likely to follow a given segment of text. A computer program performing word prediction can be an important writing aid for physically or linguistically handicapped people. Using a word predictor ensures correct spelling and saves keystrokes and effort for the user. The goal of this project was to design and implement a word predictor for Swedish that would suggest better words, and thus save more keystrokes, than any other word predictor available on the market. The constructed program uses a probabilistic language model based on the well-established ideas of the trigram predictor developed at IBM. By using such a model, the program can be easily modified to work with languages other than Swedish. In tests, the program has been shown to save more than 45 percent of the keystrokes for the user. The report focuses on the technical aspects of designing an efficient algorithm and optimizing it to save as many keystrokes as possible. Design och implementering av ett probabilistiskt ordprediktionsprogram Sammanfattning Ordprediktion {\"{a}}r att gissa vilket ord som sannolikt f{\"{o}}ljer en given sekvens av ord. Ett program som utf{\"{o}}r ordprediktion kan vara ett bra hj{\"{a}}lpmedel f{\"{o}}r motoriskt och spr{\aa}kligt handikappade vid datorarbete och kommunikation. Anv{\"{a}}ndningen av ett ordprediktionsprogram underl{\"{a}}ttar stavning och besparar knapptryckningar och d{\"{a}}rmed anstr{\"{a}}ngning f{\"{o}}r anv{\"{a}}ndaren. M{\aa}let med projektet var att utforma och implementera ett ordprediktionsprogram f{\"{o}}r svenska som f{\"{o}}resl{\aa}r b{\"{a}}ttre ord, och d{\"{a}}rmed besparar anv{\"{a}}ndaren fler knapp-tryckningar, {\"{a}}n n{\aa}got annat ordprediktionsprogram p{\aa} marknaden. Det konstruerade programmet anv{\"{a}}nder en probabilistisk spr{\aa}kmodell baserad p{\aa} den v{\"{a}}lk{\"{a}}nda trigram-prediktorn som utvecklats vid IBM. Detta val av spr{\aa}kmodell medf{\"{o}}r att programmet enkelt kan modifieras f{\"{o}}r att fungera f{\"{o}}r andra spr{\aa}k {\"{a}}n svenska. Utf{\"{o}}rda tester visar att programmet kan spara mer {\"{a}}n 45 procent av knapptryckningarna f{\"{o}}r anv{\"{a}}ndaren. I denna rapport har tyngdpunkten lagts p{\aa} de tekniska aspekterna av att skapa en effektiv algoritm och optimering av denna.},
author = {Carlberger, J.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlberger - 1997 - Design and Implementation of a Probabilistic Word Prediciton Program(3).pdf:pdf},
journal = {Language},
title = {{Design and Implementation of a Probabilistic Word Prediciton Program}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.2073},
year = {1997}
}
@misc{Beal2016,
author = {Beal, Vangie},
title = {{Huge List of Text Message {\&} Chat Abbreviations - Webopedia}},
url = {http://www.webopedia.com/quick{\_}ref/textmessageabbreviations.asp},
year = {2016}
}
@article{Nalavade,
abstract = {Majority of the currently used predictive text entry systems (like T9 for lower end mobile phones) do not provide word prediction. In these systems, the average number of key-taps per word is high resulting in higher typing efforts on the part of the user. At times, T9 provides options (words) that may not fit into the context of the message, are wrong grammatically and are not valid English words. Also, T9 is slower to adapt to usage patterns. PreText predicts the word that the user is typing with the help of grammar rules for the English language, making word prediction more precise, reducing the number of key taps required, saving the user's time and achieving an optimisation over the existing systems. It also adapts to the user's usage pattern with the help of a frequency model. The metric used here to evaluate the performance of text entry systems is KSPC [1] (keystrokes per character). The KSPC was found to be 0.7360 for PreText, providing an average improvement of 26.91{\%} over T9 which has a KSPC of 1.023 [1].},
author = {Nalavade, Deepti and Mahule, Tushar and Ketkar, Harshvardhan},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nalavade, Mahule, Ketkar - Unknown - PreText A Predictive Text Entry System for Mobile Phones(2).pdf:pdf},
isbn = {9789881701244},
journal = {Proceedings of the World Congress on {\ldots}},
keywords = {KSPC,adaptive,predictive,text entry},
pages = {2--7},
title = {{PreText: A Predictive Text Entry System for Mobile Phones}},
url = {http://www.iaeng.org/publication/WCE2008/WCE2008{\_}pp1739-1744.pdf},
volume = {III},
year = {2008}
}
@article{Ney1994a,
abstract = {We study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a German database and an English database},
author = {Ney, Hermann and Essen, Ute and Kneser, Reinhard},
doi = {10.1006/csla.1994.1001},
issn = {08852308},
journal = {Computer Speech {\&} Language},
month = {jan},
number = {1},
pages = {1--38},
title = {{On Structuring Probabilistic Dependences in Stochastic Language Modelling}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0885230884710011},
volume = {8},
year = {1994}
}
@article{Taylor2010,
abstract = {The transformative trends of the past 50 years that have led to a sharp decline in marriage and a rise of new family forms have been shaped by attitudes and behaviors that differ by class, age and race, according to a new Pew Research Center nationwide survey complemented by an analysis of demographic and economic data from the U.S. Census Bureau. A new ―marriage gap‖ in the United States is increasingly aligned with a growing income gap. Marriage, while declining among all groups, remains the norm for adults with a college education and good income but is now markedly less prevalent among those on the lower rungs of the socio-economic ladder. The survey finds that those in this less-advantaged group are as likely as others to want to marry, but they place a higher premium on economic security as a condition for marriage. This is a bar that many may not meet. The survey also finds striking differences by generation. In 1960, two-thirds (68{\%}) of all twenty-somethings were married. In 2008, just 26{\%} were. How many of today's youth will eventually marry is an open question. For now, the survey finds that the young are much more inclined than their elders to view cohabitation without marriage and other new family forms — such as same sex marriage and interracial marriage — in a positive light. Even as marriage shrinks, family— in all its emerging varieties — remains resilient. The survey finds that Americans have an expansive definition of what constitutes a family. And the vast majority of adults consider their own family to be the most important, most satisfying element of their lives.},
author = {Taylor, Paul},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - The Decline of Marriage And Rise of New Families FOR RELEASE NOVEMBER 18, 2010 Survey conducted in association with.pdf:pdf},
journal = {Pew Research Center},
title = {{The Decline of Marriage And Rise of New Families}},
url = {http://pewsocialtrends.org},
year = {2010}
}
@article{Simon2003,
author = {Simon, G E and VonKorff, M and Barlow, W},
journal = {Biological Psychiatry},
pages = {216--226},
title = {{Health care costs of primary care patients with recognized depression}},
volume = {54},
year = {2003}
}
@book{Harper2005,
abstract = {SMS or Text is one of the most popular forms of messaging. Yet, despite its immense popularity, SMS has remained unexamined by science. Not only that, but the commercial organisations, who have been forced to offer SMS by a demanding public, have had very little idea why it has been successful. Indeed, they have, until very recently, planned to replace SMS with other messaging services such as MMS. This book is the first to bring together scientific studies into the values that ‘texting' provides, examining both cultural variation in countries as different as the Philippines and Germany, as well as the differences between SMS and other communications channels like Instant Messaging and the traditional letter. It presents usability and design research which explores how SMS will evolve and what is likely to be the pattern of person-to-person messaging in the future. In short, Inside Text is a fundamental resource for anyone interested in mobile communications at the start of the 21st Century The book will be of interest to anyone in the CHI, CSCW and mobile communications research areas, as well as sociologists, anthropologists, communications scientists and policy makers.},
author = {Harper, R and Palen, L and Taylor, A},
editor = {Harper, Richard and Palen, Leysia and Taylor, Alex},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harper, Palen, Taylor - 2005 - The Inside Text Social, Cultural and Design Perspectives on SMS (The Computer Supported Cooperative Work.pdf:pdf},
isbn = {1402030606},
keywords = {no-tag},
publisher = {Springer},
title = {{The Inside Text : Social, Cultural and Design Perspectives on SMS (The Computer Supported Cooperative Work Series)}},
url = {http://www.amazon.fr/exec/obidos/ASIN/1402030592/citeulike04-21},
year = {2005}
}
@article{Raftery1994,
abstract = {We consider the problems of variable selection and accounting for model uncertainty in linear regression models. Conditioning on a single selected model ignores model uncertainty, and thus leads to the underestimation of uncertainty when making inferences about quantities of interest. The complete Bayesian solution to this problem involves averaging over all possible models when making inferences about quantities of interest. This approach is often not practical. In this paper we offer two alternative approaches. First we describe a Bayesian model selection algorithm called "Occam's "Window" which involves averaging over a reduced set of models. Second, we describe a Markov chain Monte Carlo approach which directly approximates the exact solution. Both these model averaging procedures provide better predictive performance than any single model which might reasonably have been selected. In the extreme case where there are many candidate predictors but there is no relationship between any of them and the response, standard variable selection procedures often choose some subset of variables that yields a high R{\^{A}}² and a highly significant overall F value. We refer to this unfortunate phenomenon as "Freedman's Paradox" (Freedman, 1983). In this situation, Occam's {\{}vVindow{\}} usually indicates the null model as the only one to be considered, or else a small number of models including the null model, thus largely resolving the paradox.},
author = {Raftery, Adrian and Madigan, David and Hoeting, Jennifer},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Madigan, Raftery - 1994 - Model Selection and Accounting for Model Uncertainty in Graphical Models Using Occam's Window.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {bayes factor,composition,freedman,markov chain monte carlo,model,model uncertainty,occam,posterior model probability,s paradox,s window},
number = {428},
pages = {1535--1546},
title = {{Model Selection and Accounting for Model Uncertainty in Linear Regression Models}},
url = {https://www.stat.washington.edu/raftery/Research/PDF/madigan1994.pdf},
volume = {89},
year = {1994}
}
@article{Carlberger1997,
abstract = {Word prediction is the problem of guessing which words are likely to follow a given segment of text. A computer program performing word prediction can be an important writing aid for physically or linguistically handicapped people. Using a word predictor ensures correct spelling and saves keystrokes and effort for the user. The goal of this project was to design and implement a word predictor for Swedish that would suggest better words, and thus save more keystrokes, than any other word predictor available on the market. The constructed program uses a probabilistic language model based on the well-established ideas of the trigram predictor developed at IBM. By using such a model, the program can be easily modified to work with languages other than Swedish. In tests, the program has been shown to save more than 45 percent of the keystrokes for the user. The report focuses on the technical aspects of designing an efficient algorithm and optimizing it to save as many keystrokes as possible. Design och implementering av ett probabilistiskt ordprediktionsprogram Sammanfattning Ordprediktion {\"{a}}r att gissa vilket ord som sannolikt f{\"{o}}ljer en given sekvens av ord. Ett program som utf{\"{o}}r ordprediktion kan vara ett bra hj{\"{a}}lpmedel f{\"{o}}r motoriskt och spr{\aa}kligt handikappade vid datorarbete och kommunikation. Anv{\"{a}}ndningen av ett ordprediktionsprogram underl{\"{a}}ttar stavning och besparar knapptryckningar och d{\"{a}}rmed anstr{\"{a}}ngning f{\"{o}}r anv{\"{a}}ndaren. M{\aa}let med projektet var att utforma och implementera ett ordprediktionsprogram f{\"{o}}r svenska som f{\"{o}}resl{\aa}r b{\"{a}}ttre ord, och d{\"{a}}rmed besparar anv{\"{a}}ndaren fler knapp-tryckningar, {\"{a}}n n{\aa}got annat ordprediktionsprogram p{\aa} marknaden. Det konstruerade programmet anv{\"{a}}nder en probabilistisk spr{\aa}kmodell baserad p{\aa} den v{\"{a}}lk{\"{a}}nda trigram-prediktorn som utvecklats vid IBM. Detta val av spr{\aa}kmodell medf{\"{o}}r att programmet enkelt kan modifieras f{\"{o}}r att fungera f{\"{o}}r andra spr{\aa}k {\"{a}}n svenska. Utf{\"{o}}rda tester visar att programmet kan spara mer {\"{a}}n 45 procent av knapptryckningarna f{\"{o}}r anv{\"{a}}ndaren. I denna rapport har tyngdpunkten lagts p{\aa} de tekniska aspekterna av att skapa en effektiv algoritm och optimering av denna.},
author = {Carlberger, J.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlberger - 1997 - Design and Implementation of a Probabilistic Word Prediciton Program(3).pdf:pdf},
journal = {Language},
title = {{Design and Implementation of a Probabilistic Word Prediciton Program}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.2073},
year = {1997}
}
@article{Rastrow2012,
abstract = {Automatic Speech Recognition (ASR) and Statistical Machine Translation (SMT), among other natural language processing applications, rely on a language model (LM) to provide a strong linguistic prior over word sequences of the often prohibitively large and complex hypothesis space of these systems. The language models deployed in most state-of-the-art ASR and SMT systems are n-gram models. Several statistical frameworks have been proposed to build more complex models and " put (the syntactic structure of) language back into language modeling. " Yet, n-gram models, despite being linguistically nave, are still favored, because estimating them from text is well understood, they are computationally efficient, and integrating them into ASR and SMT systems is straightforward. This dissertation proposes novel algorithms and techniques that make it practical to estimate and apply more complex language models in ASR and SMT tasks, in particular syntactic modes for speech recognition. While yielding significantly better performance than n-gram models, the syntactic structured language models (SLM) can not be efficiently trained on a large amount of text data due to the impractical size of the resulting model. A general information-ii ABSTRACT theoretic pruning scheme is proposed to significantly reduce the size of the SLM while},
author = {Rastrow, Ariya},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rastrow - 2012 - Practical and Efficient Incorporation of Syntactic Features into Statistical Language Models(2).pdf:pdf},
title = {{Practical and Efficient Incorporation of Syntactic Features into Statistical Language Models}},
url = {https://www.cs.jhu.edu/{~}mdredze/publications/ariya{\_}rastrow{\_}thesis.pdf},
volume = {Ph.D.},
year = {2012}
}
@inproceedings{Katon2002,
abstract = {Due to the aging of the U.S. population, healthcare providers will be called upon to diagnose and treat patients with chronic medical illness with increasing frequency. This paper will review the epidemiology of depression in patients with chronic illness and the impact of comorbid depression on increased use of medical resources and costs, amplifica- tion of physical symptoms, additive functional impairment, decreased ability to adhere to medications, important life- style changes (i.e., increasing exercise, changing dietary patterns, quitting smoking), and increased mortality},
author = {Katon, Wayne and Ciechanowski, Paul},
booktitle = {Journal of Psychosomatic Research},
doi = {10.1016/S0022-3999(02)00313-6},
isbn = {0022-3999},
issn = {00223999},
month = {oct},
number = {4},
pages = {859--863},
pmid = {12377294},
title = {{Impact of major depression on chronic medical illness}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022399902003136},
volume = {53},
year = {2002}
}
@misc{RStudioTeam2016,
author = {{Rstudio Team}},
booktitle = {RStudio},
title = {{RStudio – Open source and enterprise-ready professional software for R}},
url = {https://www.rstudio.com/},
year = {2016}
}
@book{Simpson1989,
abstract = {2nd ed. / prepared by J.A. Simpson and E.S.C. Weiner. Provides definitions of approximately 290,500 English words, arranged alphabetically in twenty volumes, with cross-references, etymologies, and pronunciation keys, and includes a bibliography. v. 1. A-Bazouki -- v. 2. B.B.C.-Chalypsography -- v. 3. Cham-Creeky -- v. 4. Creel-Duzepere -- v. 5. Dvandva-Follis -- v. 6. Follow-Haswed -- v. 7. Hat-Intervacuum -- v. 8. Interval-Looie -- v. 9. Look-Mouke -- v. 10. Moul-Ovum -- v. 11. Ow-Poisant -- v. 12. Poise-Quelt -- v. 13. Quemadero-Roaver -- v. 14. Rob-Sequyle -- v. 15. Ser-Soosy -- v. 16. Soot-Styx -- v. 17. Su-Thrivingly -- v. 18. Thro-Unelucidated -- v. 19. Unemancipated-Wau-wau -- v. 20. Wave-Zyxt. Bibliography.},
author = {Simpson, J. A. and Weiner, E. S. C. and {Oxford University Press.}},
edition = {2},
isbn = {0198611862},
publisher = {Clarendon Press},
title = {{The Oxford English Dictionary.}},
year = {1989}
}
@misc{RstudioTeam2016a,
author = {{RStudio and Inc.}},
publisher = {github.com/rstudio/rmarkdown},
title = {{rmarkdown: R Markdown Document Conversion}},
url = {http://rmarkdown.rstudio.com/},
year = {2016}
}
@misc{NORCa,
abstract = {General Social Survey(GSS) monitors societal change/ opinion.},
author = {of Chicago, NORC at the University},
title = {{GSS Data Explorer | NORC at the University of Chicago}},
url = {https://gssdataexplorer.norc.org/pages/show?page=gss{\%}2Fabout},
urldate = {2017-09-29}
}
@article{Leamer1978,
abstract = {Section 4.3 p. 100-110, Testing a Point-Null Hypothesis Against$\backslash$na Composite Alternative. 4.4 Weighted Likelihoods: conjugate$\backslash$npriors. Bayes factors. Diffuse priors.},
author = {Leamer, Edward E},
doi = {doi: 10.2307/2287437},
isbn = {0471015202},
issn = {00222437},
journal = {SERBIULA (sistema Librum 2.0)},
pages = {370},
pmid = {3126},
title = {{Specification Searches: Ad Hoc Inference with Nonexperimental Data}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Specification+Searches{\#}7{\%}5Cnhttp://www.anderson.ucla.edu/faculty/edward.leamer/books/specification{\_}searches/specification{\_}searches.htm},
year = {1978}
}
@article{Eisenstein2015,
abstract = {Social media features a wide range of nonstandard spellings, many of which appear inspired by phonological variation. However, the nature of the connection between variation across the spoken and written modalities remains poorly understood. Are phono-logical variables transferred to writing on the level of graphemes, or is the larger system of contextual patterning also transferred? This paper considers orthographic coda deletions corresponding to the phonological variables of (ing) and (t,d). In both cases, orthography mirrors speech: reduction of the -ing suffix depends on the word's syntactic category, and reduction of the -t,-d suffix depends on the succeeding phonological context. These spellings are more frequently used in informal conversational contexts, and in areas with high propor-tions of African Americans, again mirroring the patterning of the associated phonological variables. This suggests a deep connection between variation in the two modalities, necessi-tating a new account of the production of cross-modal variation. (150 words)},
author = {Eisenstein, Jacob},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eisenstein - 2015 - Systematic patterning in phonologically-motivated orthographic variation(2).pdf:pdf},
keywords = {computer-mediated communication Word count 7687,orthography,social media,variation},
title = {{Systematic patterning in phonologically-motivated orthographic variation *}},
year = {2015}
}
@misc{TheMathworksInc.2016,
abstract = {The MATLAB platform is optimized for solving engineering and scientific problems. The matrix-based MATLAB language is the world's most natural way to express computational mathematics.},
author = {{The Mathworks Inc.}},
booktitle = {www.mathworks.com/products/matlab},
doi = {2016-11-26},
title = {{MATLAB - MathWorks}},
url = {https://www.mathworks.com/products/matlab.html http://www.mathworks.com/products/matlab/},
urldate = {2017-09-16},
year = {2016}
}
@article{Bahl1983,
abstract = {Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of statistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them.},
author = {Bahl, Lalit R and Jelinek, Frederick and Mercer, Robert L},
doi = {10.1109/TPAMI.1983.4767370},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bahl, Jelinek, Mercer - 1983 - A maximum likelihood approach to continuous speech recognition(2).pdf:pdf},
isbn = {1558601244},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Index Terms-Markov models,maximum likelihood,parameter esti-mation,speech recognition,statistical models},
number = {2},
pages = {179--190},
pmid = {21869099},
title = {{A maximum likelihood approach to continuous speech recognition.}},
url = {https://www.cse.iitb.ac.in/{~}pb/cs626-2013/word-alignment/jelineck-speech-1983.pdf},
volume = {5},
year = {1983}
}
@article{Gimpel2011,
abstract = {We address the problem of part-of-speech tagging for English data from the popular micro-blogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90{\%} accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.},
author = {Gimpel, Kevin and Schneider, Nathan and O'Connor, Brendan and Das, Dipanjan and Mills, Daniel and Eisenstein, Jacob and Heilman, Michael and Yogatama, Dani and Flanigan, Jeffrey and Smith, Noah A},
doi = {10.1.1.206.3224},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gimpel et al. - 2011 - Part-of-Speech Tagging for Twitter Annotation, Features, and Experiments(2).pdf:pdf},
isbn = {978-1-932432-88-6},
journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Shortpapers},
number = {2},
pages = {42--47},
title = {{Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments}},
year = {2011}
}
@misc{Bekiempis2014,
author = {Bekiempis, Victoria},
booktitle = {Newsweek},
title = {{Nearly 1 in 5 Americans Sufers from Mental Illness Each Year}},
url = {http://www.newsweek.com/nearly-1-5-americans-suffer-mental-illness-each-year-230608},
year = {2014}
}
@misc{Johnson2016,
author = {Johnson, Bryan},
booktitle = {TechCrunch},
title = {{The combination of human and artificial intelligence will define humanity's future | TechCrunch}},
url = {https://techcrunch.com/2016/10/12/the-combination-of-human-and-artificial-intelligence-will-define-humanitys-future/},
urldate = {2016-12-31},
year = {2016}
}
@article{Bickel2005,
abstract = {We explore the benefit that users in several application areas can$\backslash$nexperience from a "tab-complete" editing assistance function. We$\backslash$ndevelop an evaluation metric and adapt N-gram language models to$\backslash$nthe problem of predicting the subsequent words, given an initial$\backslash$ntext fragment. Using an instance-based method as baseline, we empirically$\backslash$nstudy the predictability of call-center emails, personal emails,$\backslash$nweather reports, and cooking recipes.},
author = {Bickel, Steffen and Haider, Peter and Scheffer, Tobias},
doi = {10.3115/1220575.1220600},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bickel, Haider, Scheffer - 2005 - Predicting sentences using N-gram language models.pdf:pdf},
journal = {Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing},
pages = {193--200},
title = {{Predicting sentences using N-gram language models}},
url = {http://dx.doi.org/10.3115/1220575.1220600},
year = {2005}
}
@article{Carmignani2006,
author = {Carmignani, Nicola},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carmignani - 2006 - Predicting Words and Sentences using Statistical Models(3).pdf:pdf},
title = {{Predicting Words and Sentences using Statistical Models}},
year = {2006}
}
@article{Johnson1944a,
author = {Johnson, W},
number = {2},
pages = {1--15},
title = {{Studies in Language Behavior: I. A program of research}},
volume = {56},
year = {1944}
}
@article{Vergyri2004,
abstract = {Language modeling is a difficult problem for languages with rich morphology. In this paper we investigate the use of morphology-based language models at different stages in a speech recognition system for conversational Arabic. Class-based and single-stream factored language models using morphological word representations are applied within an N-best list rescoring framework. In addition, we explore the use of factored language models in first-pass recognition, which is facilitated by two novel procedures: the data-driven optimization of a multi-stream language model structure, and the conversion of a factored language model to a standard word-based model. We evaluate these techniques on a large-vocabulary recognition task and demonstrate that they lead to perplexity and word error rate reductions.},
author = {Vergyri, Dimitra and Kirchhoff, Katrin and Duh, Kevin and Stolcke, Andreas and Park, Menlo},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vergyri et al. - 2004 - Morphology-Based Language Modeling for Arabic Speech Recognition(2).pdf:pdf},
journal = {Interspeech},
keywords = {Arabic,language modeling,morphology,speech recognition},
pages = {4--7},
title = {{Morphology-Based Language Modeling for Arabic Speech Recognition}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.4042{\&}rep=rep1{\&}type=pdf http://academic.research.microsoft.com/Publication/2282409/morphology-based-language-modeling-for-arabic-speech-recognition},
year = {2004}
}
@article{Bilmes2003,
abstract = {We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words. GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed. These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit. This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles. Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams. In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.},
author = {Bilmes, Jeff A and Kirchhoff, Katrin},
doi = {10.3115/1073483.1073485},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bilmes, Kirchhoff - 2003 - Factored Language Models and Generalized Parallel Backoff(2).pdf:pdf},
journal = {Naacl-2003},
pages = {4--6},
title = {{Factored Language Models and Generalized Parallel Backoff}},
url = {http://www.aclweb.org/anthology/N03-2002 http://portal.acm.org/citation.cfm?doid=1073483.1073485{\%}5Cnhttp://acl.ldc.upenn.edu/N/N03/N03-2002.pdf},
volume = {2},
year = {2003}
}
@article{Wandmacher2008a,
abstract = {Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4-gram baseline, and most of them to a simple cache model as well.},
archivePrefix = {arXiv},
arxivId = {0801.4716},
author = {Wandmacher, Tonio and Antoine, Jean-Yves},
eprint = {0801.4716},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wandmacher, Antoine - 2008 - Methods to integrate a language model with semantic information for a word prediction component(2).pdf:pdf;:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wandmacher, Antoine - Unknown - Methods to integrate a language model with semantic information for a word prediction component(3).pdf:pdf},
journal = {Computational Linguistics},
number = {June},
pages = {10},
title = {{Methods to integrate a language model with semantic information for a word prediction component}},
url = {http://arxiv.org/abs/0801.4716 https://arxiv.org/ftp/arxiv/papers/0801/0801.4716.pdf},
year = {2008}
}
@incollection{Carroll1967,
address = {Providence},
author = {Carroll, J.B.},
booktitle = {Computational Analysis of Present-Day American English},
editor = {Kucera, H and Francis, W.N.},
pages = {406--424},
publisher = {Brown University Press},
title = {{On sampling from a lognormal model of frequency distribution}},
year = {1967}
}
@misc{George1993,
author = {George, Edward I and Mcculloch, Robert E},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.1993.10476353},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Variable Selection via Gibbs Sampling.pdf:pdf},
issn = {0162-1459},
pages = {881--889},
title = {{Variable Selection Via Gibbs Sampling}},
url = {https://www2.stat.duke.edu/courses/Spring06/sta376/Support/RegressionETC/george+mcculloch.jasa93.pdf},
volume = {88},
year = {1993}
}
@article{Lennox1993,
abstract = {Clinical and epidemiologic evidence suggests that alcoholism complicated by concurrent or a lifetime history of depression is slower to remit and more prone to relapse than uncomplicated alcoholism. Consequently, alcoholics with a history of depressive illness, on average, are likely to use more health care and to have higher treatment costs than those without depression complications. This article contrasts evidence of the suitability of three models for predicting the impact of depression on an alcoholic's health-care use: a null model (assuming no differences) a cumulative-effect model (arguing for a linear increase associated with comorbid depression), and a synergistic model (wherein alcoholism complicated with depression is qualitatively as well as quantitatively different than uncomplicated alcoholism). To test these models, health-care costs and utilization of 491 ``pure'' alcoholics (those with no history of depression diagnosis) and 199 depression-complicated alcoholics, who received alcohol treatment while enrolled in a self-insured health-care program of a major U.S. manufacturing company, were compared. Results are discussed in terms of the implications for cost containment and the likelihood of relapse among the depression-complicated alcoholism group.},
author = {Lennox, Richard D and Scott-Lennox, Jane A and Bohlig, E Michael},
doi = {10.1007/BF02519238},
issn = {1556-3308},
journal = {The journal of mental health administration},
number = {2},
pages = {138--152},
title = {{The cost of depression-complicated alcoholism: Health-Care utilization and treatment effectiveness}},
url = {http://dx.doi.org/10.1007/BF02519238},
volume = {20},
year = {1993}
}
@article{Francis1979,
author = {Francis, W N and Kucera, H},
journal = {Letters to the Editor},
number = {2},
pages = {7},
title = {{Brown corpus manual}},
url = {http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM},
volume = {5},
year = {1979}
}
@article{Kneser1995,
abstract = {In stochastic language modeling, backing-off is a widely used$\backslash$nmethod to cope with the sparse data problem. In case of unseen events$\backslash$nthis method backs off to a less specific distribution. In this paper we$\backslash$npropose to use distributions which are especially optimized for the task$\backslash$nof backing-off. Two different theoretical derivations lead to$\backslash$ndistributions which are quite different from the probability$\backslash$ndistributions that are usually used for backing-off. Experiments show an$\backslash$nimprovement of about 10{\%} in terms of perplexity and 5{\%} in terms of word$\backslash$nerror rate},
author = {Kneser, R. and Ney, H.},
doi = {10.1109/ICASSP.1995.479394},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kneser, Ney - 1995 - Improved backing-off for M-gram language modeling.pdf:pdf},
isbn = {0-7803-2431-5},
issn = {1520-6149},
journal = {1995 International Conference on Acoustics, Speech, and Signal Processing},
pages = {181--184},
title = {{Improved backing-off for M-gram language modeling}},
volume = {1},
year = {1995}
}
@article{Sundarkantham2011a,
abstract = {Language is a unique phenomenon that distinguishes man from other animals. It is our primary method of communication with each other, yet very little is understood about how language is acquired when we are infants. A greater understanding in this area would have the potential to improve man machine communication The problem that is attempted to be solved in this paper is that of programming a computer to play the Shannon Game. To play the Shannon game, one must predict which words are most likely to follow a given segment of English Text. Word Prediction would be most useful for writers with physical disabilities and severe spelling problems. The aim of this paper is to improve on existing results by writing a program that is capable of automatically inferring a grammar from a Natural Language Corpus, and applying this to the Shannon Game. To play the Shannon Game, a stochastic Grammar for an approximation to the target language must be inferred from a text sample, and as the quality of this grammar improves so too does the quality of the predictor that uses the inferred grammar. The proposed algorithm in the paper uses Support Vector Machine to perform the part of speech tagging which produces 97.6{\%} correct predictions.},
author = {Sundarkantham, K and Shalinie, S Mercy and Pushparathi, S.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sundarkantham, Shalinie, Pushparathi - 2011 - Word predictor using natural language grammar induction technique.pdf:pdf},
issn = {18234690},
journal = {Journal of Engineering Science and Technology},
keywords = {K-means clustering,Natural language grammatical inference,Support vector machines},
number = {2},
pages = {204--215},
title = {{Word predictor using natural language grammar induction technique}},
url = {www.jatit.org},
volume = {6},
year = {2011}
}
@article{Wild2015,
abstract = {The basic idea of latent semantic analysis (LSA) is, that text do have a higher order (=latent semantic) structure which, however, is obscured by word usage (e.g. through the use of synonyms or polysemy). By using conceptual indices that are derived statistically via a truncated singular value decomposition (a two-mode factor analysis) over a given document-term matrix, this variability problem can be overcome.},
author = {Wild, Fridolin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wild - 2015 - Package ‘ lsa '.pdf:pdf},
pages = {1--11},
title = {{Package ‘ lsa '}},
url = {https://cran.r-project.org/web/packages/lsa/lsa.pdf},
year = {2015}
}
@phdthesis{McCarthy2005,
author = {McCarthy, Philip M},
title = {{An assessment of the range and usefulness of lexical diversity measures and the potential of the measure of textual, lexical diversity.}},
year = {2005}
}
@incollection{JelMer80,
address = {Amsterdam},
author = {Jelinek, Fred and Mercer, Robert L},
booktitle = {Proceedings, Workshop on Pattern Recognition in Practice},
editor = {Gelsema, Edzard S and Kanal, Laveen N},
keywords = {2000 book nlp},
pages = {381--397},
publisher = {North Holland},
title = {{Interpolated estimation of {\{}M{\}}arkov source parameters from sparse data}},
year = {1980}
}
@article{Bernicot,
abstract = {The purpose of this research was to gain insights into SMS communication among French-speaking adolescents. We analyzed the effects of the writers' characteristics (age, gender, and SMS-messaging experience) on message length (number of characters and number of words), dialogue structure (with or without an opening and a closing), and message function (informative vs. relational). The SMS messages were produced in a real-world situation. We found differences across writers' characteristics for all the dependant variables. The commonly reported distinctions between girls and boys were mitigated. Moreover, for dialogical structure, the messages differed from those found in traditional oral and written interactions since 73{\%} of them did not have the conventional opening-message-closing format (the opening and/or the closing was missing). The results are discussed in terms of the specific characteristics that define the SMS register, and potentially relevant approaches to be taken in future research are addressed.},
author = {Bernicot, Josie and Volckaert-Legrier, Olga and Goumi, Antonine and Bert-Erboul, Alain},
doi = {10.1016/j.pragma.2012.07.009},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bernicot et al. - Unknown - Forms and Functions of SMS Messages A Study of Variations in a Corpus Written by Adolescents(2).pdf:pdf},
issn = {1701-1715},
keywords = {French language,adolescents,gender,register,text-messaging,writing},
title = {{Forms and Functions of SMS Messages: A Study of Variations in a Corpus Written by Adolescents}}
}
@article{Zurhellen2011,
author = {Zurhellen, Sarah},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zurhellen - Unknown - A Misnomer of Sizeable Proportions SMS and Oral Tradition(2).pdf:pdf},
journal = {Oral Tradition},
number = {2},
pages = {637--642},
title = {{SMS and Oral Tradition}},
volume = {26},
year = {2011}
}
@article{10.2307/105741,
author = {Bayes, Mr. and Price, Mr.},
issn = {02607085},
journal = {Philosophical Transactions (1683-1775)},
pages = {370--418},
publisher = {The Royal Society},
title = {{An Essay towards Solving a Problem in the Doctrine of Chances. By the Late Rev. Mr. Bayes, F. R. S. Communicated by Mr. Price, in a Letter to John Canton, A. M. F. R. S.}},
url = {http://www.jstor.org/stable/105741},
volume = {53},
year = {1763}
}
@article{McCarthy2010,
abstract = {The main purpose of this study was to examine the validity of the approach to lexical diversity assessment known as the measure of textual lexical diversity (MTLD). The index for this approach is calculated as the mean length of word strings that maintain a criterion level of lexical variation. To validate the MTLD approach, we compared it against the performances of the primary competing indices in the field, which include vocd-D, TTR, Maas, Yule's K, and an HD-D index derived directly from the hypergeometric distribution function. The comparisons involved assessments of convergent validity, divergent validity, internal validity, and incremental validity. The results of our assessments of these indices across two separate corpora suggest three major findings. First, MTLD performs well with respect to all four types of validity and is, in fact, the only index not found to vary as a function of text length. Second, HD-D is a viable alternative to the vocd-D standard. And third, three of the indices--MTLD, vocd-D (or HD-D), and Maas--appear to capture unique lexical information. We conclude by advising researchers to consider using MTLD, vocd-D (or HD-D), and Maas in their studies, rather than any single index, noting that lexical diversity can be assessed in many ways and each approach may be informative as to the construct under investigation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCarthy, Philip M and Jarvis, Scott},
doi = {10.3758/BRM.42.2.381},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McCarthy, Jarvis - 2010 - MTLD, vocd-D, and HD-D a validation study of sophisticated approaches to lexical diversity assessment(2).pdf:pdf},
isbn = {9788578110796},
issn = {1554-3528},
journal = {Behavior research methods},
keywords = {Humans,Language Tests,Language Tests: statistics {\&} numerical data,Psychology, Experimental,Psychology, Experimental: methods,Reproducibility of Results},
number = {2},
pages = {381--92},
pmid = {20479170},
title = {{MTLD, vocd-D, and HD-D: a validation study of sophisticated approaches to lexical diversity assessment.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20479170},
volume = {42},
year = {2010}
}
@article{Fazly2002,
author = {Fazly, a},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fazly - 2002 - The use of syntax in word completion utilities(2).pdf:pdf},
title = {{The use of syntax in word completion utilities}},
url = {ftp://www.learning.cs.toronto.edu/public{\_}html/public{\_}html/cs/ftp/pub/gh/Fazly-thesis.pdf},
year = {2002}
}
@article{How2005b,
abstract = {1},
author = {How, Yijue and Kan, Min-yen},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/How, Kan - 2005 - Optimizing predictive text entry for short message service on mobile phones(2).pdf:pdf},
journal = {Proceedings of Human Computer Interfaces International},
pages = {1--10},
title = {{Optimizing predictive text entry for short message service on mobile phones}},
year = {2005}
}
@misc{Brants2006,
author = {Brants, T and Franz, A},
booktitle = {Linguistic Data Consortium, Philiadelphia},
title = {{Web {\{}1T{\}} 5-gram version 1}},
url = {https://catalog.ldc.upenn.edu/ldc2006t13},
urldate = {2017-04-03},
year = {2006}
}
@article{Clarke2016,
abstract = {Improving Health Outcomes for Patients with Depression: A Population Health Imperative. Report on an Expert Panel Meeting Janice L. Clarke, RN, Alexis Skoufalos, EdD, Alice Medalia, PhD, and A. Mark Fendrick, MD Editorial: A Call to Action: David B. Nash, MD, MBA S-2 Overview: Depression and the Population Health Imperative S-3 Promoting Awareness of the Issues and Opportunities for Improvement S-5 Cognitive Dysfunction in Affective Disorders S-5 Critical Role of Employers in Improving Health Outcomes for Employees with Depression S-6 Closing the Behavioral Health Professional and Process Gaps S-6 Achieving the Triple Aim for Patients with Depressive Disorders S-6 Improving the Experience of Care for Patients with Depression S-6 Improving Quality of Care and Health Outcomes for Patients with Depression S-7 Changing the Cost of Care Discussion from How Much to How Well S-8 Panel Insights and Recommendations S-9 Conclusion S-10},
author = {Clarke, Janice L and Skoufalos, Alexis and Medalia, Alice and Fendrick, A Mark},
doi = {10.1089/pop.2016.0114},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clarke et al. - 2016 - Improving Health Outcomes for Patients with Depression A Population Health Imperative. Report on an Expert Panel.pdf:pdf},
issn = {1942-7891},
journal = {Population Health Management},
number = {S2},
pages = {S--1--S--12},
title = {{Improving Health Outcomes for Patients with Depression: A Population Health Imperative. Report on an Expert Panel Meeting}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5076486/pdf/pop.2016.0114.pdf http://online.liebertpub.com/doi/10.1089/pop.2016.0114},
volume = {19},
year = {2016}
}
@misc{RstudioTeam2016,
author = {{Rstudio Team}},
booktitle = {RStudio},
title = {{RStudio – Open source and enterprise-ready professional software for R}},
url = {https://www.rstudio.com/},
year = {2016}
}
@article{Raftery1997,
author = {Raftery, Adrian E and Madigan, David and Hoeting, Jennifer A},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raftery, Madigan, Hoeting - Unknown - Bayesian Model Averaging for Linear Regression Models.pdf:pdf},
journal = {Journal of American Statistical Association},
keywords = {bayes factor,markov chain monte carlo,model composition,model uncertainty,occam,posterior,s window},
number = {437},
pages = {179--191},
title = {{Bayesian Model Averaging for Linear Regression Models}},
url = {https://www.stat.washington.edu/raftery/Research/PDF/rmh1997.pdf},
volume = {92},
year = {1997}
}
@article{Felice2012,
author = {Felice, Mariano},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Felice - 2012 - Linguistic Indicators for Quality Estimation of Machine Translations(2).pdf:pdf},
journal = {Social Sciences},
number = {May},
title = {{Linguistic Indicators for Quality Estimation of Machine Translations}},
year = {2012}
}
@book{Data2007,
abstract = {This is an introductory course in machine learning (ML) that covers the basic theory, algorithms, and applications. ML is a key technology in Big Data, and in many financial, medical, commercial, and scientific applications. It enables computational systems to adaptively improve their performance with experience accumulated from the observed data. ML has become one of the hottest fields of study today, taken up by undergraduate and graduate students from 15 different majors at Caltech. This course balances theory and practice, and covers the mathematical as well as the heuristic aspects.},
author = {Data, Learning From},
isbn = {9780471681823},
keywords = {rning from data},
title = {{LEARNING FROM DATA}},
url = {https://work.caltech.edu/telecourse},
year = {2007}
}
@inproceedings{Shahyad2011a,
abstract = {The purpose of the present research was to comparison of content, motivation and frequency of S.M.S Messages sent by boys versus girls. 288 high school students (125 girls and 138 boys) aged 14 to 18 participated in the study that have been chosen via cluster multistage sampling method and completed Type of SMS using Assessment Questionnaire (TSAQ; Shayad, 2010). Data was analyzed by using multivariate analysis of covariance (MANOVA). The results showed that there was a significant difference between boys and girls with regard to motivation, content and frequency of S.M.S messages. Girls send S.M.S more frequently than boys. In fact they send an average of 39 S.M.S a day versus 15 S.M.S sent of the average by a high school boy. Girls seek reassuring information while boys try to sending information for assurance and avoid of face-to-face relationship when they have recourse to S.M.S Boys also send more S.M.S with uncommon content, gibe content and impersonal information. ?? 2011 Published by Elsevier Ltd.},
author = {Shahyad, Shima and Pakdaman, Shahla and Hiedary, Mohamood and Miri, Mirnader and Asadi, Masoud and Nasri, Azad and Alipour, Asghar Shir},
booktitle = {Procedia - Social and Behavioral Sciences},
doi = {10.1016/j.sbspro.2011.03.207},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shahyad et al. - 2011 - A Comparison of motivation, frequency and content of S.M.S. messages sent in boys and girls high school stude(3).pdf:pdf},
issn = {18770428},
keywords = {Adolescent,Content,High school student,Motivation,Number of sent S.M.S},
title = {{A Comparison of motivation, frequency and content of S.M.S. messages sent in boys and girls high school student}},
year = {2011}
}
@misc{Koohafkan2015,
author = {Koohafkan, Michael C},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koohafkan - 2015 - Package 'kfigr' Integrated Code Chunk Anchoring and Referencing for R Markdown.pdf:pdf},
title = {{Package 'kfigr' | Integrated Code Chunk Anchoring and Referencing for R Markdown}},
year = {2015}
}
@article{Cruz,
abstract = {Resumen: Este trabajo presenta un m{\'{e}}todo autom{\'{a}}tico para reducir el conjunto de categor{\'{i}}as de palabras que ser{\'{a}} utilizado por un sistema de predicci{\'{o}}n de palabras en Portugu{\'{e}}s. El m{\'{e}}todo se basa en una medida de similitud que se aplica a una matriz de asociaci{\'{o}}n, generada mediante el empleo de una medida de disparidad (odds ratio) aplicada sobre la matriz de distribuci{\'{o}}n de probabilidades de bigramas de categor{\'{i}}as (bipos) presentes en un corpus. Los resultados presentados en este trabajo muestran que la utilizaci{\'{o}}n del m{\'{e}}todo de agrupamiento propuesto, con un umbral adecuado de similitud, tiene potencial para mejorar el sistema de predicci{\'{o}}n de palabras. Adem{\'{a}}s posibilita la utilizaci{\'{o}}n de nuevas t{\'{e}}cnicas de agrupamiento de categor{\'{i}}as como agrupamiento borroso. Los resultados tambi{\'{e}}n muestran que cuando se utiliza un sistema de predicci{\'{o}}n de palabras basado en un modelo sint{\'{a}}ctico, la agrupaci{\'{o}}n no se puede realizar entre las categor{\'{i}}as sint{\'{a}}cticas m{\'{a}}s importantes, aunque los grupos generados parezcan correctos desde el punto de vista ling{\"{u}}{\'{i}}stisco. Palabras clave: Agrupamiento de categor{\'{i}}as de palabras, sistema de predicci{\'{o}}n de palabras, modelo del espacio vectorial, optimizaci{\'{o}}n, portugu{\'{e}}s. Abstract: This paper presents an automatic method for reducing the part-of-speech tagset to be considered by a word prediction system in Portuguese. The method is based on a similarity measure applied to a association matrix, generated by em-ploying a odds ratio association measure in the bigrams of parts-of-speech (bipos) probability distribution in a corpus. The results reported in this paper show that using the proposed clustering method with an appropriate threshold value over the similarity has the potential to improve the word prediction system. Moreover, it makes possible to use new clustering techniques such as fuzzy clustering. The re-sults also show that when using a word prediction system based on a syntactic model, the clustering cannot be performed between the major syntactic categories, even if the clusters generated seem correct from a linguistic point of view.},
author = {Cruz, Daniel and Teodiano, Cavalieri and Bastos, Freire and M{\'{a}}rio, Filho and Filho, Sarcinelli and Elena, Sira and Cagigas, Palazuelos and Guarasa, Javier Mac{\'{i}}as and {Mart{\'{i}}n S{\'{a}}nchez}, Jos{\'{e}} L},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cruz et al. - Unknown - A Part-of-Speech Tag Clustering for a Word Prediction System in Portuguese Language Agrupamiento de Categor{\'{i}}a(3).pdf:pdf},
keywords = {Part-of-speech clustering,Portuguese language,optimization,vector space model,word prediction system},
title = {{A Part-of-Speech Tag Clustering for a Word Prediction System in Portuguese Language * Agrupamiento de Categor{\'{i}}as para un Sistema de Predicci{\'{o}}n de Palabras en Portugu{\'{e}}s}}
}
@article{Center2015,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ilboudo, Sidb??wendin David Olivier and Sombi??, Issa and Soubeiga, Andr?? Kamba and Dr??bel, Tania},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ilboudo et al. - 2016 - Facteurs influenant le refus de consulter au centre de sant dans la rgion rurale Ouest du Burkina Faso.pdf:pdf},
isbn = {9788578110796},
issn = {09953914},
journal = {Sante Publique},
keywords = {Burkina Faso,Delivery of health care,Motivation,Patient preference,Rural health services,Treatment refusal},
number = {3},
pages = {391--397},
pmid = {25246403},
title = {{Facteurs influen??ant le refus de consulter au centre de sant?? dans la r??gion rurale Ouest du Burkina Faso}},
url = {http://www.pewinternet.org/2015/04/01/us-smartphone-use-in-2015/},
volume = {28},
year = {2016}
}
@misc{Fritz2008,
author = {Fritz, ‎Ben},
booktitle = {wikipedia},
title = {{Box Office Mojo}},
url = {http://www.boxofficemojo.com/},
year = {2008}
}
@book{Crystal1995,
abstract = {Includes references to New Zealand. Азбучен показалец. Библиогр. An exploration of the history, structure, and use of the English language discusses its origins and evolution, its use in literature, and its structural features, and addresses grammar, pronunciation, and vocabulary. 1. Modelling English -- pt. I. The history of English -- 2. The origins of English -- 3. Old English -- Early borrowings -- Runes -- The Old English corpus -- Literary texts -- The Anglo-Saxon chronicle -- Spelling -- Sounds -- Grammar -- Vocabulary -- Late borrowings -- Dialects -- 4. Middle English -- French and English -- The transition from Old English -- The Middle English corpus -- Literary texts -- Chaucer -- Spelling -- Sounds -- Grammar -- Vocabulary -- Latin borrowings -- Dialects -- Middle Scots -- The origins of standard English -- 5. Early modern English -- Caxton -- Transitional texts -- Renaissance English -- The inkhorn controversy -- Shakespeare -- The King James bible -- Spelling and regularization -- Punctuation -- Sounds -- Grammar -- Vocabulary -- The academy debate -- Johnson -- 6. Modern English -- Transition -- Grammatical trends -- Prescriptivism -- American English -- Breaking the rules -- Variety awareness -- Scientific language -- Literary voices -- Dickens -- Recent trends -- 7. World English -- The new world -- American dialects -- Canada -- Black English vernacular -- Australia -- New Zealand -- South Africa -- South Asia -- West Africa -- East Africa -- South-Est Asia and the South Pacific -- A world language -- Numbers of speakers -- Standard English -- The future of English -- English threatened and as threat -- pt. II. English vocabulary -- 8. The nature of the lexicon -- Lexemes -- The size of the English lexicon -- Abbreviations -- Proper names -- The size of a person's lexicon -- 9. The sources of the lexicon -- Native vocabulary -- Foreign borrowings -- Word-formation -- Unusual structures -- Lexical creation -- Literary neologism -- 10. Etymology -- Lexical history -- Semantic change -- Folk etymology -- Place names -- Surnames -- First names -- Nicknames -- Object names -- Eponyms -- 11. The structure of the lexicon -- Semantic structure -- Semantic fields -- Dictionary and thesaurus -- Collocations -- Lexical predictability -- Idioms -- Synonyms -- Antonyms -- Hyponyms -- Incompatibility -- Other sense relations -- 12. Lexical dimensions -- Loaded vocabulary -- Taboo -- Swearing -- Jargon -- Doublespeak -- Political correctness -- Catch phrases -- Vogue words -- Slogans -- Graffiti -- Slang -- Quotations -- Proverbs -- Archaisms -- Clichés -- Last words. pt. III. English grammar -- 13. Grammatical mythology -- The nature of grammar -- Knowing vs knowing about -- Traditional grammar -- Prescriptive grammar -- The 20th-century legacy -- The main branches of grammar -- 14. The structure of words -- Morphology -- Suffixation -- Adjectives -- Nouns -- The apostrophe -- Pronouns -- Verbs -- 15. Word classes -- Parts of speech -- Traditional definitions -- New classes -- Nouns -- Pronouns -- Adjectives -- Adverbs -- Verbs -- Prepositions -- Conjunctions -- Interjections -- 16. The structure of sentences -- Spoken and written syntax -- Types of sentence -- Sentence structure -- Sentence functions -- Clause elements and types -- Phrases-- Noun phrases -- Verb phrases -- Multiple sentences -- Abbreviation -- Disjuncts and comment clauses -- Reporting speech -- Sentence information -- Beyond the sentence -- pt. IV. Spoken and written English -- 17. The sound system -- Phonetics and phonology -- Vocal organs -- Vowels -- Consonants -- Syllables -- Connected speech -- Prosody -- Sound symbolism -- Pronunciation in practice -- 18. The writing system -- Graphetics and graphology -- Typography -- The alphabet -- Properties of letters -- Letter frequency -- Letter distribution -- Letter symbolism -- Analysing handwriting -- Graphetic variety -- Spelling -- Sources of irregularity -- Spelling reform -- Punctuation -- The development of the writing system. pt. V. Using English -- 19. Varieties of discourse -- Structure vs use -- Pragmatic issues -- The nature of discourse -- Microlinguistic studies -- Texts and varieties -- Speech vs writing -- Mixed medium -- Monologue and dialogue -- 20. Regional variation -- Accent and dialect -- International and intranational -- A day in the life of the language -- American and British English -- American dialects -- British dialects -- Scotland -- Wales -- Ireland -- Canada -- Caribbean -- Pidgins and Creoles -- Australia -- New Zealand -- South Africa -- New Englishes -- 21. Social variation -- Sociolinguistic perspective -- Received pronunciation -- Prescriptive attitudes -- Gender -- Occupation -- Religion -- Science -- Law -- Plain English -- Politics -- News media -- Journalism -- Broadcasting -- Weather forecasting -- Sports commentary -- Advertising -- Restricted varieties -- New varieties -- 22. Personal variation -- Individual differences -- Deviance -- Word games -- Rule-breaking varieties -- The edges of language -- Jokes and puns -- Comic alphabets -- Variety humour -- Literary freedom -- Phonetics and phonology -- Graphetics and graphology -- Grammar and lexicon -- Discourse and variety -- Stylometry -- pt. VI. Learning about English -- 23. Learning English as a mother tongue -- Child language acquisition -- Literacy -- Grammatical development -- Early words and sounds -- Reading and writing -- Insufficient language -- Language disability -- 24. New ways of studying English -- Technological revolution -- Corpus studies -- National and international corpora -- Dictionaries -- Innovations -- Sources and resources -- Appendices -- I. Glossary -- II. Special symbols and abbreviations -- III. References -- IV. Further reading -- V. Index of linguistic items -- VI. Index of authors and personalities -- VII. Index of topics.},
address = {Cambridge},
author = {Crystal, David},
edition = {1},
isbn = {0521401798},
publisher = {Cambridge University Press},
title = {{The Cambridge encyclopedia of the English language}},
year = {1995}
}
@article{doi:10.1198/016214507000001337,
author = {Liang, Feng and Paulo, Rui and Molina, German and Clyde, Merlise A and Berger, Jim O},
doi = {10.1198/016214507000001337},
journal = {Journal of the American Statistical Association},
number = {481},
pages = {410--423},
publisher = {Taylor {\&} Francis},
title = {{Mixtures of g Priors for Bayesian Variable Selection}},
url = {https://doi.org/10.1198/016214507000001337},
volume = {103},
year = {2008}
}
@article{Kneser1997,
abstract = {A new method is presented to quickly adapt a given language model to local text characteristics. The ba-sic approach is to choose the adaptive models as close as possible to the background estimates while con-straining them to respect the locally estimated uni-gram probabilities. Several means are investigated to speed up the calculations. We measure both perplex-ity and word error rate to gauge the quality of our model.},
author = {Kneser, Reinhard and Peters, Jochen and Klakow, Dietrich},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kneser, Peters, Klakow - 1997 - Language model adaptation using dynamic marginals(2).pdf:pdf},
number = {140403},
pages = {1--24},
title = {{Language model adaptation using dynamic marginals}},
url = {http://ai2-s2-pdfs.s3.amazonaws.com/f3f3/be564291d3782ac99d27810038f297bfe7ba.pdf},
volume = {2},
year = {1997}
}
@article{Gries2014,
author = {Gries, Stefan Th.},
doi = {10.1017/CBO9781139833882.005},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gries - 2014 - Quantitative corpus approaches to linguistic analysis seven or eight levels of resolution and the lessons they teach us.pdf:pdf},
isbn = {9781107038509},
journal = {Developments in English: expanding electronic evidence},
number = {3},
pages = {29--47},
title = {{Quantitative corpus approaches to linguistic analysis: seven or eight levels of resolution and the lessons they teach us}},
volume = {i},
year = {2014}
}
@article{Gendron2015,
author = {Gendron, Gerald R and Gendron, Gerald R},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gendron, Gendron - 2015 - Natural Language Processing A Model to Predict a Sequence of Words(2).pdf:pdf},
journal = {ModSim World 2015},
keywords = {data,good-turing smoothing,katz back off,n-gram,natural language processing,predictive model,predictive text analytics,product,text mining},
number = {13},
pages = {1--10},
title = {{Natural Language Processing : A Model to Predict a Sequence of Words}},
year = {2015}
}
@article{Shahyad2011,
abstract = {The purpose of the present research was to comparison of content, motivation and frequency of S.M.S Messages sent by boys versus girls. 288 high school students (125 girls and 138 boys) aged 14 to 18 participated in the study that have been chosen via cluster multistage sampling method and completed Type of SMS using Assessment Questionnaire (TSAQ; Shayad, 2010). Data was analyzed by using multivariate analysis of covariance (MANOVA). The results showed that there was a significant difference between boys and girls with regard to motivation, content and frequency of S.M.S messages. Girls send S.M.S more frequently than boys. In fact they send an average of 39 S.M.S a day versus 15 S.M.S sent of the average by a high school boy. Girls seek reassuring information while boys try to sending information for assurance and avoid of face-to-face relationship when they have recourse to S.M.S Boys also send more S.M.S with uncommon content, gibe content and impersonal information. ?? 2011 Published by Elsevier Ltd.},
author = {Shahyad, Shima and Pakdaman, Shahla and Hiedary, Mohamood and Miri, Mirnader and Asadi, Masoud and Nasri, Azad and Alipour, Asghar Shir},
doi = {10.1016/j.sbspro.2011.03.207},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shahyad et al. - 2011 - A Comparison of motivation, frequency and content of S.M.S. messages sent in boys and girls high school stude(3).pdf:pdf},
isbn = {9802133882314},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {Adolescent,Content,High school student,Motivation,Number of sent S.M.S},
pages = {895--898},
title = {{A Comparison of motivation, frequency and content of S.M.S. messages sent in boys and girls high school student}},
volume = {15},
year = {2011}
}
@article{Thurlow2011,
abstract = {On the day we first started putting this chapter together, soon-to-be-President-of-the-United-States Barack Obama announced his choice of vice-presidential running mate by sending a text message to journalists and Democratic Party senators and supporters. One not-so-restrained New York Times journalist characterized the event: 'Mr. Obama's use of the newfound medium is the widest use of texting by a presidential candidate in history.' The following morning, again in the USA, a National Public Radio journalist talked about 'the most highly anticipated text message in human history'. This already newsworthy event was evidently being given an added mediatized spin thanks to texting. No doubt like many readers of the current volume, we are not convinced of the historic proportions of the Obama campaign's text message. This was, however, certainly a communicative event loaded with pragmatic – and metapragmatic – force. Why choose to use texting to deliver this public message? (After all, supporters could just as easily have been notified by the ancient technology of email.) What did the choice of text message mean to voters? Why should it warrant such media interest? Why make so much fuss about a text message which bore so little resemblance to the millions of text messages sent every day by ordinary people around the world? In this chapter, we consider these matters by stepping back from the hyperbolic commentary on text messaging by journalists and the entertaining observations of popular writers. To this end, we start with a comprehensive but potted review of the scholarly, research-driven literature on text messaging; this work highlights the range of applications to which texting has been put as well as the ways in which sociolinguists, discourse analysts and other communication scholars have been attending to language in texting messaging. Shifting next to a more specifically pragmatic and metapragmatic focus, we present some of our own empirical research as a way to illustrate general phenomena covered in the wider scholarly literature and to ground text messaging as a pragmatic phenomenon. We close our chapter with some brief thoughts about gaps in the academic literature and possible directions for future research on the language of text messaging. Before we go any further, however, we offer the following brief account of text messaging as a digital technology.},
author = {Thurlow, Crispin and Poff, Michele},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thurlow, Poff - 2011 - Text Messaging(2).pdf:pdf},
publisher = {Mouton de Gruyter},
title = {{Text Messaging}},
year = {2011}
}
@article{Thurlow2011a,
abstract = {On the day we first started putting this chapter together, soon-to-be-President-of-the-United-States Barack Obama announced his choice of vice-presidential running mate by sending a text message to journalists and Democratic Party senators and supporters. One not-so-restrained New York Times journalist characterized the event: 'Mr. Obama's use of the newfound medium is the widest use of texting by a presidential candidate in history.' The following morning, again in the USA, a National Public Radio journalist talked about 'the most highly anticipated text message in human history'. This already newsworthy event was evidently being given an added mediatized spin thanks to texting. No doubt like many readers of the current volume, we are not convinced of the historic proportions of the Obama campaign's text message. This was, however, certainly a communicative event loaded with pragmatic – and metapragmatic – force. Why choose to use texting to deliver this public message? (After all, supporters could just as easily have been notified by the ancient technology of email.) What did the choice of text message mean to voters? Why should it warrant such media interest? Why make so much fuss about a text message which bore so little resemblance to the millions of text messages sent every day by ordinary people around the world? In this chapter, we consider these matters by stepping back from the hyperbolic commentary on text messaging by journalists and the entertaining observations of popular writers. To this end, we start with a comprehensive but potted review of the scholarly, research-driven literature on text messaging; this work highlights the range of applications to which texting has been put as well as the ways in which sociolinguists, discourse analysts and other communication scholars have been attending to language in texting messaging. Shifting next to a more specifically pragmatic and metapragmatic focus, we present some of our own empirical research as a way to illustrate general phenomena covered in the wider scholarly literature and to ground text messaging as a pragmatic phenomenon. We close our chapter with some brief thoughts about gaps in the academic literature and possible directions for future research on the language of text messaging. Before we go any further, however, we offer the following brief account of text messaging as a digital technology.},
author = {Thurlow, Crispin and Poff, Michele},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thurlow, Poff - 2011 - Text Messaging(2).pdf:pdf},
publisher = {Mouton de Gruyter},
title = {{Text Messaging}},
year = {2011}
}
@article{Baayen2015,
author = {Baayen, R H and Baayen, Maintainer R H},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baayen, Baayen - 2015 - Package 'languageR' Title Data sets and functionswiAnalyzing Linguistic Data A practical introduction to stat(2).pdf:pdf},
title = {{Package 'languageR' Title Data sets and functionswiAnalyzing Linguistic Data: A practical introduction to statistics''}},
year = {2015}
}
@misc{Needham1990,
abstract = {IMDb, the world's most popular and authoritative source for movie, TV and celebrity content.},
author = {Needham, Col},
title = {{IMDb - Movies, TV and Celebrities}},
url = {http://www.imdb.com/ http://www.imdb.de/},
urldate = {2017-11-24},
year = {1990}
}
@article{Piantadosi2014,
abstract = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf 's law. This paper first shows that human language has highly complex, reliable structure in the frequency distribution over and above this classic law, though prior data visualization methods obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf's law, and are then used to evaluate many of the theoretical explanations of Zipf's law in language. No prior account straightforwardly explains all the basic facts, nor is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf's law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
author = {Piantadosi, Steven T},
doi = {10.3758/s13423-014-0585-6},
isbn = {1342301405856},
issn = {1531-5320},
journal = {Psychonomic bulletin $\backslash${\&} review},
keywords = {frequency f,frequent word has a,known as zipf,language,obeying a power law,r,s law,statistics,that scales according to,the r th most,zipf},
month = {oct},
number = {5},
pages = {1112----1130},
pmid = {24664880},
title = {{Zipf's word frequency law in natural language: a critical review and future directions}},
url = {http://link.springer.com/10.3758/s13423-014-0585-6 http://www.ncbi.nlm.nih.gov/pubmed/24664880 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4176592},
volume = {21},
year = {2014}
}
@article{Wilson2015,
author = {Wilson, Adam and Norden, Natalia},
journal = {URL: http://www. r-project. org/254},
title = {{The R Project for Statistical Computing The R Project for Statistical Computing}},
url = {https://www.r-project.org/},
volume = {3},
year = {2015}
}
@article{Zurhellena,
abstract = {This article is one of a series of short essays, collectively titled " Further Explorations, " published as part of a special issue of Oral Tradition in honor of John Miles Foley's 65 th birthday and 2011 retirement. The surprise Festschrift, guest-edited by Lori and Scott Garner entirely without his knowledge, celebrates John's tremendous impact on studies in oral tradition through a series of essays contributed by his students from the University of Missouri-Columbia (1979-present) and from NEH Summer Seminars that he has directed (1987-1996). http://journal.oraltradition.org/issues/26ii In The Pathways Project, John Miles Foley (2011-) discusses briefly the social role of SMS (Short Message Service), suggesting that " even so-called text messaging, a misnomer of sizeable proportions given that the activity really amounts to a long-distance emergent communication enacted virtually, knits people together into interactive groups and keeps them connected and 'present' to one another. " 1 In this essay, I propose a merger of current research on text messaging and the study of oral traditions in order to shed light on the relationship between this new mode of communication and the workings of consciousness being transformed by the eAgora. Focusing first on the limitations of text messaging as a medium that unexpectedly encouraged language innovation, we can explore how text messaging language merges effective communicative practices from both oral and written technologies in order to generate more efficient communication within a newly-limited, writing-based technology. Moreover, in addition to its efficiency, the kind of linguistic play found in text messaging can be viewed as a source of pleasure for those who engage in texting (" texters "). Thus, by employing the discourse of orality and literacy, we can explain how text messaging, while impossible to imagine without the myriad writing technologies mastered before it, actually encourages its literacy-obsessed users to practice communicative techniques more often found within oral cultures, or more precisely, communicative techniques found in cultures in the incipient stages of literacy. Such cultures are ripe for language innovation precisely because they have begun to record knowledge but have not yet standardized the recording procedure. Coincident with a perspective that sees text messaging as bridging a consciousness gap between oral and literate cultures, then, is the recognition that close study of the ways in which text messaging reworks language could lead to fruitful discoveries about the most current ways in which Computer-Mediated Communication (CMC) directs human life toward ever-emerging horizons of consciousness. When David Crystal (2008) hyperbolized the emergence of text messaging in the following passage, this form of communication was already a well-developed medium. Nevertheless, his humorous figuring of text messaging's inception, while not quite accurate, highlights precisely the form's limits that made it such an unlikely competitor in the tightly-wound market of twenty-first-century technologies (173-74): Oral Tradition, 26/2 (2011): 637-624},
author = {Zurhellen, Sarah},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zurhellen - Unknown - A Misnomer of Sizeable Proportions SMS and Oral Tradition(2).pdf:pdf},
title = {{" A Misnomer of Sizeable Proportions " : SMS and Oral Tradition}}
}
@techreport{PortioResearch2015,
abstract = {This report explains how three key trends are shaping the future of communications and technology: ? A rapid shift to a mobile first mindset ? The unprecedented reach of SMS ? The emphasis on content personalisation These three trends have already fundamentally changed how people purchase goods and services, and they will continue to change the way consumers interact with companies and services. Throughout this report, we bring your attention to the ubiquity of SMS. 6.1 billion people, out of a total human population of 7.3 billion, use an SMS-capable mobile phone. SMS can be used to reach 84{\%} of the human race alive today. SMS is a true mobile first technology, and SMS offers enterprises an opportunity to communicate with their customers, staff and suppliers in a highly personalised and extremely responsive way. The future of the telecoms, media, technology and consumer electronics industries are merging and changing faster now than ever before, and we believe the trends discussed in this report will continue to be profoundly important over the next decade.},
author = {PortioResearch},
booktitle = {Megatrends in consumer technology},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/PortioResearch - 2015 - SMS The language of 6 billion people.pdf:pdf},
pages = {1--49},
title = {{SMS: The language of 6 billion people}},
url = {www.portioresearch.com{\%}5Cnportioresearch.com},
year = {2015}
}
@article{Draper1995,
author = {Draper, David},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Draper - 1995 - Assessment and Propagation of Model Uncertainty.pdf:pdf},
journal = {J.R. Statist. Soc. B},
keywords = {multidimensional register contrast,yi},
number = {109},
pages = {45--97},
title = {{Assessment and Propagation of Model Uncertainty}},
url = {https://classes.soe.ucsc.edu/ams206/Winter11/draper-1995-model-uncertainty.pdf},
volume = {57},
year = {1995}
}
@article{Neunerdt,
abstract = {—Using social media tools such as blogs and forums have become more and more popular in recent years. Hence, a huge collection of social media texts from different communities is available for accessing user opinions, e.g., for marketing studies or acceptance research. Typically, methods from Natural Language Processing are applied to social media texts to automatically recognize user opinions. A fundamental component of the linguistic pipeline in Natural Language Processing is Part-of-Speech tagging. Most state-of-the-art Part-of-Speech taggers are trained on newspaper corpora, which differ in many ways from non-standardized social media text. Hence, applying common taggers to such texts results in performance degradation. In this paper, we present extensions to a basic Markov model tagger for the annotation of social media texts. Considering the German standard Stuttgart/ ubinger TagSet (STTS), we distinguish 54 tag classes. Applying our approach improves the tagging accuracy for social media texts considerably, when we train our model on a combination of annotated texts from newspapers and Web comments.},
author = {Neunerdt, Melanie and Reyer, Michael and Mathar, Rudolf},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neunerdt, Reyer, Mathar - 2013 - A POS Tagger for Social Media Texts trained on web comments.pdf:pdf},
keywords = {German,Index Terms—Natural language processing,opinion mining,part-of-speech tagging},
title = {{A POS Tagger for Social Media Texts Trained on Web Comments}}
}
@article{Klakow1998a,
abstract = {Building probabilistic models of language is a central task in natural language and speech processing allowing to integrate the syntactic and/or semantic (and recently pragmatic) constraints of the language into the systems. Probabilistic language models are an attractive alternative to the more traditional rule-based systems, such as context free grammars, because of the recent availability of massive amount of text corpora which can be used to efficiently train the models and because instead of binary grammaticality judgement offered by the rule-based systems, likelihood of any sequence of lexical units can be obtained, which is a crucial factor in such tasks as speech recognition. Probabilistic language models also find their application in part-of-speech tagging, machine translation, semantic disambiguation and numerous other fields. The most widely used language models are based on the estimation of the proba-bility of observing a given lexical unit conditioned on the observations of n−1 preced-ing lexical units, and are known as n-gram models. When the n-gram estimates are poor, whatever the reason for that may be, a technique called smoothing is applied to adjust the estimates and hopefully produce more accurate model. Smoothing techniques may be roughly divided into the backing-off and interpolation. In the first case, the best n-gram model in the current context is selected, whereas in the second case all the n-gram models of different specificities are combined together to form a better predictor. In this thesis, a recently proposed novel interpolation scheme is investigated, namely, the log-linear interpolation. Unlike the original publication, however, which dealt with combining the models of unrelated nature, the aim of this thesis is to formulate the theoretical framework for smoothing the n-gram probability estimates obtained from similar language models with different levels of specificity on the same corpus, which will be called log-linear n-gram smoothing, and compare it to the well-established linear interpolation and back-off methods. The framework being proposed includes probability combination, parameter optimisation, dealing with data sparsity and parameter clustering. The resulting technique is shown to outperform the conventional linear interpo-lation and back-off techniques when applied to the n-gram smoothing tasks. ii},
author = {Klakow, Dietrich},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klakow - 1998 - Log-linear interpolation of language models(2).pdf:pdf},
journal = {Proc. ICSLP},
number = {January},
pages = {1--4},
title = {{Log-linear interpolation of language models}},
url = {http://www.mirlab.org/conference{\_}papers/International{\_}Conference/ICSLP 1998/PDF/SCAN/SL980522.PDF http://www.mirlab.org/conference{\_}papers/International{\_}Conference/ICSLP 1998/PDF/AUTHOR/SL980522.PDF},
year = {1998}
}
@article{Komatsu2005,
author = {Komatsu, Hiroyuki and Takabayashi, Satoru and Masui, Toshiyuki},
doi = {10.1109/AMT.2005.1505271},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Komatsu, Takabayashi, Masui - 2005 - Corpus-based predictive text input(3).pdf:pdf},
isbn = {0780390350},
journal = {Proceedings of the 2005 International Conference on Active Media Technology, AMT 2005},
number = {October},
pages = {75--80},
title = {{Corpus-based predictive text input}},
volume = {2005},
year = {2005}
}
@article{Rcpp2016,
author = {Rcpp, Linkingto},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rcpp - 2016 - R Package ‘reshape2'.pdf:pdf},
title = {{R Package ‘reshape2'}},
url = {https://cran.r-project.org/web/packages/reshape2/reshape2.pdf},
year = {2016}
}
@article{Samuel1959,
author = {Samuel, AL},
doi = {10.1147/rd.33.0210},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samuel - Unknown - MACHINE LEARNING USING THE GAME OF CHECKERS.pdf:pdf},
isbn = {0018-8646},
issn = {0018-8646},
journal = {IBM Journal of research and development},
number = {3},
pages = {210--229},
title = {{Some studies in machine learning using the game of checkers}},
url = {https://pdfs.semanticscholar.org/e9e6/bb5f2a04ae30d8ecc9287f8b702eedd7b772.pdf http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5392560},
volume = {3},
year = {1959}
}
@article{Suster2011,
author = {Suster, Simoň},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Suster - 2011 - Measuring lexical and syntactic complexity of the language production of the World-of-Warcraft gaming community(3).pdf:pdf},
title = {{Measuring lexical and syntactic complexity of the language production of the World-of-Warcraft gaming community}},
year = {2011}
}
@article{Chelba1998,
abstract = {The paper presents a language model that devel-ops syntactic structure and uses it to extract mean-ingful information from the word history, thus en-abling the use of long distance dependencies. The model assigns probability to every joint sequence of words–binary-parse-structure with headword an-notation and operates in a left-to-right manner — therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/9811022v2},
author = {Chelba, Ciprian and Jelinek, Frederick},
eprint = {9811022v2},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chelba, Jelinek - 1998 - Exploiting Syntactic Structure for Language Modeling.pdf:pdf},
journal = {Proceedings of the Thirty-Sixth Annual Meeting of the {\{}A{\}}ssociation for {\{}C{\}}omputational {\{}L{\}}inguistics and Seventeenth International Conference on Computational Linguistics},
pages = {225--231},
primaryClass = {arXiv:cs},
title = {{Exploiting Syntactic Structure for Language Modeling}},
url = {https://arxiv.org/pdf/cs/9811022.pdf},
year = {1998}
}
@article{Agarwal2007,
author = {Agarwal, Sachin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal - 2007 - Context based word prediction for texting language.pdf:pdf},
journal = {Large Scale Semantic Access to Content (Text,},
pages = {360--368},
title = {{Context based word prediction for texting language}},
url = {http://dl.acm.org/citation.cfm?id=1931426},
year = {2007}
}
@article{Landauer1997,
abstract = {Bellcore How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by ex-tracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched. Prologue "How much do we know at any time? Much more, or so I believe, than we know we know!" —Agatha Christie, The Moving Finger A typical American seventh grader knows the meaning of 10-15 words today that she did not know yesterday. She must have acquired most of them as a result of reading because (a) the majority of English words are used only in print, (b) she already knew well almost all the words she would have encoun-tered in speech, and (c) she learned less than one word by direct instruction. Studies of children reading grade-school text find that about one word in every 20 paragraphs goes from wrong to right on a vocabulary test. The typical seventh grader would have read less than 50 paragraphs since yesterday, from which she should have learned less than three new words. Apparently, she mastered the meanings of many words that she did not encounter. Evidence for all these assertions is given in detail later. This phenomenon offers an ideal case in which to study a problem that has plagued philosophy and science since Plato We thank Karen Lochbaum for valuable help in analysis; George Furnas for early ideas and inspiration; Peter Foltz, Walter Kintsch, and Ernie Mross for unpublished data; and for helpful comments on the ideas and drafts, we thank, in alphabetic order, 24 centuries ago, the fact that people have much more knowl-edge than appears to be present in the information to which they have been exposed. Plato's solution, of course, was that people must come equipped with most of their knowledge and need only hints and contemplation to complete it. In this article we suggest a very different hypothesis to explain the mystery of excessive learning. It rests on the simple notion that some domains of knowledge contain vast numbers of weak interrelations that, if properly exploited, can greatly amplify learning by a process of inference. We have discovered that a very simple mechanism of induction, the choice of the correct dimensionality in which to represent similarity between objects and events, can sometimes, in particular in learning about the similarity of the meanings of words, produce sufficient enhance-ment of knowledge to bridge the gap between the information available in local contiguity and what people know after large amounts of experience. Overview},
author = {Landauer, Thomas K and Dutnais, Susan T and Anderson, Richard and Carroll, Doug and Fbltz, Peter and Pumas, George and Kintsch, Walter and Menn, Lise and Streeter, Lynn},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Landauer et al. - 1997 - A Solution to Plato's Problem The Latent Semantic Analysis Theory of Acquisition, Induction, and Representat(2).pdf:pdf},
journal = {Psychological Review},
number = {2},
pages = {211--240},
title = {{A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge}},
url = {http://www.indiana.edu/{~}clcl/Q550{\_}WWW/Papers/Landauer{\_}Dumais{\_}1997.pdf},
volume = {1},
year = {1997}
}
@misc{Cool-Smileys2010,
author = {Cool-Smileys},
title = {{List of Text Emoticons: The Ultimate Resource}},
url = {http://cool-smileys.com/text-emoticons},
year = {2010}
}
@article{Lyddy,
abstract = {Concerns over effects of 'textisms' on literacy have been reinforced by research identifying processing costs associated with reading textisms. But to what extent do such studies reflect actual textism use? This study examined the textual characteristics of 936 text messages in English (13391 words). Message length, nonstandard spelling, sender and message characteristics and word frequency were analyzed. The data showed that 25{\%} of word content used nonstandard spelling, the most frequently occurring category involving omission of capital letters. Types of nonstandard spelling varied only slightly depending on the purpose of the text message, while the overall proportion of nonstandard spelling did not differ significantly. Less than 0.2{\%} of content was 'semantically unrecoverable.' Implications for experimental studies of textisms are discussed. Text messaging, short message service (SMS) or 'texting' continues to be a popular means of communi-cation, among young people in particular. A report by Lenhart, Ling, Campbell, and Purcell (2010) high-lighted the rapid increase in text messaging in the United States, where 72{\%} of teenagers use text messag-ing, compared to 51{\%} in 2006. In a British survey, 52{\%} of young people aged 11-18 (n = 1000), along with 28{\%} of adults aged 18-65 (n = 2000), named texting as the most important form of communication that they use to stay in touch with friends (Mobile Life Report, 2008); for the young people surveyed, texting ranked above instant messaging (17{\%}), e-mail (12{\%}), calls via mobile (9{\%}) or landline (10{\%}), and letters (0{\%}). A British survey of 2117 adults shows the increasing popularity of texting from 2005 to 2010, with 62{\%} of those aged 16-24 preferring texting over other means of communicating with friends (Ofcom, 2011). Ling's (2010) cross-sectional analysis suggests that texting follows a life-phase pattern, with older teens and those in their early 20s making the most use of the medium, with usage dropping off with age. The authors would like to thank Maria Bakardjieva and two anonymous reviewers for helpful criticisms and suggestions regarding data presentation and analysis.},
author = {Lyddy, Fiona and Farina, Francesca and Hanney, James and Farrell, Lynn and Kelly, Niamh and Neill, O '},
doi = {10.1111/jcc4.12045},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lyddy et al. - Unknown - An Analysis of Language in University Students' Text Messages(2).pdf:pdf},
keywords = {Language use,Linguistic,Literacy,Mobile phones,Text messaging},
title = {{An Analysis of Language in University Students' Text Messages *}}
}
@article{Sundarkantham2011,
abstract = {Language is a unique phenomenon that distinguishes man from other animals. It is our primary method of communication with each other, yet very little is understood about how language is acquired when we are infants. A greater understanding in this area would have the potential to improve man machine communication The problem that is attempted to be solved in this paper is that of programming a computer to play the Shannon Game. To play the Shannon game, one must predict which words are most likely to follow a given segment of English Text. Word Prediction would be most useful for writers with physical disabilities and severe spelling problems. The aim of this paper is to improve on existing results by writing a program that is capable of automatically inferring a grammar from a Natural Language Corpus, and applying this to the Shannon Game. To play the Shannon Game, a stochastic Grammar for an approximation to the target language must be inferred from a text sample, and as the quality of this grammar improves so too does the quality of the predictor that uses the inferred grammar. The proposed algorithm in the paper uses Support Vector Machine to perform the part of speech tagging which produces 97.6{\%} correct predictions.},
author = {Sundarkantham, K and Shalinie, S Mercy and Pushparathi, S.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sundarkantham, Shalinie, Pushparathi - 2011 - Word predictor using natural language grammar induction technique.pdf:pdf},
issn = {18234690},
journal = {Journal of Engineering Science and Technology},
keywords = {K-means clustering,Natural language grammatical inference,Support vector machines},
number = {2},
pages = {204--215},
title = {{Word predictor using natural language grammar induction technique}},
url = {www.jatit.org},
volume = {6},
year = {2011}
}
@misc{CenterforBehavioralHealthStatisticsandQuality2016,
author = {{Center for Behavioral Health Statistics and Quality}},
booktitle = {No.SMA 16-4984, NSDUH Series H-51},
doi = {(HHS Publication No. SMA 16-4984, NSDUH Series H-51},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Center for Behavioral Health Statistics and Quality - 2016 - Key substance use and mental health indicators in the United States Results.pdf:pdf},
keywords = {alcohol use,behavioral health,co-occurring mental and substance use disorders,co-occurring mental and substance use disorders am,illicit drug use,major depressive episode among adults,major depressive episode among youths,mental health service use,mental health service use among adults with mental,mental health service use among those with co-occu,mental illness among adults,need for substance use treatment,prescription drug misuse,specialty substance use treatment,substance dependence or abuse,substance use,substance use disorders,substance use treatment,tobacco use,treatment for depression,trends},
number = {1},
pages = {877--726},
title = {{Key substance use and mental health indicators in the United States: Results from the 2015 national survey on drug use and health}},
url = {http://store.samhsa.gov. http://www.samhsa.gov/data/},
urldate = {2017-06-14},
volume = {7},
year = {2016}
}
@article{Ghayoomi2005a,
abstract = {Word prediction is the problem of guessing which words are likely to follow in a given segment of a text to help a user with disabilities. As the user enters each letters of the required word, the system displays a list of the most probable words that could appear in that position. In our research we designed and implemented a word predictor for the Persian language. Three standard performance metrics were used to evaluate the system including keystroke saving, the most important one. The system achieved 57.57{\%} saving in keystrokes.},
author = {Ghayoomi, Masood and Assi, Seyyed Mostafa},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Assi - 2005 - Word Prediction in a Running Text A Statistical Language Modeling for the Persian Language(3).pdf:pdf},
journal = {Proceedings of the Australasian Language Technology Workshop},
number = {December},
pages = {57--63},
title = {{Word Prediction in a Running Text: A Statistical Language Modeling for the Persian Language}},
year = {2005}
}
@book{Zipf49,
annote = {Basic Study on word frequency relative to word lengths},
author = {Zipf, George K},
keywords = {bibtex-import},
publisher = {Addison-Wesley (Reading MA)},
title = {{Human Behavior and the Principle of Least Effort}},
year = {1949}
}
@misc{Bailey2016,
author = {Bailey, Cathy},
booktitle = {Georgetown Linguistics},
title = {{Brown Corpus Word Frequency List}},
url = {http://www.lextutor.ca/freq/lists{\_}download/},
year = {2016}
}
@article{Tagg2009,
author = {Tagg, Caroline},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tagg - 2009 - A CORPUS LINGUISTICS STUDY OF SMS TEXT MESSAGING(2).pdf:pdf},
number = {March},
title = {by},
year = {2009}
}
@misc{Norvig,
author = {Norvig, Peter},
title = {{English Letter Frequency Counts: Mayzner Revisited or ETAOIN SRHLDCU}},
url = {http://norvig.com/mayzner.html}
}
@article{doi:10.1080/0092623X.2016.1178675,
annote = {PMID: 27096488},
author = {Haupert, M L and Gesselman, Amanda N and Moors, Amy C and Fisher, Helen E and Garcia, Justin R},
doi = {10.1080/0092623X.2016.1178675},
journal = {Journal of Sex {\&} Marital Therapy},
number = {5},
pages = {424--440},
publisher = {Routledge},
title = {{Prevalence of Experiences With Consensual Nonmonogamous Relationships: Findings From Two National Samples of Single Americans}},
url = {http://dx.doi.org/10.1080/0092623X.2016.1178675},
volume = {43},
year = {2017}
}
@article{Garay-Vitoria2004,
abstract = {Prediction is one of the most extended techniques to enhance the rate of communication for people with motor and speech impairments who use Augmentative and Alternative Communication systems. There is an enormous diversity of prediction methods and techniques mentioned in the literature. Therefore, the designer finds tremendous difficulties in understanding and comparing them in order to decide the most convenient technique for a specific design. This paper presents a survey on prediction techniques applied to communicators with the intention of helping them to understand this field. Prediction applications and related features, such as block size, dictionary structure, prediction method, interface, special features, measurement and results, are detailed. Systems found in the literature are studied and described. Finally, a discussion is carried out on the possible comparison among the different methods. {\textcopyright} Springer-Verlag 2004.},
author = {Garay-Vitoria, Nestor and Abascal, Julio},
doi = {10.1007/978-3-540-30111-0_35},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garay-Vitoria, Abascal - 2004 - A comparison of prediction techniques to enhance the communication rate(3).pdf:pdf},
isbn = {978-3-540-23375-6},
issn = {03029743 16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {October},
pages = {400--417},
title = {{A comparison of prediction techniques to enhance the communication rate}},
volume = {3196},
year = {2004}
}
@misc{UniversityofWash,
abstract = {This Specialization from leading researchers at the University of Washington introduces you to the exciting, high-demand field of Machine Learning. Through a series of practical case studies, you will gain applied experience in major areas of Machine Learning including Prediction, Classification, Clustering, and Information Retrieval. You will learn to analyze large and complex datasets, create systems that adapt and improve over time, and build intelligent applications that can make predictions from data.},
author = {{University of Washington}},
keywords = {Coursera,certificates,courses,education,free,mooc,online,specializations},
title = {{Machine Learning - University of Washington | Coursera}},
url = {https://www.coursera.org/specializations/machine-learning https://www.coursera.org/course/machlearning},
urldate = {2017-09-08}
}
@article{Wickham2016b,
author = {Wickham, Hadley},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham - 2016 - Package ‘stringr'(2).pdf:pdf},
title = {{Package ‘stringr'}},
year = {2016}
}
@misc{AmericanPsychiatricAssociation2017,
author = {{American Psychiatric Association}},
title = {{What Is Depression?}},
url = {https://www.psychiatry.org/patients-families/depression/what-is-depression},
urldate = {2017-06-13},
year = {2017}
}
@article{Eicher2011,
abstract = {SUMMARY Bayesian model averaging (BMA) has become widely accepted as a way of accounting for model uncertainty, notably in regression models for identifying the determinants of economic growth. To implement BMA the user must specify a prior distribution in two parts: a prior for the regression parameters and a prior over the model space. Here we address the issue of which default prior to use for BMA in linear regression. We compare 12 candidate parameter priors: the unit information prior (UIP) corresponding to the BIC or Schwarz approximation to the integrated likelihood, a proper data-dependent prior, and 10 priors considered by Fern{\'{a}}ndez et al. (Journal of Econometrics 2001; 100: 381–427). We also compare two model priors: the uniform model prior and a prior with prior expected model size 7. We compare them on the basis of cross-validated predictive performance on a well-known growth dataset and on two simulated examples from the literature. We found that the UIP with uniform model prior generally outperformed the other priors considered. It also identified the largest set of growth determinants.},
author = {Eicher, Theo S and Papageorgiou, Chris and Raftery, Adrian E},
doi = {10.1002/jae.1112},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eicher, Papageorgiou, Raftery - 2011 - JOURNAL OF APPLIED ECONOMETRICS DEFAULT PRIORS AND PREDICTIVE PERFORMANCE IN BAYESIAN MODEL AVERA.pdf:pdf},
journal = {J. Appl. Econ},
pages = {30--55},
title = {{JOURNAL OF APPLIED ECONOMETRICS DEFAULT PRIORS AND PREDICTIVE PERFORMANCE IN BAYESIAN MODEL AVERAGING, WITH APPLICATION TO GROWTH DETERMINANTS}},
url = {https://pdfs.semanticscholar.org/dab7/263752fa35159d3c5c90b5c3ee2cf1fcc74a.pdf},
volume = {26},
year = {2011}
}
@article{Lim2012,
abstract = {Background Quantification of the disease burden caused by different risks informs prevention by providing an account of health loss different to that provided by a disease-by-disease analysis. No complete revision of global disease burden caused by risk factors has been done since a comparative risk assessment in 2000, and no previous analysis has assessed changes in burden attributable to risk factors over time. Methods We estimated deaths and disability-adjusted life years; DALYs; sum of years lived with disability [YLD] and years of life lost [YLL]) attributable to the independent effects of 67 risk factors and clusters of risk factors for 21 regions in 1990 and 2010. We estimated exposure distributions for each year, region, sex, and age group, and relative risks per unit of exposure by systematically reviewing and synthesising published and unpublished data. We used these estimates, together with estimates of cause-specific deaths and DALYs from the Global Burden of Disease Study 2010, to calculate the burden attributable to each risk factor exposure compared with the theoretical-minimum-risk exposure. We incorporated uncertainty in disease burden, relative risks, and exposures into our estimates of attributable burden. Findings In 2010, the three leading risk factors for global disease burden were high blood pressure (7 0{\%} [95{\%} uncertainty interval 6 2-7 7] of global DALYs); tobacco smoking including second-hand smoke (6 3{\%} [5 5-7 0]), and alcohol use (5 5{\%} [5 0-5 9]). In 1990, the leading risks were childhood underweight (7 9{\%} [6 8-9 4]), household air pollution from solid fuels; (HAP; 7 0{\%} [5 6-8 3]), and tobacco smoking including second-hand smoke (6 1{\%} [5 4-6 8]). Dietary risk factors and physical inactivity collectively accounted for 10 0{\%} (95{\%} UI 9 2-10 8) of global DALYs in 2010, with the most prominent dietary risks being diets low in fruits and those high in sodium. Several risks that primarily affect childhood communicable diseases, including unimproved water and sanitation and childhood micronutrient deficiencies, fell in rank between 1990 and 2010, with unimproved water ' and sanitation accounting for 0 9{\%} (0 4-1 6) of global DALYs in 2010. However, in most of sub-Saharan Africa childhood underweight, HAP, and non-exclusive and discontinued breastfeeding were the leading risks in 2010, while HAP was the leading risk in south Asia. The leading risk factor in Eastern Europe, most of Latin America, and southern sub-Saharan Africa in 2010 was alcohol use; in most of Asia, North Africa and Middle East, and central Europe it was high blood pressure. Despite declines, tobacco smoking including second-hand smoke remained the leading risk in high-income north America and western Europe. High body-mass index has increased globally and it is the leading risk in Australasia and southern Latin America, and also ranks high in other high-income regions, North Africa and Middle East, and Oceania. Interpretation Worldwide, the contribution of different risk factors to disease burden has changed substantially, with a shift away from risks for communicable diseases in children towards those for non-communicable diseases in adults. These changes are related to the ageing population, decreased mortality among children younger than 5 years, changes in cause-of-death composition, and changes in risk factor exposures. New evidence has led to changes in the magnitude of key risks including unimproved water and sanitation, vitamin A and zinc deficiencies, and ambient particulate matter pollution. The extent to which the epidemiological shift has occurred and what the leading risks currently are varies greatly across regions. In much of sub-Saharan Africa, the leading risks are still those associated with poverty and those that affect children.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Lim, Stephen S. and Vos, Theo and Flaxman, Abraham D. and Danaei, Goodarz and Shibuya, Kenji and Adair-Rohani, Heather and Amann, Markus and Anderson, H. Ross and Andrews, Kathryn G. and Aryee, Martin and Atkinson, Charles and Bacchus, Loraine J. and Bahalim, Adil N. and Balakrishnan, Kalpana and Balmes, John and Barker-Collo, Suzanne and Baxter, Amanda and Bell, Michelle L. and Blore, Jed D. and Blyth, Fiona and Bonner, Carissa and Borges, Guilherme and Bourne, Rupert and Boussinesq, Michel and Brauer, Michael and Brooks, Peter and Bruce, Nigel G. and Brunekreef, Bert and Bryan-Hancock, Claire and Bucello, Chiara and Buchbinder, Rachelle and Bull, Fiona and Burnett, Richard T. and Byers, Tim E. and Calabria, Bianca and Carapetis, Jonathan and Carnahan, Emily and Chafe, Zoe and Charlson, Fiona and Chen, Honglei and Chen, Jian Shen and Cheng, Andrew Tai Ann and Child, Jennifer Christine and Cohen, Aaron and Colson, K. Ellicott and Cowie, Benjamin C. and Darby, Sarah and Darling, Susan and Davis, Adrian and Degenhardt, Louisa and Dentener, Frank and {Des Jarlais}, Don C. and Devries, Karen and Dherani, Mukesh and Ding, Eric L. and Dorsey, E. Ray and Driscoll, Tim and Edmond, Karen and Ali, Suad Eltahir and Engell, Rebecca E. and Erwin, Patricia J. and Fahimi, Saman and Falder, Gail and Farzadfar, Farshad and Ferrari, Alize and Finucane, Mariel M. and Flaxman, Seth and Fowkes, Francis Gerry R. and Freedman, Greg and Freeman, Michael K. and Gakidou, Emmanuela and Ghosh, Santu and Giovannucci, Edward and Gmel, Gerhard and Graham, Kathryn and Grainger, Rebecca and Grant, Bridget and Gunnell, David and Gutierrez, Hialy R. and Hall, Wayne and Hoek, Hans W. and Hogan, Anthony and Hosgood, H. Dean and Hoy, Damian and Hu, Howard and Hubbell, Bryan J. and Hutchings, Sally J. and Ibeanusi, Sydney E. and Jacklyn, Gemma L. and Jasrasaria, Rashmi and Jonas, Jost B. and Kan, Haidong and Kanis, John A. and Kassebaum, Nicholas and Kawakami, Norito and Khang, Young Ho and Khatibzadeh, Shahab and Khoo, Jon Paul and Kok, Cindy and Laden, Francine and Lalloo, Ratilal and Lan, Qing and Lathlean, Tim and Leasher, Janet L. and Leigh, James and Li, Yang and Lin, John Kent and Lipshultz, Steven E. and London, Stephanie and Lozano, Rafael and Lu, Yuan and Mak, Joelle and Malekzadeh, Reza and Mallinger, Leslie and Marcenes, Wagner and March, Lyn and Marks, Robin and Martin, Randall and McGale, Paul and McGrath, John and Mehta, Sumi and Mensah, George A. and Merriman, Tony R. and Micha, Renata and Michaud, Catherine and Mishra, Vinod and Hanafiah, Khayriyyah Mohd and Mokdad, Ali A. and Morawska, Lidia and Mozaffarian, Dariush and Murphy, Tasha and Naghavi, Mohsen and Neal, Bruce and Nelson, Paul K. and Nolla, Joan Miquel and Norman, Rosana and Olives, Casey and Omer, Saad B. and Orchard, Jessica and Osborne, Richard and Ostro, Bart and Page, Andrew and Pandey, Kiran D. and Parry, Charles D.H. and Passmore, Erin and Patra, Jayadeep and Pearce, Neil and Pelizzari, Pamela M. and Petzold, Max and Phillips, Michael R. and Pope, Dan and Pope, C. Arden and Powles, John and Rao, Mayuree and Razavi, Homie and Rehfuess, Eva A. and Rehm, J??rgen T. and Ritz, Beate and Rivara, Frederick P. and Roberts, Thomas and Robinson, Carolyn and Rodriguez-Portales, Jose A. and Romieu, Isabelle and Room, Robin and Rosenfeld, Lisa C. and Roy, Ananya and Rushton, Lesley and Salomon, Joshua A. and Sampson, Uchechukwu and Sanchez-Riera, Lidia and Sanman, Ella and Sapkota, Amir and Seedat, Soraya and Shi, Peilin and Shield, Kevin and Shivakoti, Rupak and Singh, Gitanjali M. and Sleet, David A. and Smith, Emma and Smith, Kirk R. and Stapelberg, Nicolas J.C. and Steenland, Kyle and St??ckl, Heidi and Stovner, Lars Jacob and Straif, Kurt and Straney, Lahn and Thurston, George D. and Tran, Jimmy H. and {Van Dingenen}, Rita and {Van Donkelaar}, Aaron and Veerman, J. Lennert and Vijayakumar, Lakshmi and Weintraub, Robert and Weissman, Myrna M. and White, Richard A. and Whiteford, Harvey and Wiersma, Steven T. and Wilkinson, James D. and Williams, Hywel C. and Williams, Warwick and Wilson, Nicholas and Woolf, Anthony D. and Yip, Paul and Zielinski, Jan M. and Lopez, Alan D. and Murray, Christopher J.L. and Ezzati, Majid},
doi = {10.1016/S0140-6736(12)61766-8},
eprint = {15334406},
isbn = {0140-6736},
issn = {01406736},
journal = {The Lancet},
number = {9859},
pages = {2224--2260},
pmid = {23245609},
title = {{A comparative risk assessment of burden of disease and injury attributable to 67 risk factors and risk factor clusters in 21 regions, 1990-2010: A systematic analysis for the Global Burden of Disease Study 2010}},
volume = {380},
year = {2012}
}
@article{Covington2010b,
author = {Covington, M.A. and McFall, J.D.},
journal = {Journal of Quantitative Linguistics},
number = {2},
pages = {94--100},
title = {{Cutting the Gordian Knot: The Moving-Average Type-Token Ratio (MATTR).}},
volume = {17},
year = {2010}
}
@article{Devol2007,
abstract = {Good information on trends and costs on national and state levels. Implications and impact of chronic disease in the workplace are presented.},
author = {Devol, Ross and Bedroussian, Armen},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Devol, Bedroussian - 2007 - An Unhealthy America The Economic Burden of Chronic Disease.pdf:pdf},
journal = {Milken Institute},
number = {October},
pages = {1--252},
title = {{An Unhealthy America : The Economic Burden of Chronic Disease}},
url = {www.milkeninstitute.org http://health-equity.pitt.edu/id/eprint/847},
year = {2007}
}
@article{Ostrogonac2012,
author = {Ostrogonac, Stevan and Secujski, Milan and Miskovic, Dragisa},
doi = {10.1109/TELFOR.2012.6419309},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ostrogonac, Secujski, Miskovic - 2012 - Impact of training corpus size on the quality of different types of language models for Serbian.pdf:pdf},
isbn = {9781467329842},
journal = {2012 20th Telecommunications Forum, TELFOR 2012 - Proceedings},
keywords = {Language model,discrimination coefficient,evaluation,perplexity},
number = {November},
pages = {720--723},
title = {{Impact of training corpus size on the quality of different types of language models for Serbian}},
year = {2012}
}
@misc{Feinerer2015a,
annote = {R package version 0.6-2},
author = {Feinerer, Ingo and Hornik, Kurt},
title = {{tm: Text Mining Package}},
url = {http://cran.r-project.org/package=tm},
year = {2015}
}
@article{Francis2010,
abstract = {Text mining is an emerging technology that can be used to augment existing data in corporate databases by making unstructured text data available for analysis. An excellent introduction to text mining is provided by Weiss, et al. (2005). Francis (2006) provides a short introduction to text mining with a focus on insurance applications. One of the difficulties in getting started with text mining is acquiring the tools, i.e., the software for implementing text mining. Much of the software is expensive and/or difficult to use. For instance, some of the software requires purchase of expensive data mining suites. Other commercial software is more suited to large scale industrial strength applications such as cataloging and searching academic papers. One innovation that has made budget-friendly software available to text miners is open source software. Two of the very popular open source products that will be featured in this paper are R and Perl. R is a programming language that has wide acceptance for statistical and applied mathematical applications. Perl is a programming language that is particularly strong in text processing. Both languages can be easily downloaded from the Internet. Moreover, a large body of literature now exists to guide users through specialized applications such as text mining. This paper will rely heavily on information in the book Practical Text Mining in Perl by Roger Bilisoly (2008) when illustrating text mining applications in Perl. It will also rely heavily on the R tm library. While this library is described by Feinerer, Hornik, and Meyer (2008), the applications are not insurance applications, and a significant amount of trial and error can be required to achieve successful application of the software to insurance problems. We hope to make it easier for potential users to employ Perl and/or R for insurance text mining projects by illustrating their application to insurance problems with detailed information on the code and functions needed to perform the different text mining tasks. In this paper we will illustrate the application of the text mining software using two applications: 1) analyzing a free-form accident description field in an insurance claims database and 2) analyzing free-form comments in a survey of actuaries. Both applications involve relatively small text fields, compared to those anticipated by many text mining products. We will also show how text mining can be used to generate new information from the unstructured text data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Francis, Louise and Flynn, Matt},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Francis, Flynn - 2010 - Text Mining Handbook(2).pdf:pdf},
isbn = {9780521836579},
issn = {1098-6596},
journal = {Society},
keywords = {{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_},data mining,predictive modeling,text mining},
number = {2008},
pages = {1--61},
pmid = {25246403},
title = {{Text Mining Handbook}},
url = {http://www.casact.net/pubs/forum/10spforum/CompleteS10.pdf{\#}page=5},
year = {2010}
}
@article{Trnka2007,
abstract = {Word prediction can be used to enhance the communication rate of people with disabilities who use Augmentative and Alternative Communication (AAC) devices. We use statistical methods in a word prediction system, which are trained on a corpus, and then measure the efficacy of the resulting system by calculating the theoretical keystroke savings on some held out data. Ideally training and testing should be done on a large corpus of AAC text covering a variety of topics, but no such corpus exists. We discuss training and testing on a wide variety of corpora meant to approximate text from AAC users. We show that training on a combination of in-domain data with out-of-domain data is often more beneficial than either data set alone and that advanced language modeling such as topic modeling is portable even when applied to very different text.},
author = {Trnka, Keith and McCoy, Kathleen F.},
doi = {10.1145/1296843.1296877},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka, McCoy - 2007 - Corpus studies in word prediction(2).pdf:pdf},
isbn = {9781595935731},
journal = {Proceedings of the 9th international ACM SIGACCESS conference on Computers and accessibility - Assets '07},
keywords = {cor-,language modeling,statistical methods,word prediction},
pages = {195},
title = {{Corpus studies in word prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1296843.1296877},
year = {2007}
}
@article{Low,
author = {Low, Melvin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Low - 2016 - Character-level Recurrent Text Prediction.pdf:pdf},
pages = {1--6},
title = {{Character-level Recurrent Text Prediction}}
}
@misc{RCoreTeam2016,
author = {{The R Foundation}},
title = {{The R project for statistical computing}},
url = {https://www.r-project.org/},
volume = {8/1},
year = {2016}
}
@article{doi:10.1093/mind/XLI.164.409,
author = {JOHNSON, W E},
doi = {10.1093/mind/XLI.164.409},
journal = {Mind},
number = {164},
pages = {409},
title = {{I.—PROBABILITY: THE DEDUCTIVE AND INDUCTIVE PROBLEMS}},
url = {+ http://dx.doi.org/10.1093/mind/XLI.164.409},
volume = {XLI},
year = {1932}
}
@article{VandenBosch2008,
abstract = {Word completion is a basic technology for reducing the effort involved in text entry on mobile devices and in augmentative communication devices, where efficiency and ease of use are needed, but where a low memory footprint is also required. Standard solutions compress a lexicon into a suffix tree with a small memory footprint and high retrieval speed. Keystroke savings, a measurable correlate of text entry effort gain, typically improve when the algorithm would also take into account the previous word; however, this comes at the cost of a large footprint. We develop two word completion algorithms that encode the previous word in the input. The first algorithm utilizes a character buffer that includes a fixed number of recent keystrokes, including those belonging to previous words. The second algorithm includes the complete previous word as an extra input feature. In simulation studies, the first algorithm yields marked improvements in keystroke savings, but has a large memory footprint. The second algorithm can be tuned by frequency thresholding to have a small footprint, and be less than one order of magnitude slower than the baseline system, while its keystroke savings improve over the baseline.},
author = {van den Bosch, Antal and Bogers, Toine},
doi = {10.1145/1409240.1409315},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van den Bosch, Bogers - 2008 - Efficient context-sensitive word completion for mobile devices(3).pdf:pdf},
isbn = {9781595939524},
journal = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services - MobileHCI '08},
keywords = {con-,ergonomics,mobile devices,predictive text processing,scaling,text sensitivity,word completion},
pages = {465},
title = {{Efficient context-sensitive word completion for mobile devices}},
url = {http://portal.acm.org/citation.cfm?doid=1409240.1409315},
year = {2008}
}
@article{Copestake1997,
abstract = {Current communication devices designed$\backslash$nfor non-speaking users are inadequate to$\backslash$nsupport conversation because the speed$\backslash$nwith which a user can input information is typically very limited. We describe some practical work on word prediction, and discuss its limitations as a technique for speeding up free text entry. We then outline an alternative approach, currently under development, which combines prediction with a constrained technique for natural language generation.},
author = {Copestake, Ann},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Copestake - 1997 - Augmented and alternative NLP techniques for augmentative and alternative communication.pdf:pdf},
journal = {Proceedings of the ACL workshop on Natural Language Processing for Communication Aids},
pages = {37--42},
title = {{Augmented and alternative NLP techniques for augmentative and alternative communication}},
url = {http://anthology.aclweb.org/W/W97/W97-0506.pdf http://acl.ldc.upenn.edu/W/W97/W97-0506.pdf},
year = {1997}
}
@misc{Von2016,
author = {von Ahn, Luis},
title = {{Carnegie Mellon University - List of Profane Words}},
url = {https://www.cs.cmu.edu/{~}biglou/resources/bad-words.txt},
year = {2016}
}
@inproceedings{How2005a,
abstract = {1},
author = {How, Yijue and Kan, Min-Yen},
booktitle = {Proceedings of Human Computer Interfaces International},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/How, Kan - 2005 - Optimizing predictive text entry for short message service on mobile phones(2).pdf:pdf},
pages = {1--10},
title = {{Optimizing predictive text entry for short message service on mobile phones}},
year = {2005}
}
@article{Toutanova2003,
abstract = {We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) ﬁne-grained modeling of unknown word features. Using these ideas together, the resulting tagger gives a 97.24{\%} accuracy on the Penn Treebank WSJ, an error reduction of 4.4{\%} on the best previous single automatically learned tagging result},
author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D},
doi = {10.3115/1073445.1073478},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Toutanova, Klein, Manning - 2003 - Feature-rich part-of-speech tagging with a cyclic dependency network.pdf:pdf},
journal = {In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL '03),},
pages = {252--259},
title = {{Feature-rich part-of-speech tagging with a cyclic dependency network}},
url = {http://dl.acm.org/citation.cfm?id=1073478},
year = {2003}
}
@article{Mandelbrot1953,
author = {Mandelbrot, Benoit},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mandelbrot - 1953 - An Informational Theory of the Statistical Structure of Language(2).pdf:pdf},
journal = {Communication Theory},
number = {2},
pages = {486--502},
title = {{An Informational Theory of the Statistical Structure of Language}},
url = {http://www.uvm.edu/pdodds/files/papers/others/1953/mandelbrot1953a.pdf},
year = {1953}
}
@article{Hsu2008,
abstract = {Despite the availability of better performing techniques, most language models are trained using popular toolkits that do not support perplexity optimization. In this work, we present an efficient data structure and optimized algorithms specifically designed for iterative parameter tuning. With the resulting implementation, we demonstrate the feasibility and effectiveness of such iterative techniques in language model estimation. Index Terms: language modeling, smoothing, interpolation 1.},
author = {Hsu, Bo-June and Glass, James},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu, Glass - 2008 - Iterative language model estimation efficient data structure {\&} algorithms(2).pdf:pdf},
issn = {19909772},
journal = {Proceedings of Interspeech},
keywords = {Index Terms,interpolation,language modeling,smoothing},
pages = {1--4},
title = {{Iterative language model estimation: efficient data structure {\&} algorithms}},
url = {http://people.csail.mit.edu/jrg/2008/paul-interspeech08.pdf},
volume = {8},
year = {2008}
}
@article{Katz1987,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 2572004 Estimation the recognizer Article Source : CiteSeer CITATIONS 37 READS 653 4 , including : Some : Callsurf Quaero Lori French 342 , 990 SEE G . Adda French 161 , 604 SEE All . The . All - text and , letting .Abstract-Thedescriptionofanoveltypeofrn-gramlanguagemodelisgiven.Themodeloffers,viaanonlinearrecursiveprocedure,acom-putationandspaceefficientsolutiontotheproblemofestimatingprob-abilitiesfromsparsedata.Thissolutioncomparesfavorablytootherproposedmethods.Whilethemethodhasbeendevelopedforandsuc-cessfullyimplementedintheIBMRealTimeSpeechRecognizers,itsgeneralitymakesitapplicableinotherareaswheretheproblemofes-timatingprobabilitiesfromsparsedataarises.Sparsenessofdataisaninherentpropertyofanyrealtext,anditisaproblemthatonealwaysencounterswhilecollectingfre-quencystatisticsonwordsandwordsequences(m-grams)fromatextoffinitesize.Thismeansthatevenforaverylargedatacol-lection,themaximumlikelihoodestimationmethoddoesnotallowustoadequatelyestimateprobabilitiesofrarebutneverthelesspos-siblewordsequences-manysequencesoccuronlyonce("single-tons");manymoredonotoccuratall.Inadequacyofthemaximumlikelihoodestimatorandthenecessitytoestimatetheprobabilitiesofm-gramswhichdidnotoccurinthetextconstitutetheessenceoftheproblem.Themainideaoftheproposedsolutiontotheproblemistore-duceunreliableprobabilityestimatesgivenbytheobservedfre-quenciesandredistributethe"freed"probability"mass"amongm-gramswhichneveroccurredinthetext.Thereductionisachievedbyreplacingmaximumlikelihoodestimatesform-gramshavinglowcountswithrenormalizedTuring'sestimates[l],andthere-distributionisdoneviatherecursiveutilizationoflowerlevelcon-ditionaldistributions.WefoundTuring'smethodattractivebe-causeofitssimplicityanditscharacterizationastheoptimalempiricalBayes'estimatorofamultinomialprobability.Robbinsin[2]introducestheempiricalBayes'methodologyandNadasin[3]givesvariousderivationsoftheTuring'sformula.LetNbeasampletextsizeandletn,bethenumberofwords(m-grams)whichoccurredinthetextexactlyrtimes,sothatN=Crn,.},
author = {Katz, Slavam and Lamel, Lori and Adda, G},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz, Lamel, Adda - 1987 - Estimation of probabilities from sparse data for the language model component of a speech recognizer(2).pdf:pdf},
issn = {0096-3518},
journal = {IEEE transactions on acoustics, speech, and signal processing},
number = {3},
pages = {400--401},
title = {{Estimation of probabilities from sparse data for the language model component of a speech recognizer}},
url = {https://www.researchgate.net/profile/Lori{\_}Lamel/publication/2572004{\_}Estimation{\_}of{\_}probabilities{\_}from{\_}Sparse{\_}data{\_}for{\_}the{\_}language{\_}model{\_}component{\_}of{\_}a{\_}speech{\_}recognizer/links/5422cdc10cf26120b7a55d60.pdf},
volume = {35},
year = {1987}
}
@article{Kasesniemi2002,
author = {Kasesniemi, Eija-Liisa and Rautianen, Pirjo},
title = {{Mobile culture of children and teenagers in Finland}},
year = {2002}
}
@article{Pecina,
author = {Pecina, Pavel},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pecina - Unknown - LEXICAL ASSOCIATION MEASURES Collocation Extraction(2).pdf:pdf},
title = {{LEXICAL ASSOCIATION MEASURES Collocation Extraction}}
}
@misc{Services2014,
author = {Services, Nash Information},
booktitle = {Nash Information Services, LLC},
title = {{The Numbers: Where Data and the Movie Business Meet}},
url = {http://www.the-numbers.com/},
urldate = {2017-11-22},
year = {2014}
}
@misc{Ponset,
author = {Ponset, P.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ponset - Unknown - Package 'modeest'.pdf:pdf},
title = {{Package 'modeest'}},
url = {https://cran.r-project.org/web/packages/modeest/modeest.pdf}
}
@article{Agarwal2007b,
abstract = {The use of digital mobile phones has led to a tremendous increase in communication using SMS. On a phone keypad, multiple words are mapped to same numeric code. We propose a Context Based Word Prediction system for SMS messaging in which context is used to predict the most appropriate word for a given code. We extend this system to allow informal words (short forms for proper English words). The mapping from informal word to its proper English words is done using Double Metaphone Encoding based on their phonetic similarity. The results show 31{\%} improvement over the traditional frequency based word estimation. Introduction The growth of wireless technology has provided us with many new ways of communication such as SMS (Short Message Service). SMS messaging can also be used to interact with automated systems or participating in contests. With tremendous increase in Mobile Text Messaging, there is a need for an efficient text input system. With limited keys on the mobile phone, multiple letters are mapped to same number (8 keys, 2 to 9, for 26 alphabets). The many to one mapping of alphabets to numbers gives us same numeric code for multiple words.},
author = {Agarwal, Sachin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal - 2007 - Context based word prediction for texting language.pdf:pdf},
journal = {Large Scale Semantic Access to Content (Text,},
pages = {360--368},
title = {{Context based word prediction for texting language}},
url = {http://dl.acm.org/citation.cfm?id=1931426},
year = {2007}
}
@inproceedings{Komatsu2005a,
author = {Komatsu, Hiroyuki and Takabayashi, Satoru and Masui, Toshiyuki},
booktitle = {Proceedings of the 2005 International Conference on Active Media Technology, AMT 2005},
doi = {10.1109/AMT.2005.1505271},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Komatsu, Takabayashi, Masui - 2005 - Corpus-based predictive text input(3).pdf:pdf},
isbn = {0780390350},
title = {{Corpus-based predictive text input}},
year = {2005}
}
@article{Siivola2007a,
abstract = {— -gram models are the most widely used language models in large vocabulary continuous speech recognition. Since the size of the model grows rapidly with respect to the model order and available training data, many methods have been proposed for pruning the least relevant -grams from the model. However, correct smoothing of the -gram probability distri-butions is important and performance may degrade significantly if pruning conflicts with smoothing. In this paper, we show that some of the commonly used pruning methods do not take into account how removing an -gram should modify the backoff distributions in the state-of-the-art Kneser–Ney smoothing. To solve this problem, we present two new algorithms: one for pruning Kneser–Ney smoothed models, and one for growing them incrementally. Experiments on Finnish and English text corpora show that the proposed pruning algorithm provides considerable improvements over previous pruning algorithms on Kneser–Ney-smoothed models and is also better than the baseline entropy pruned Good–Turing smoothed models. The models created by the growing algorithm provide a good starting point for our pruning algorithm, leading to further improvements. The improvements in the Finnish speech recognition over the other Kneser–Ney smoothed models are statistically significant, as well.},
author = {Siivola, Vesa and Hirsim{\"{a}}ki, Teemu and Virpioja, Sami},
doi = {10.1109/TASL.2007.896666},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Siivola, Hirsim{\"{a}}ki, Virpioja - 2007 - On Growing and Pruning Kneser – Ney Smoothed N -Gram Models.pdf:pdf},
issn = {15587916},
journal = {IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
keywords = {Index Terms—Modeling,Modeling,Natural languages,Smoothing methods,Speech recognition,natural languages,smoothing methods,speech recognition},
number = {5},
pages = {1617--1624},
title = {{On Growing and Pruning Kneser – Ney Smoothed N -Gram Models}},
url = {https://d9b0b70b-a-62cb3a1a-s-sites.googlegroups.com/site/vesassiivola/publications/TASLP2007.pdf?attachauth=ANoY7co{\_}yL6A2AcUftxT0UElk2lwJzg6GS5qtDIzthvhtLroCrvE0o{\_}T2nJQ9inyZ-0cQ2tsXoCCqDrabPuADszWxeHzt8{\_}DtnYcC4a-kfFeuF1lJe7DWYH3YYEg0xl09utrk1hkg5SREJu6DX},
volume = {15},
year = {2007}
}
@article{Egede2007,
abstract = {Objective: The objective of this study was to determine the prevalence and odds of major depression and the incremental effect of major depression on utilization, lost productivity and functional disability in individuals with common chronic medical disorders. Method: Data on 30,801 adults from the 1999 National Health Interview Survey were analyzed. The 12-month prevalence and age/sex-adjusted odds of major depression were calculated for adults with hypertension (HTN), diabetes mellitus (DM), coronary artery disease (CAD), congestive heart failure (CHF), stroke or cerebrovascular accident (CVA), chronic obstructive pulmonary disease (COPD) and end-stage renal disease (ESRD). The association between chronic condition status (with and without major depression) and utilization, lost productivity and functional disability was determined by controlling for covariates. Results: The 12-month prevalence and age/sex-adjusted odds of major depression by chronic conditions were as follows: CHF, 7.9{\%} [odds ratio (OR)=1.96]; HTN, 8.0{\%} (OR=2.00); DM, 9.3{\%} (OR=1.96); CAD, 9.3{\%} (OR=2.30); CVA, 11.4{\%} (OR=3.15); COPD, 15.4{\%} (OR=3.21); ESRD, 17.0{\%} (OR=3.56); any chronic condition, 8.8{\%} (OR=2.61). Compared to adults without chronic conditions, those with chronic conditions plus major depression had greater odds of ???1 ambulatory visit [OR=1.50; 95{\%} confidence interval (95{\%} CI)=1.28, 1.77]; ???1 emergency room visit (OR=1.94; 95{\%} CI=1.55, 2.45); and ???1 day in bed due to illness (OR=1.60; 95{\%} CI=1.28, 2.00); and functional disability (OR=2.48; 95{\%} CI=1.96, 3.15). Conclusion: The 12-month prevalence and odds of major depression are high in individuals with chronic medical conditions, and major depression is associated with significant increases in utilization, lost productivity and functional disability. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Egede, Leonard E.},
doi = {10.1016/j.genhosppsych.2007.06.002},
isbn = {0163-8343 (Print)$\backslash$r0163-8343 (Linking)},
issn = {01638343},
journal = {General Hospital Psychiatry},
keywords = {Chronic medical disorders,Major depression,National Health Interview Survey},
month = {sep},
number = {5},
pages = {409--416},
pmid = {17888807},
title = {{Major depression in individuals with chronic medical disorders: prevalence, correlates and association with health resource utilization, lost productivity and functional disability}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0163834307001260},
volume = {29},
year = {2007}
}
@article{Evert2005,
abstract = {You shall know a word by the company it keeps! With this slogan, Firth (1957) drew attention to a fact that language scholars had intuitively known for a long time: In natural language, words are not combined randomly into phrases and sentences, con- strained only by the rules of syntax. The particular ways in which they go together are a rich and important source of information both about language and about the world we live in. In the 1930s, J. R. Firth coined the term collocations for such char- acteristic, or habitual word combinations (as he called them). While Firth used to be lamentably vague about his precise understanding of this concept (cf. Lehr 1996, 21), the term itself and the general idea behind it that collocations correspond to some conventional way of saying things (Manning and Sch{\"{u}}tze 1999, 151) were eagerly taken up by researchers in various fields, leading to the serious terminolog- ical confusion that surrounds the concept of collocations today. As Choueka puts it: even though any two lexicographers would agree that once upon a time, hit the road and similar idioms are collocations, they would most certainly disagree on al- most anything else (Choueka 1988, 4). Feel free to replace lexicographers with any profession that is concerned with language data.},
author = {Evert, Stefan},
doi = {10.1073/pnas.141413598},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/der Philosophisch-Historischen et al. - 2004 - The Statistics of Word Cooccurrences Word Pairs and Collocations(2).pdf:pdf},
isbn = {0027-8424 (Print)},
issn = {00278424},
journal = {Unpublished doctoral dissertation Institut fur maschinelle Sprachverarbeitung Universitat Stuttgart},
number = {August 2004},
pages = {353},
pmid = {11447261},
title = {{The Statistics of Word Cooccurrences Word Pairs and Collocations}},
url = {http://en.scientificcommons.org/19948039},
volume = {98},
year = {2005}
}
@article{Agarwal2007a,
abstract = {The use of digital mobile phones has led to a tremendous increase in communication using SMS. On a phone keypad, multiple words are mapped to same numeric code. We propose a Context Based Word Prediction system for SMS messaging in which context is used to predict the most appropriate word for a given code. We extend this system to allow informal words (short forms for proper English words). The mapping from informal word to its proper English words is done using Double Metaphone Encoding based on their phonetic similarity. The results show 31{\%} improvement over the traditional frequency based word estimation. Introduction The growth of wireless technology has provided us with many new ways of communication such as SMS (Short Message Service). SMS messaging can also be used to interact with automated systems or participating in contests. With tremendous increase in Mobile Text Messaging, there is a need for an efficient text input system. With limited keys on the mobile phone, multiple letters are mapped to same number (8 keys, 2 to 9, for 26 alphabets). The many to one mapping of alphabets to numbers gives us same numeric code for multiple words.},
author = {Agarwal, Sachin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal - 2007 - Context based word prediction for texting language.pdf:pdf},
journal = {Large Scale Semantic Access to Content (Text,},
pages = {360--368},
title = {{Context based word prediction for texting language}},
url = {http://dl.acm.org/citation.cfm?id=1931426},
year = {2007}
}
@article{Wasserman1996,
author = {Wasserman, Larry and Kass, Robert E.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wasserman, Kass - 1996 - The Selection of Prior Distributions by Formal Rules.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {435},
pages = {1343--1370},
title = {{The Selection of Prior Distributions by Formal Rules}},
url = {http://mathfaculty.fullerton.edu/sbehseta/KassWasserman-JASA-1996.pdf},
volume = {91},
year = {1996}
}
@misc{EatonJohn2017,
author = {{Eaton, John}, W.},
doi = {10.3169/itej.65.790},
issn = {1342-6907},
title = {{Octave}},
url = {https://www.gnu.org/software/octave/ http://ci.nii.ac.jp/naid/110009669121/ http://jlc.jst.go.jp/DN/JST.JSTAGE/itej/65.790?lang=en{\&}from=CrossRef{\&}type=abstract},
year = {2017}
}
@article{Gelman2008,
abstract = {We propose a new prior distribution for classical (nonhierarchical) lo-gistic regression models, constructed by first scaling all nonbinary variables to have mean 0 and standard deviation 0.5, and then placing independent Student-t prior distributions on the coefficients. As a default choice, we recommend the Cauchy distribution with center 0 and scale 2.5, which in the simplest setting is a longer-tailed version of the distribution attained by assuming one-half additional success and one-half additional failure in a logistic regression. Cross-validation on a corpus of datasets shows the Cauchy class of prior distributions to outperform existing implementations of Gaussian and Laplace priors. We recommend this prior distribution as a default choice for routine ap-plied use. It has the advantage of always giving answers, even when there is complete separation in logistic regression (a common problem, even when the sample size is large and the number of predictors is small), and also au-tomatically applying more shrinkage to higher-order interactions. This can be useful in routine data analysis as well as in automated procedures such as chained equations for missing-data imputation. We implement a procedure to fit generalized linear models in R with the Student-t prior distribution by incorporating an approximate EM algorithm into the usual iteratively weighted least squares. We illustrate with several applications, including a series of logistic regressions predicting voting pref-erences, a small bioassay experiment, and an imputation model for a public health data set.},
author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
doi = {10.1214/08-AOAS191},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman et al. - 2008 - A WEAKLY INFORMATIVE DEFAULT PRIOR DISTRIBUTION FOR LOGISTIC AND OTHER REGRESSION MODELS.pdf:pdf},
journal = {The Annals of Applied Statistics},
keywords = {Bayesian inference,generalized linear model,hierarchical model,least squares,linear regression,logistic regression,multilevel model,noninformative prior distribution,weakly informative prior distribution},
number = {4},
pages = {1360--1383},
title = {{A WEAKLY INFORMATIVE DEFAULT PRIOR DISTRIBUTION FOR LOGISTIC AND OTHER REGRESSION MODELS}},
url = {http://www.stat.columbia.edu/{~}gelman/research/published/priors11.pdf},
volume = {2},
year = {2008}
}
@book{Pecina2009,
author = {Pecina, Pavel},
booktitle = {Studies in computational and theoretical linguistics},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pecina - Unknown - LEXICAL ASSOCIATION MEASURES Collocation Extraction(2).pdf:pdf},
isbn = {9788090417557},
title = {{Collocation Extraction AND THEORETICAL LINGUISTICS}},
year = {2009}
}
@article{Arnold2016a,
author = {Arnold, Taylor and Tilton, Lauren},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arnold, Tilton - 2016 - Package 'coreNLP' Wrappers Around Stanford CoreNLP Tools(2).pdf:pdf},
title = {{Package 'coreNLP' | Wrappers Around Stanford CoreNLP Tools}},
year = {2016}
}
@techreport{Low2016,
abstract = {Text prediction is an application of language models to mobile devices. Currently, the state of the art models use neural networks. Unfortunately, mobile devices are constrainted in both computing power and space and are thus unable to run most (if not all) neural networks. Recently, however, character-level architectures have appeared that have outperformed previous architectures for machine transla-tion. They are advantageous in that they do not require a word embedding matrix and thus require a lot less space. This project evaluates one recent character-level architecture on the text prediction task. We find that the network performs qual-itatively well, as well as achieving perplexity levels close to existing methods on the Brown corpus.},
author = {Low, Melvin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Low - 2016 - Character-level Recurrent Text Prediction.pdf:pdf},
pages = {1--6},
title = {{Character-level Recurrent Text Prediction}},
year = {2016}
}
@article{Ling2007,
abstract = {Abstract While instant messaging ( IM ) via computers is well entrenched in the United States, text messaging on mobile phones is a more recent technology in America . To investigate the emergence of American texting, this study compared text messages and IMs produced by ... $\backslash$n},
author = {Ling, Rich and Baron, Naomi S},
doi = {10.1177/0261927X06303480},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling, Baron, R{\&}d - 2007 - Text Messaging and IM Linguistic Comparison of American College Data(2).pdf:pdf},
isbn = {3012294988},
issn = {0261-927X},
journal = {Journal of Language and Social Psychology},
number = {3},
pages = {291--298},
pmid = {17926767},
title = {{Text Messaging and IM Linguistic Comparison of American College Data}},
url = {http://jls.sagepub.com/cgi/doi/10.1177/0261927X06303480{\%}5Cnpapers3://publication/doi/10.1177/0261927X06303480},
volume = {26},
year = {2007}
}
@misc{M.eikmichalke2016,
annote = {From Duplicate 1 (koRpus: An R Package for Text Analysis - m.eik michalke)

(Version 0.06-5)},
author = {eik Michalke, M. and M.eik michalke},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Michalke - 2015 - koRpus An R Package for Text Analysis.pdf:pdf},
title = {{koRpus: An R Package for Text Analysis}},
url = {http://reaktanz.de/?c=hacking{\&}s=koRpus},
year = {2016}
}
@article{Geza2002,
abstract = {The practical challenge of creating a Hungarian e-mail reader has initiated our work on statistical text analysis. The starting point was statistical analysis for automatic discrimination of the language of texts. Later it was extended to automatic re-generation of diacritic signs and more detailed language structure analysis. Parallel study of three different languages -Hungarian. German and English -using text corpora of similar size explores both similarities and differences. Corpora of publicly available Internet sources were used. The corpus size was the same (approximately 20Mbytes, 2.5-3.5 million word forms) for all languages. Besides traditional corpus coverage, word length and occurence statistics, some new features about prosodic boundaries (sentence beginning and final positions, preceding and following a comma) were also computed. Among others, it was found, that the coverage of corpora by the most frequent words follows a parallel logarithmic rule for all languages in the 40-85{\%} coverage range, known as Zipf's law in linguistics. The functions are much nearer for English and German than for Hungarian. Further conclusions are also drawn. The language detection and diacritic re-generation applications are discussed in detail with implications on Hungarian speech generation. Diverse further application domains, such as predictive text input, word hyphenation, language modeling in speech recognition, corpus-based speech synthesis, etc. are also foreseen.},
author = {Geza, Nemeth and {Csaba Zainko}},
doi = {10.1556/ALing.49.2002.3-4.8},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Geza, Csaba Zainko - 2002 - Multilingual Statistical Text Analysis, Zipf's Law and Hungarian Speech Generation(2).pdf:pdf},
issn = {1216-8076},
journal = {Corpus},
keywords = {Zipf's law,corpus analysis,corpus-based speech synthesis,language modeling,multilinguality,text corpora,word length},
number = {2001},
pages = {3--4},
title = {{Multilingual Statistical Text Analysis, Zipf's Law and Hungarian Speech Generation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.679{\&}rep=rep1{\&}type=pdf},
volume = {49},
year = {2002}
}
@article{Carlberger1997,
author = {Carlberger, J.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlberger - 1997 - Design and Implementation of a Probabilistic Word Prediciton Program(3).pdf:pdf},
journal = {Language},
title = {{Design and Implementation of a Probabilistic Word Prediciton Program}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.2073},
year = {1997}
}
@book{Taylor2015,
author = {Taylor, John},
pages = {93},
title = {{The Oxford Handbook of the Word}},
year = {2015}
}
@article{Wagacha,
author = {Wagacha, Peter Waiganjo and Chege, Dennis},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wagacha, Chege - Unknown - Adaptive and Optimization Predictive Text Entry for Short Message Service (Sms)(2).pdf:pdf},
journal = {Review Literature And Arts Of The Americas},
title = {{Adaptive and Optimization Predictive Text Entry for Short Message Service (Sms)}}
}
@misc{Hasselgren2003a,
abstract = {Due to the emergence of SMS messages, the significance of effective text entry on limited-size keyboards has increased. In this paper, we describe and discuss a new method to enter text more efficiently using a mobile telephone keyboard. This method, which we called HMS, predicts words from a sequence of keystrokes using a dictionary and a function combining bigram frequencies and word length. We implemented the HMS text entry method on a software-simulated mobile telephone keyboard and we...},
author = {Hasselgren, Jon and Montnemery, E},
booktitle = {{\ldots} for Text Entry Methods},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasselgren, Montnemery - 2003 - Hms A predictive text entry method using bigrams(2).pdf:pdf},
pages = {43--50},
title = {{Hms: A predictive text entry method using bigrams}},
url = {http://dl.acm.org/citation.cfm?id=1628201},
year = {2003}
}
@misc{Silvola,
author = {Silvola, Vesa},
title = {{VariKN toolkit}},
url = {http://vsiivola.github.io/variKN/},
urldate = {2017-04-03}
}
@article{Good1953,
abstract = {A random sample is drawn from a population of animals of various species. (The theory may also be applied to studies of literary vocabulary, for example.) If a particular species is represented r times in the sample of size N, then r/N is not a good estimate of the population frequency, p, when r is small. Methods are given for estimating p, assuming virtually nothing about the underlying population. The estimates are expressed in terms of smoothed values of the numbers nr (r= 1, 2, 3, ...), where nr is the number of distinct species that are each represented r times in the sample. (nr may be described as ‘the frequency of the frequency r'.) Turing is acknowledged for the most interesting formula in this part of the work. An estimate of the proportion of the population represented by the species occurring in the sample is an immediate corollary. Estimates are made of measures of heterogeneity of the population, including Yule's ‘characteristic' and Shannon's ‘entropy'. Methods are then discussed that do depend on assumptions about the underlying population. It is here that most work has been done by other writers. It is pointed out that a hypothesis can give a good fit to the numbers nr but can give quite the wrong value for Yule's characteristic. An example of this is Fisher's fit to some data of Williams's on Macrolepidoptera.},
author = {Good, I.J.},
doi = {10.1093/biomet/40.3-4.237},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Good - 1953 - The population frequencies of spiecies and the estimation of population parameters.pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
number = {3-4},
pages = {237--264},
title = {{The population frequencies of spiecies and the estimation of population parameters}},
url = {http://links.jstor.org/sici?sici=0006-3444{\%}28195312{\%}2940{\%}3A3{\%}2F4{\%}3C237{\%}3ATPFOSA{\%}3E2.0.CO{\%}3B2-K https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/40.3-4.237 http://biomet.oxfordjournals.org/content/40/3-4/237.abstract},
volume = {40},
year = {1953}
}
@book{Plag2003,
abstract = {This textbook provides an accessible introduction to the study of word-formation, focusing specifically on English. Assuming no prior linguistic knowledge, it explains the fundamentals of word-formation, encouraging students to undertake their own morphological analyses of English words, and familiarising them with the methodological tools to obtain and analyse relevant data. Cover; Half-title; Series-title; Title; Copyright; Contents; Preface; Abbreviations and notational conventions; Introduction: what this book is about and how it can be used; 1 Basic concepts; 2 Studying complex words; 3 Productivity and the mental lexicon; 4 Affixation; 5 Derivation without affixation; 6 Compounding; 7 Theoretical issues: modeling word-formation; Answer key to exercises; References; Subject index; Affix index; Author index.},
address = {Cambridge},
author = {Plag, Ingo.},
isbn = {0521525632},
pages = {240},
publisher = {Cambridge University Press},
title = {{Word-formation in English}},
year = {2003}
}
@misc{Leek,
abstract = {One of the most common tasks performed by data scientists and data analysts are prediction and machine learning. This course will cover the basic components of building and applying prediction functions with an emphasis on practical applications. The course will provide basic grounding in concepts such as training and tests sets, overfitting, and error rates. The course will also introduce a range of model based and algorithmic machine learning methods including regression, classification trees, Naive Bayes, and random forests. The course will cover the complete process of building prediction functions including data collection, feature creation, algorithms, and evaluation.},
author = {Leek, Jeff PhD and Peng, Roger D. PhD and {Caffo, Brian}, PhD},
title = {{Coursera | Practical Machine Learning | Data Science Specialization by Johns Hopkins University}},
url = {https://www.coursera.org/learn/practical-machine-learning},
urldate = {2017-09-08}
}
@article{Ghayoomi,
abstract = {Word prediction is the problem of guessing the words which are likely to follow in a given text segment by displaying a list of the most probable words that could appear in that position. In this research, we designed and implemented three word predictors for Persian. Our baseline is a statistical-based system which uses language models. The first system uses word statistics; in the second one we use the main syntactic categories of a Persian POS tagged corpus; and the last one uses the main syntactic categories along with their morphological, syntactic and semantic subcategories. Using KeyStroke Saving (KSS) as the most important metrics to evaluate systems' performance, the primary word-based statistical system achieved 37{\%} KSS, and the second system that used only the main syntactic categories with word-statistics achieved 38.95{\%} KSS. Our last system which used all of the available information to the words get the best result by 42.45{\%} KSS.},
author = {Ghayoomi, Masood and Daroodi, Ehsan},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Daroodi - Unknown - A POS-based Word Prediction System for the Persian Language(2).pdf:pdf},
keywords = {POS tagging,statistical language modeling,word prediction},
title = {{A POS-based Word Prediction System for the Persian Language}}
}
@article{PettinatiHM2011,
author = {{Pettinati HM}, Dundon WD},
journal = {Psychiatric Times},
title = {{Comorbid Depression and Alcohol Dependence}},
url = {http://www.psychiatrictimes.com/major-depressive-disorder/comorbid-depression-and-alcohol-dependence/page/0/1},
year = {2011}
}
@article{Arnold2016,
archivePrefix = {arXiv},
arxivId = {0712.0689},
author = {Arnold, Jeffrey B.},
doi = {10.1007/978-1-61779-968-6},
eprint = {0712.0689},
isbn = {9780470057247},
issn = {00335770},
pmid = {18963060},
title = {{Introduction To ggthemes}},
url = {https://cran.r-project.org/web/packages/ggthemes/vignettes/ggthemes.html},
year = {2016}
}
@article{Matiasek2002,
author = {Matiasek, Johannes and Baroni, Marco and Trost, Harald},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matiasek, Baroni, Trost - 2002 - FASTY—A multi-lingual approach to text prediction(2).pdf:pdf},
isbn = {3-540-43904-8},
journal = {Computers Helping People with Special {\ldots}},
number = {c},
pages = {243--250},
title = {{FASTY—A multi-lingual approach to text prediction}},
url = {http://link.springer.com/chapter/10.1007/3-540-45491-8{\_}51},
volume = {2398},
year = {2002}
}
@article{VandenBosch2008a,
abstract = {Word completion is a basic technology for reducing the effort involved in text entry on mobile devices and in augmentative communication devices, where efficiency and ease of use are needed, but where a low memory footprint is also required. Standard solutions compress a lexicon into a suffix tree with a small memory footprint and high retrieval speed. Keystroke savings, a measurable correlate of text entry effort gain, typically improve when the algorithm would also take into account the previous word; however, this comes at the cost of a large footprint. We develop two word completion algorithms that encode the previous word in the input. The first algorithm utilizes a character buffer that includes a fixed number of recent keystrokes, including those belonging to previous words. The second algorithm includes the complete previous word as an extra input feature. In simulation studies, the first algorithm yields marked improvements in keystroke savings, but has a large memory footprint. The second algorithm can be tuned by frequency thresholding to have a small footprint, and be less than one order of magnitude slower than the baseline system, while its keystroke savings improve over the baseline.},
author = {van den Bosch, Antal and Bogers, Toine},
doi = {10.1145/1409240.1409315},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van den Bosch, Bogers - 2008 - Efficient context-sensitive word completion for mobile devices(3).pdf:pdf},
isbn = {9781595939524},
journal = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services - MobileHCI '08},
keywords = {con-,ergonomics,mobile devices,predictive text processing,scaling,text sensitivity,word completion},
pages = {465},
title = {{Efficient context-sensitive word completion for mobile devices}},
url = {http://portal.acm.org/citation.cfm?doid=1409240.1409315},
year = {2008}
}
@book{Yule1944,
author = {Yule, George Udny},
pages = {306},
publisher = {Cambridge University Press},
title = {{The Statistical Study of Literary Vocabulary}},
year = {1944}
}
@article{Tagg2009a,
author = {Tagg, Caroline},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tagg - 2009 - A CORPUS LINGUISTICS STUDY OF SMS TEXT MESSAGING(2).pdf:pdf},
title = {{A CORPUS LINGUISTICS STUDY OF SMS TEXT MESSAGING}},
year = {2009}
}
@inproceedings{Talbot2007,
abstract = {A Bloom filter (BF) is a randomised data structure for set membership queries. Its space requirements fall significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability. Here we present a general framework for deriving smoothed language model probabilities from BFs. We investigate how a BF containing n-gram statistics can be used as a direct replacement for a conventional n-gram model. Recent work has demonstrated that corpus statistics can be stored efficiently within a BF, here we consider how smoothed language model probabilities can be derived efficiently from this randomised representation. Our proposal takes advantage of the one-sided error guarantees of the BF and simple inequalities that hold between related n-gram statistics in order to further reduce the BF storage requirements and the error rate of the derived probabilities. We use these models as replacements for a conventional language model in machine translation experiments.},
author = {Talbot, David and Osborne, Miles},
booktitle = {EMNLP-2007},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Talbot, Osborne - 2007 - Smoothed Bloom filter language models Tera-scale LMs on the cheap.pdf:pdf},
number = {June},
pages = {468--476},
title = {{Smoothed Bloom filter language models: Tera-scale LMs on the cheap}},
url = {http://homepages.inf.ed.ac.uk/miles/papers/emnlp07.pdf papers2://publication/uuid/6DB50926-4247-433E-9B2D-1C1D3B9B4442},
year = {2007}
}
@article{Lluisa2007,
abstract = {Internet and Information and Communication Technologies represent the largest network of human online communication ever. Language is the material that enables communication to flow in this ever-growing digital world of emails,},
author = {Llu{\"{i}}sa, Santiago Posteguillo、Mar{\'{i}}a Jos{\'{e}} Esteve、M.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - The Texture of Internet(2).pdf:pdf},
isbn = {1-84718-173-2; ISBN 13: 9781847181732},
journal = {Portal.Acm.Org},
pages = {235},
title = {{The Texture of Internet: Netlinguistics in Progress}},
url = {http://www.cambridgescholars.com/download/sample/61182},
year = {2007}
}
@article{Health2010,
abstract = {Major Depression Among Adults},
author = {Health, Mental and States, United},
journal = {Health and Education},
pages = {2010--2012},
title = {{Major Depression Among Adults}},
url = {https://www.nimh.nih.gov/health/statistics/prevalence/major-depression-among-adults.shtml http://www.ncbi.nlm.nih.gov/pubmed/},
year = {2010}
}
@misc{wiki:supervisedlearning,
annote = {[Online; accessed 
8-September-2017
]},
author = {Wikipedia},
title = {{Supervised learning --- Wikipedia{\{},{\}} The Free Encyclopedia}},
url = {https://en.wikipedia.org/w/index.php?title=Supervised{\_}learning{\&}oldid=791408094},
year = {2017}
}
@misc{Wickham2016,
author = {Wickham, Hadley},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham - 2016 - R Package 'ggplot2' Create Elegant Data Visualisations Using the Grammar of Graphics.pdf:pdf},
title = {{R Package 'ggplot2' | Create Elegant Data Visualisations Using the Grammar of Graphics}},
year = {2016}
}
@article{DeJonge2013,
abstract = {Data cleaning, or data preparation is an essential part of statistical analysis. In fact, in practice it is often more time-consuming than the statistical analysis itself. These lecture notes describe a range of techniques, implemented in the R statistical environment, that allow the reader to build data cleaning scripts for data suffering from a wide range of errors and inconsistencies, in textual format. These notes cover technical as well as subject-matter related aspects of data cleaning. Technical aspects include data reading, type conversion and string matching and manipulation. Subject-matter related aspects include topics like data checking, error localization and an introduction to imputation methods in R. References to relevant literature and R packages are provided throughout.},
author = {de Jonge, Edwin and van der Loo, Mark},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de Jonge, van der Loo - 2013 - An introduction to data cleaning with R(2).pdf:pdf},
isbn = {1572-0314},
journal = {Statistics Netherlands},
keywords = {data editing,methodology,statistical software},
pages = {53},
title = {{An introduction to data cleaning with R}},
url = {http://cran.r-project.org/doc/contrib/de{\_}Jonge+van{\_}der{\_}Loo-Introduction{\_}to{\_}data{\_}cleaning{\_}with{\_}R.pdf},
year = {2013}
}
@article{Carlberger1997b,
abstract = {Prolet, a word prediction program, has been in use for the last ten years as a writing aid, and was designed to accelerate the writing process and minimize the writing effort for persons with motor dysfunction.},
author = {Carlberger, Alice and Carlberger, Johan and Magnuson, Tina and Hunnicutt, M Sharon and Palazuelos-cagigas, Sira E and Navarro, Santiago A and Electronica, Ingenieria},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carlberger et al. - 1997 - Profet, A New Generation of Word Prediction.pdf:pdf},
journal = {In Proceedings of the 2nd Workshop on NLP for Communication Aids},
keywords = {word-prediction},
pages = {23--28},
title = {{Profet, A New Generation of Word Prediction:}},
url = {http://ucrel.lancs.ac.uk/acl/W/W97/W97-0504.pdf http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.5847},
year = {1997}
}
@article{Derczynski2013,
abstract = {Part-of-speech information is a pre-requisite in many NLP algorithms. However, Twitter text is difficult to part-of-speech tag: it is noisy, with linguistic errors and idiosyncratic style. We present a detailed error analysis of existing taggers, motivating a series of tagger augmentations which are demonstrated to improve performance. We identify and evaluate techniques for improving English part-of-speech tagging performance in this genre. Further, we present a novel approach to system combination for the case where available taggers use different tagsets, based on voteconstrained bootstrapping with unlabeled data. Coupled with assigning prior probabilities to some tokens and handling of unknown words and slang, we reach 88.7{\%} tagging accuracy (90.5{\%} on development data). This is a new high in PTB-compatible tweet part-of-speech tagging, reducing token error by 26.8{\%} and sentence error by 12.2{\%}. The model, training data and tools are made available.},
author = {Derczynski, Leon and Ritter, Alan and Clark, Sam and Bontcheva, Kalina},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Derczynski et al. - 2013 - Twitter part-of-speech tagging for all Overcoming sparse and noisy data(2).pdf:pdf},
issn = {13138502},
journal = {Proceedings of the Recent Advances in Natural Language Processing},
number = {September},
pages = {198--206},
title = {{Twitter part-of-speech tagging for all: Overcoming sparse and noisy data}},
url = {http://www.aclweb.org/website/old{\_}anthology/R/R13/R13-1026.pdf},
year = {2013}
}
@article{Kornai2002,
abstract = {The commonsensical assumption that any language has only finitely many words is shown to be false by a combination of formal and empirical arguments. Zipf's Law and related formulas are investigated and a more complex model is offered.},
author = {Kornai, Andr{\'{a}}s},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kornai - 2002 - How Many Words Are There(2).pdf:pdf},
issn = {10542353},
journal = {Glottometrics},
keywords = {s law,vocabulary size,zipf},
number = {2002},
pages = {61--86},
pmid = {12374001},
title = {{How Many Words Are There?}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.6.5245{\&}rank=1},
volume = {4},
year = {2002}
}
@article{Zurhellen,
abstract = {This article is one of a series of short essays, collectively titled " Further Explorations, " published as part of a special issue of Oral Tradition in honor of John Miles Foley's 65 th birthday and 2011 retirement. The surprise Festschrift, guest-edited by Lori and Scott Garner entirely without his knowledge, celebrates John's tremendous impact on studies in oral tradition through a series of essays contributed by his students from the University of Missouri-Columbia (1979-present) and from NEH Summer Seminars that he has directed (1987-1996). http://journal.oraltradition.org/issues/26ii In The Pathways Project, John Miles Foley (2011-) discusses briefly the social role of SMS (Short Message Service), suggesting that " even so-called text messaging, a misnomer of sizeable proportions given that the activity really amounts to a long-distance emergent communication enacted virtually, knits people together into interactive groups and keeps them connected and 'present' to one another. " 1 In this essay, I propose a merger of current research on text messaging and the study of oral traditions in order to shed light on the relationship between this new mode of communication and the workings of consciousness being transformed by the eAgora. Focusing first on the limitations of text messaging as a medium that unexpectedly encouraged language innovation, we can explore how text messaging language merges effective communicative practices from both oral and written technologies in order to generate more efficient communication within a newly-limited, writing-based technology. Moreover, in addition to its efficiency, the kind of linguistic play found in text messaging can be viewed as a source of pleasure for those who engage in texting (" texters "). Thus, by employing the discourse of orality and literacy, we can explain how text messaging, while impossible to imagine without the myriad writing technologies mastered before it, actually encourages its literacy-obsessed users to practice communicative techniques more often found within oral cultures, or more precisely, communicative techniques found in cultures in the incipient stages of literacy. Such cultures are ripe for language innovation precisely because they have begun to record knowledge but have not yet standardized the recording procedure. Coincident with a perspective that sees text messaging as bridging a consciousness gap between oral and literate cultures, then, is the recognition that close study of the ways in which text messaging reworks language could lead to fruitful discoveries about the most current ways in which Computer-Mediated Communication (CMC) directs human life toward ever-emerging horizons of consciousness. When David Crystal (2008) hyperbolized the emergence of text messaging in the following passage, this form of communication was already a well-developed medium. Nevertheless, his humorous figuring of text messaging's inception, while not quite accurate, highlights precisely the form's limits that made it such an unlikely competitor in the tightly-wound market of twenty-first-century technologies (173-74): Oral Tradition, 26/2 (2011): 637-624},
annote = {From Duplicate 2 (" A Misnomer of Sizeable Proportions " : SMS and Oral Tradition - Zurhellen, Sarah)

Not primary research},
author = {Zurhellen, Sarah},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zurhellen - Unknown - A Misnomer of Sizeable Proportions SMS and Oral Tradition(2).pdf:pdf},
title = {{" A Misnomer of Sizeable Proportions " : SMS and Oral Tradition}}
}
@inproceedings{How2005,
abstract = {1},
author = {How, Yijue and Kan, Min-Yen},
booktitle = {Proceedings of Human Computer Interfaces International},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/How, Kan - 2005 - Optimizing predictive text entry for short message service on mobile phones(2).pdf:pdf},
pages = {1--10},
title = {{Optimizing predictive text entry for short message service on mobile phones}},
year = {2005}
}
@article{Paperno2016,
abstract = {We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages shar- ing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preced- ing the target word. To succeed on LAM- BADA, computational models cannot sim- ply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA ex- emplifies a wide range of linguistic phe- nomena, and that none of several state-of- the-art language models reaches accuracy above 1{\%} on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the develop- ment of new models capable of genuine understanding of broad context in natural language text.},
archivePrefix = {arXiv},
arxivId = {1606.06031},
author = {Paperno, Denis and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern, Raquel},
eprint = {1606.06031},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paperno et al. - 2016 - The LAMBADA dataset Word prediction requiring a broad discourse context(2).pdf:pdf},
journal = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Berlin, Germany, August 7-12, 2016},
keywords = {Distributional semantics},
pages = {1525--1534},
title = {{The LAMBADA dataset : Word prediction requiring a broad discourse context}},
year = {2016}
}
@article{Greenberg2015,
abstract = {BACKGROUND: The economic burden of depression in the United States--including major depressive disorder (MDD), bipolar disorder, and dysthymia--was estimated at {\$}83.1 billion in 2000. We update these findings using recent data, focusing on MDD alone and accounting for comorbid physical and psychiatric disorders.\backslashn\backslashnMETHOD: Using national survey (DSM-IV criteria) and administrative claims data (ICD-9 codes), we estimate the incremental economic burden of individuals with MDD as well as the share of these costs attributable to MDD, with attention to any changes that occurred between 2005 and 2010.\backslashn\backslashnRESULTS: The incremental economic burden of individuals with MDD increased by 21.5{\%} (from {\$}173.2 billion to {\$}210.5 billion, inflation-adjusted dollars). The composition of these costs remained stable, with approximately 45{\%} attributable to direct costs, 5{\%} to suicide-related costs, and 50{\%} to workplace costs. Only 38{\%} of the total costs were due to MDD itself as opposed to comorbid conditions.\backslashn\backslashnCONCLUSIONS: Comorbid conditions account for the largest portion of the growing economic burden of MDD. Future research should analyze further these comorbidities as well as the relative importance of factors contributing to that growing burden. These include population growth, increase in MDD prevalence, increase in treatment cost per individual with MDD, changes in employment and treatment rates, as well as changes in the composition and quality of MDD treatment services.},
author = {Greenberg, Paul E. and Fournier, Andree-Anne and Sisitsky, Tammy and Pike, Crystal T. and Kessler, Ronald C.},
doi = {10.4088/JCP.14m09298},
isbn = {0160-6689},
issn = {0160-6689},
journal = {The Journal of Clinical Psychiatry},
month = {feb},
pages = {155--162},
pmid = {25742202},
title = {{The Economic Burden of Adults With Major Depressive Disorder in the United States (2005 and 2010)}},
url = {http://www.psychiatrist.com/jcp/article/pages/2015/v76n02/v76n0204.aspx},
year = {2015}
}
@misc{RCoreTeam2013,
abstract = {Maintainer: R Core Team {\textless}R-core@r-project.org{\textgreater} Description: R statistical functions License: Part of R 3.1.1 Built: R 3.1.1; x86{\_}64-apple-darwin10.8.0; 2014-07-11 12:31:14 UTC; unix},
author = {{R Core Team}},
doi = {Version: 3.4.0},
isbn = {3-900051-07-0},
keywords = {anova.glm,anova.lm,confint,glm,lm},
title = {{The R Stats Package}},
url = {https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html},
year = {2013}
}
@article{Crystal,
author = {Crystal, David and Clark, Rykia and Timmons, Taylor and Kanski, Alison and Campbell, Jeanna and Reese, Hanna and Murphy, Katey},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crystal et al. - Unknown - Txting The gr8 db8(2).pdf:pdf},
title = {{Txting: The gr8 db8}}
}
@misc{Xie2014,
author = {Xie, Yihui},
title = {{The printr Package | Automatically Print R Objects According to knitr Output Format}},
url = {http://yihui.name/printr/},
year = {2014}
}
@article{Ormel1994,
author = {Ormel, J.},
doi = {10.1001/jama.272.22.1741},
issn = {00987484},
journal = {JAMA: The Journal of the American Medical Association},
number = {22},
pages = {1741--1748},
title = {{Common mental disorders and disability across cultures. Results from the WHO Collaborative Study on Psychological Problems in General Health Care}},
url = {http://jama.ama-assn.org/cgi/doi/10.1001/jama.272.22.1741},
volume = {272},
year = {1994}
}
@article{Even-zohar1995,
author = {Even-zohar, Yair},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Even-zohar - 1995 - to Word Prediction(2).pdf:pdf},
pages = {124--131},
title = {{to Word Prediction *}},
year = {1995}
}
@misc{mitchell1997machine,
author = {Mitchell, Tom M and Others},
publisher = {McGraw-Hill Boston, MA:},
title = {{Machine learning. WCB}},
year = {1997}
}
@book{Wickham2015,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wickham, Hadley},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickham - 2015 - Advanced R(2).pdf:pdf},
isbn = {978-1-4665-8697-0},
issn = {1098-6596},
number = {9},
pages = {466},
pmid = {25246403},
title = {{Advanced R}},
volume = {53},
year = {2015}
}
@techreport{Baroni2006,
abstract = {This document provides a tutorial introduction to the zipfR package (Evert and Baroni, 2007) for Large-Number-of-Rare-Events (LNRE) modeling of lexical distri- butions (Baayen, 2001). We assume that R is installed on your computer and that you have basic familiarity with it. If this is not the case, please start by visiting the R page at http://www.r-project.org/. The page provides links to download sites, documentation and introductory material, as well as a wide selection of textbooks on R programming.},
author = {Baroni, Marco and Evert, Stefan},
booktitle = {R tutorial},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baroni, Evert - 2006 - The zipfR package for lexical statistics A tutorial introduction.pdf:pdf},
keywords = {R,R GNU},
number = {August},
pages = {1--25},
title = {{The zipfR package for lexical statistics: A tutorial introduction}},
year = {2006}
}
@book{Adger2003,
abstract = {1. Core Concepts -- 2. Morphosyntactic Features -- 3. Constituency and Theta Roles -- 4. Representing Phrase Structure -- 5. Functional Categories I -- TP -- 6. Subjects and Objects -- 7. Functional Categories II -- the DP 8. Functional Categories III -- CP -- 9. Wh-movement -- 10. Locality.},
address = {Oxford},
author = {Adger, David.},
isbn = {0199243700},
pages = {424},
publisher = {Oxford University Press},
title = {{Core syntax : a minimalist approach}},
year = {2003}
}
@misc{Davies2015,
author = {Davies, Mark},
title = {{Corpus of Contemporary American English (COCA)}},
year = {2015}
}
@article{BNC2005,
abstract = {The written part of the (90{\%}) includes, for example, extracts from regional and national newspapers, specialist periodicals and journals for all ages and interests, academic books and popular fiction, published and unpublished letters and memoranda, school and university},
author = {BNC, Webmaster},
isbn = {Version 3, BNC XML Edition},
journal = {British National Corpus},
number = {2001},
pages = {6},
title = {{British National Corpus}},
url = {http://www.natcorp.ox.ac.uk/},
year = {2005}
}
@article{Garay-Vitoria1997,
author = {Garay-Vitoria, Nestor and Gonz{\'{a}}lez-Abascal, Julio},
doi = {10.1145/238218.238333},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garay-Vitoria, Gonz{\'{a}}lez-Abascal - 1997 - Intelligent word-prediction to enhance text input rate (a syntactic analysis-based word-pred(2).pdf:pdf},
isbn = {0897918398},
journal = {Proceedings of the 2nd international conference on Intelligent user interfaces  - IUI '97},
number = {December},
pages = {241--244},
title = {{Intelligent word-prediction to enhance text input rate (a syntactic analysis-based word-prediction aid for people with severe motor and speech disability)}},
url = {http://portal.acm.org/citation.cfm?doid=238218.238333},
year = {1997}
}
@article{Covington2010,
abstract = {Type–token ratio (TTR), or vocabulary size divided by text length (V/N), is a time- honoured but unsatisfactory measure of lexical diversity. The problem is that the TTR of a text sample is affected by its length. We present an algorithm for rapidly computing TTR through a moving window that is independent of text length, and we demonstrate that this measurement can detect changes within a text as well as differences between texts.},
author = {Covington, Michael A. and Mcfall, Joe D},
journal = {Journal of Quantitative Linguistics},
number = {2},
pages = {94--100},
title = {{Cutting the Gordian Knot : The Moving-Average Type – Token Ratio ( MATTR )}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09296171003643098},
volume = {17},
year = {2010}
}
@article{Clyde1985,
abstract = {13.1 INTRODUCTION In Chapter 12, we considered inference in a normal linear regression model with q predictors. In many instances, the set of predictor variables X can be quite large, as one considers many potential variables and possibly transfor-mations and interactions of these variables that may be relevant to modelling the response Y. One may start with a large set to reduce chances that an im-portant predictor has been omitted, but employ variable selection to eliminate variables do not appear to be necessary and avoid over-fitting. Historically, variable selection methods, such as forwards, backwards, and stepwise selec-tion, maximum adjusted R 2 , AIC, Cp, etc., have been used, and, as is well known, these can each lead to selection of a different final model (Weisberg 1985). Other modeling decisions that may arise in practice include specify-ing the structural form of the model, including choice of transformation of the response, error distribution, or choice of functional form that relates the mean to the predictors. Decisions on how to handle " outliers " may involve multiple tests with a somewhat arbitrary cut-off for p-values or the use of " ro-bust " outlier resistant methods. Many of the modeling decisions are made conditional on previous choices, and final measures of " significance " may be questionable. While one may not be surprised that approaches for selection of a model reach different conclusions, a major problem with such analyses is that often only a " best " model and its associated summaries are presented, giving a},
author = {Clyde, Merlise},
doi = {10.1002/9780470317105},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clyde - Unknown - MODEL AVERAGING.pdf:pdf},
isbn = {9780470317105},
pages = {1--25},
title = {{Model averaging}},
url = {https://www2.stat.duke.edu/courses/Spring05/sta244/Handouts/press.pdf https://stat.duke.edu/courses/Spring05/sta244/Handouts/press.pdf},
year = {1985}
}
@article{Neunerdt2013,
author = {Neunerdt, Melanie and Reyer, Michael and Mathar, Rudolf},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neunerdt, Reyer, Mathar - 2013 - A POS Tagger for Social Media Texts trained on web comments.pdf:pdf},
journal = {Polibits},
number = {48},
pages = {61--68},
title = {{A POS Tagger for Social Media Texts trained on web comments}},
url = {http://polibits.ojs.gelbukh.com/ojs/index.php/polibits/article/viewFile/1783/1723},
year = {2013}
}
@misc{Benoit2016b,
annote = {From Duplicate 2 (quanteda: Quantitative Analysis of Textual Data - Benoit, Kenneth; Nulty, Paul)

R package version 0.9.8.5},
author = {Benoit, Kenneth and Nulty, Paul and Watanabe, Kohei and Lauderdale, Benjamin and Obeng, Adam and Barber{\'{a}}, Pablo and Lowe, Will},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Benoit et al. - 2016 - Package 'quanteda' Quantitative Analysis of Textual Data.pdf:pdf},
title = {{quanteda: Quantitative Analysis of Textual Data}},
url = {https://github.com/kbenoit/quanteda},
year = {2016}
}
@inproceedings{Paperno2016a,
abstract = {We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages shar- ing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preced- ing the target word. To succeed on LAM- BADA, computational models cannot sim- ply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA ex- emplifies a wide range of linguistic phe- nomena, and that none of several state-of- the-art language models reaches accuracy above 1{\%} on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the develop- ment of new models capable of genuine understanding of broad context in natural language text.},
archivePrefix = {arXiv},
arxivId = {1606.06031},
author = {Paperno, Denis and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern, Raquel},
booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Berlin, Germany, August 7-12, 2016},
eprint = {1606.06031},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paperno et al. - 2016 - The LAMBADA dataset Word prediction requiring a broad discourse context(2).pdf:pdf},
keywords = {Distributional semantics},
pages = {1525--1534},
title = {{The LAMBADA dataset : Word prediction requiring a broad discourse context}},
year = {2016}
}
@article{Aliprandi2007,
abstract = {We present FastType, an innovative system for word and letter prediction for an inflected language, namely the Italian language. The system is based on combined statistical and lexical methods and it uses robust language resources. Word prediction is particularly useful to minimise keystrokes for users with special needs, and to reduce misspellings for users having limited Italian proficiency. Word prediction can be effectively used in language learning, by suggesting correct and well-formed words to non-native users. This is significant, and particularly difficult to cope with, for inflected languages such as Italian, where the correct word form depends on the context. After describing the system, we evaluate its performances and, besides the high Keystrokes Saving, we show that FastType outclasses typical word prediction limitations getting outstanding results even over a very large dictionary of words.},
author = {Aliprandi, Carlo and Carmignani, Nicola and Mancarella, Paolo},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aliprandi, Carmignani, Mancarella - 2007 - An Inflected-Sensitive Letter and Word Prediction System(3).pdf:pdf},
journal = {System},
keywords = {2006,2007,accepted,alternative and,and particularly difficult to,assistive technology,augmentative communication,august 5,computer aided language learning,december 30,june 10,natural language processing,nlp,received,revised,this is significant,users,word prediction},
number = {2},
pages = {79 -- 85},
title = {{An Inflected-Sensitive Letter and Word Prediction System}},
volume = {5},
year = {2007}
}
@article{Moreno-Sanchez2016,
abstract = {Despite being a paradigm of quantitative linguistics, Zipf's law for words suffers from three main problems: its formulation is ambiguous, its validity has not been tested rigorously from a statistical point of view, and it has not been confronted to a representatively large number of texts. So, we can summarize the current support of Zipf's law in texts as anecdotic. We try to solve these issues by studying three different versions of Zipf's law and fitting them to all available English texts in the Project Gutenberg database (consisting of more than 30 000 texts). To do so we use state-of-the art tools in fitting and goodness-of-fit tests, carefully tailored to the peculiarities of text statistics. Remarkably, one of the three versions of Zipf's law, consisting of a pure power-law form in the complementary cumulative distribution function of word frequencies, is able to fit more than 40{\%} of the texts in the database (at the 0.05 significance level), for the whole domain of frequencies (from 1 to the maximum value), and with only one free parameter (the exponent).},
archivePrefix = {arXiv},
arxivId = {1509.04486},
author = {Moreno-Sanchez, Isabel and Font-Clos, Francesc and Corral, Alvaro},
doi = {10.1371/journal.pone.0147073},
eprint = {1509.04486},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreno-Sanchez, Font-Clos, Corral - 2016 - Large-scale analysis of Zipf's law in English texts.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
keywords = {PACS numbers:},
number = {1},
pmid = {26800025},
title = {{Large-scale analysis of Zipf's law in English texts}},
volume = {11},
year = {2016}
}
@article{StefanEvert2015,
abstract = {Statistical models and utilities for the analysis of word frequency distributions. The utilities include functions for loading, manipulating and visualizing word frequency data and vocabulary growth curves. The package also implements several statistical models for the distribution of word frequencies in a population. (The name of this library derives from the most famous word frequency distribution, Zipf's law.)},
author = {Evert, Stefan},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evert - 2015 - Package zipfR.pdf:pdf},
journal = {R Package},
pages = {94},
title = {{Package "zipfR"}},
year = {2015}
}
@article{Sandnes2015,
abstract = {Users with reduced physical functioning such as ALS patients need more time and effort to operate computers. Most of the previous assistive technologies use prefix based predictive text input algorithms. Prefix based predictive text entry is suitable for languages such as English where the average word length is approximately 5 characters. Other languages such as Norwegian and German have longer mean word lengths as words are combined into longer compound words and prefix approaches are thus less effective. This paper proposes a new abbreviation expansion algorithm. Users mentally determine an abbreviation of the word, typically comprising significant consonants and the system proposes words that contain the matched characters. The approach is non disruptive in that it does not require the user to learn a new system or abbreviation mnemonics, and it can be used with any text input device. The system is dynamic and adapts to the users style of abbreviated input. The algorithm is easier to implement than previous approaches and no a priori system training is required. Our experimental evaluations demonstrate that the algorithm achieves real time performance with modest computer hardware.},
author = {Sandnes, Frode Eika},
doi = {10.1016/j.procs.2015.09.254},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sandnes - 2015 - Reflective Text Entry A Simple Low Effort Predictive Input Method Based on Flexible Abbreviations(2).pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {ALS,abbreviation expansion,dyslexia,longest common subsequence,low physical effort,text input},
number = {1877},
pages = {105--112},
publisher = {Elsevier Masson SAS},
title = {{Reflective Text Entry: A Simple Low Effort Predictive Input Method Based on Flexible Abbreviations}},
url = {http://dx.doi.org/10.1016/j.procs.2015.09.254},
volume = {67},
year = {2015}
}
@techreport{WMC2015,
author = {{The Women's Media Center}},
title = {{WMC Divided 2015: The Media Gender Gap | Women's Media Center}},
url = {http://www.womensmediacenter.com/pages/2015-wmc-divided-media-gender-gap},
year = {2015}
}
@misc{Flixter,
author = {Flixter},
title = {{Rotten Tomatoes: Movies | TV Shows | Movie Trailers | Reviews}},
url = {http://www.rottentomatoes.com/},
urldate = {2017-11-24}
}
@article{Boulton2007,
abstract = {t Overweight rates have been climbing over the past few decades among children. About 9 million (or roughly one in six kids ages 6–19) were overweight in 2004 – more than triple the number of overweight children in 1980. 14 t Given current trends, one in three children born in 2000 will develop diabetes over the course of a lifetime. 15 Chronic Diseases: Often Preventable, Frequently Manageable Many chronic diseases could be prevented, delayed, or alleviated, through simple lifestyle changes. t The U.S. Centers for Disease Control and Prevention (CDC) 16 estimates that eliminating three risk factors – poor diet, inactivity, and smoking – would prevent: t 80{\%} of heart disease and stroke; t 80{\%} of type 2 diabetes; and, t 40{\%} of cancer. 7 To get this number, total spending on chronic disease during 2005 ({\$}1.5 trillion) was divided by the total population (300 million Americans). 15 Laino C. One in three kids will develop diabetes. Web MD [serial online].},
author = {Boulton, Aj and Vileikyte, L and Ragnarson and Tennvall, G and Apelqvist, J},
doi = {10.1331/JAPhA.2007.08541},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boulton et al. - 2007 - A Vision for a Healthier Future(2).pdf:pdf},
issn = {15443450},
journal = {Accessed Aprilppt. Accessed on April},
number = {17},
pages = {1719--1724},
title = {{A Vision for a Healthier Future}},
url = {http://www.cdc.gov/nccdphp/overview.htm http://www.rwjf.org/files/research/ http://www.cms.hhs.gov/NationalHealthEx-pendData/02{\_}NationalHealthAccountsHistorical.asp{\#}TopOfPage. http://www.cdc.gov/nccdphp/overview.htm. http://www.kff.org/insur-ance/7527/. h},
volume = {6},
year = {2007}
}
@misc{Tagg2015,
abstract = {This thesis reports a study using a corpus of text messages in English (CorTxt) to explore linguistic features which define texting as a language variety. It focuses on how the language of texting, Txt, is shaped by texters actively fulfilling interpersonal goals. The thesis starts with an overview of the literature on texting, which indicates the need for thorough linguistic investigation of Txt based on a large dataset. It then places texting within the tradition of research into the speech-writing continuum, which highlights limitations of focusing on mode at the expense of other user-variables. The thesis also argues the need for inductive investigation alongside the quantitative corpus-based frameworks that dominate the field. A number of studies are then reported which explore the unconventional nature of Txt. Firstly, drawing on the argument that respelling constitutes a meaning-making resource, spelling variants are retrieved using word-frequency lists and categorised according to form and function. Secondly, identification of everyday creativity in CorTxt challenges studies focusing solely on spelling as a creative resource, and suggests that creativity plays an important role in texting because of, rather than despite, physical constraints. Thirdly, word frequency analysis suggests that the distinct order of the most frequent words in CorTxt can be explained with reference to the frequent phrases in which they occur. Finally, application of a spoken grammar model reveals similarities and differences between spoken and texted interaction. The distinct strands of investigation highlight, on the one hand, the extent to which texting differs from speech and, on the other, the role of user agency, awareness and choice in shaping Txt. The argument is made that this can be explained through performativity and, in particular, the observation that texters perform brevity, speech-like informality and group deviance in construing identities through Txt.},
author = {Tagg, Caroline},
booktitle = {University of Birmingham},
number = {March},
pages = {412},
title = {{a Corpus Linguistics Study of Sms Text Messaging}},
year = {2015}
}
@misc{Xie2016a,
annote = {R package version 1.15.1},
author = {Xie, Yihui},
publisher = {https://github.com/yihui/knitr},
title = {{knitr: A General-Purpose Package for Dynamic Report Generation in R}},
url = {https://github.com/yihui/knitr},
year = {2016}
}
@article{Goodman2008,
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0108005v1},
author = {Goodman, Joshua T},
eprint = {0108005v1},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman - 2001 - A Bit of Progress in Language Modeling Extended Version(2).pdf:pdf},
primaryClass = {arXiv:cs},
title = {{A Bit of Progress in Language Modeling Extended Version}},
url = {http://www.research.microsoft.com},
year = {2008}
}
@article{Gagolewski2016,
abstract = {Allows for fast, correct, consistent, portable, as well as convenient character string/text processing in every locale and any native encoding. Owing to the use of the ICU library, the package provides R users with platform-independent functions known to Java, Perl, Python, PHP, and Ruby programmers. Among available
features there are: pattern searching (e.g., with ICU Java-like regular expressions or the Unicode Collation Algorithm), random string generation, case mapping, string transliteration, concatenation,
Unicode normalization, date-time formatting and parsing, etc.},
author = {Gagolewski, Marek},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gagolewski - 2016 - Package 'stringi' Character String Processing Facilities(2).pdf:pdf},
journal = {CRAN},
title = {{Package 'stringi': Character String Processing Facilities}},
year = {2016}
}
@misc{Avvo,
author = {Avvo},
title = {{New Avvo survey explores modern attitudes around love, sex, and dating - AvvoStories}},
url = {http://stories.avvo.com/relationships/new-avvo-survey-explores-modern-attitudes-around-love-sex-dating.html},
urldate = {2017-10-27}
}
@book{Yule1944a,
author = {Yule, George Udny},
booktitle = {Cambridge University Press},
pages = {306},
publisher = {Cambridge University Press},
title = {{The Statistical Study of Literary Vocabulary}},
year = {1944}
}
@misc{Duggan2015,
author = {Duggan, Maeve},
title = {{Mobile Messaging and Social Media 2015 | Pew Research Center}},
url = {http://www.pewinternet.org/2015/08/19/mobile-messaging-and-social-media-2015/},
urldate = {2016-12-14},
year = {2015}
}
@article{Reid2004a,
abstract = {The increasingly widespread use of text -messaging has led to the questioning of the social and psychological effects of this novel communication medium. A selection of findings from an online questionnaire that was developed by the author to answer this pertinent question are presented. McKenna's recent work on the way the Internet can help some people develop relationships is drawn upon and taken a step further by exploring the differences between those who prefer texting ('Texters') and those who prefer talking on their mobiles ('Talkers'). A large sample of 982 respondents completed the questionnaire. Results showed there was a clear distinction between Texters and Talkers in the way they used their mobiles and their underlying motivations. The key finding to emerge in the preliminary analyses was that Texters seemed to form close knit 'text circles' with their own social ecology, interconnecting with a close group of friends in perpetual text contact. Compared to Talkers, Texters were found to be more lonely and socially anxious, and more likely to disclose their 'real-self' through text than via face-to-face or voice call exchanges. S tructural equation modeling produced a model showing that where respondents located their real-self and whether they were a Texter or a Talker mediated between the loneliness and social anxiety measures and the impact of these on relational outcomes, in line with McKenna's theoretical framework. Thus it appears that there is something special about texting that allows some people to translate their loneliness and/or social anxiety into productive relationships whilst for others the mobile does not afford the same effect. .Applications and explorations for future research are discussed.},
author = {Reid, Donna and Reid, Fraser},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reid, Reid - 2004 - The social and psychological effects of Text Insights into the Social and Psychological Effects of SMS Text Messa(2).pdf:pdf},
title = {{The social and psychological effects of Text Insights into the Social and Psychological Effects of SMS Text Messaging}},
url = {www.plymouth.ac.uk},
year = {2004}
}
@inproceedings{Ghayoomi2009,
abstract = {—The prediction task in national language processing means to guess the missing letter, word, phrase, or sentence that likely follow in a given segment of a text. Since 1980s many systems with different methods were developed for different languages. In this paper an overview of the existing prediction methods that have been used for more than two decades are described and a general classification of the approaches is presented. The three main categories of the classification are statistical modeling, knowledge-based modeling, and heuristic modeling (adaptive). Index Terms—Word prediction, Assistant technology, Lan-guage modeling.},
author = {Ghayoomi, Masood and Momtazi, Saeedeh},
booktitle = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
doi = {10.1109/ICSMC.2009.5346027},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Momtazi - 2009 - An overview on the existing language models for prediction systems as writing assistant tools.pdf:pdf},
isbn = {9781424427949},
issn = {1062922X},
keywords = {Assistant technology,Language modeling,Word prediction},
pages = {5083--5087},
title = {{An overview on the existing language models for prediction systems as writing assistant tools}},
year = {2009}
}
@misc{Xie2013,
abstract = {Suitable for both beginners and advanced users, this book shows you how to write reports in simple languages such as Markdown. The reports range from homework, projects, exams, books, blogs, and web pages to any documents related to statistical graphics, computing, and data analysis. While familiarity with LaTeX and HTML is helpful, the book requires no prior experience with advanced programs or languages. For beginners, the text provides enough features to get started on basic applications. For power users, the last several chapters enable an understanding of the extensibility of the knitr package.},
author = {Xie, Yihui},
isbn = {1482203537},
title = {{knitr: Elegant, flexible and fast dynamic report generation with R | knitr}},
url = {http://yihui.name/knitr/ http://yihui.name/knitr/{\%}5Cnpapers2://publication/uuid/21E89689-A1A0-4422-A0C8-45D31FA20C5B},
year = {2013}
}
@misc{Michalke2015,
author = {eik Michalke, M.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Michalke - 2015 - koRpus An R Package for Text Analysis.pdf:pdf},
title = {{koRpus: An R Package for Text Analysis}},
url = {http://reaktanz.de/?c=hacking{\&}s=koRpus},
year = {2015}
}
@book{Herdan1960,
author = {Herdan, Gustav},
pages = {448},
publisher = {Mouton},
title = {{Type-token Mathematics}},
year = {1960}
}
@misc{Hlavac2015,
abstract = {Produces LaTeX code for well-formatted tables that hold regression analysis results from several models side-by-side,as well as summary statistics},
author = {Hlavac, Marek and Package, Type and Regression, Title Well-formatted and Tables, Summary Statistics},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hlavac et al. - 2015 - Well-Formatted Regression and Summary Statistics Tables.pdf:pdf},
pages = {1--11},
title = {{Well-Formatted Regression and Summary Statistics Tables}},
url = {https://cran.r-project.org/web/packages/stargazer/stargazer.pdf},
year = {2015}
}
@article{Federico2008,
abstract = {Research in speech recognition and machine translation is boosting the use of large scale n-gram language models. We present an open source toolkit that permits to efficiently handle language models with billions of n-grams on conventional machines. The IRSTLM toolkit supports distribution of n-gram collection and smoothing over a computer cluster, language model compression through probability quantization, lazy-loading of huge language models from disk. IRSTLM has been so far successfully deployed with the Moses toolkit for statistical machine translation and with the FBK-irst speech recognition system. Efficiency of the tool is reported on a speech transcription task of Italian political speeches using a language model of 1.1 billion four-grams.},
author = {Federico, Marcello and Bertoldi, Nicola and Cettolo, Mauro},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Federico, Bertoldi, Cettolo - 2008 - IRSTLM An open source toolkit for handling large scale language models(2).pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Automatic speech recognition,Language modeling,Statistical machine translation},
pages = {1618--1621},
title = {{IRSTLM: An open source toolkit for handling large scale language models}},
year = {2008}
}
@misc{Ng,
abstract = {Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI. This course provides a broad introduction to machine learning, datamining, and statistical pattern recognition. Topics include: (i) Supervised learning (parametric/non-parametric algorithms, support vector machines, kernels, neural networks). (ii) Unsupervised learning (clustering, dimensionality reduction, recommender systems, deep learning). (iii) Best practices in machine learning (bias/variance theory; innovation process in machine learning and AI). The course will also draw from numerous case studies and applications, so that you'll also learn how to apply learning algorithms to building smart robots (perception, control), text understanding (web search, anti-spam), computer vision, medical informatics, audio, database mining, and other areas.},
author = {Ng, Andrew},
title = {{Machine Learning by Stanford University}},
url = {https://www.coursera.org/learn/machine-learning/home/welcome},
urldate = {2017-09-08},
year = {2015}
}
@techreport{Dahl2016,
author = {Dahl, Author David B and Scott, Maintainer David},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dahl, Scott - 2016 - Package ‘xtable'.pdf:pdf},
pages = {1--34},
title = {{Package ‘xtable'}},
url = {https://cran.r-project.org/web/packages/xtable/xtable.pdf},
year = {2016}
}
@article{Griffiths2003,
abstract = {We explore the consequences of viewing semantic association as the result of attempting to predict the concepts likely to arise in a particular context. We argue that the success of existing accounts of semantic representation comes as a result of indirectly addressing this problem, and show that a closer correspondence to human data can be obtained by taking a probabilistic approach that explicitly models the generative structure of language.},
author = {Griffiths, Thomas L and Steyvers, Mark},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths, Steyvers - 2003 - Prediction and Semantic Association.pdf:pdf},
isbn = {0262025507},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {Distributional semantics},
pages = {11--18},
title = {{Prediction and Semantic Association}},
url = {https://papers.nips.cc/paper/2153-prediction-and-semantic-association.pdf papers3://publication/uuid/EA850935-AD77-4502-8B69-E5DF29AF6A0B},
volume = {15},
year = {2003}
}
@misc{Hornik2016,
abstract = {Description An interface to the Apache OpenNLP tools (version 1.5.3). The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text written in Java. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. See {\textless}http://opennlp.apache.org/{\textgreater} for more information. Imports NLP ({\textgreater}= 0.1-6.3), openNLPdata ({\textgreater}= 1.5.3-1), rJava ({\textgreater}= 0.6-3)},
author = {Hornik, Kurt},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hornik - 2016 - Apache OpenNLP Tools Interface.pdf:pdf},
title = {{Apache OpenNLP Tools Interface}},
year = {2016}
}
@article{Shortis2007,
author = {Shortis, Tim},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2007 - Gr8 Txtpectations The Creativity of Text Spelling The vernacular spelling tradition(2).pdf:pdf},
number = {June},
pages = {21--26},
title = {{The Creativity of Text Spelling}},
year = {2007}
}
@article{Mezei2012,
abstract = {Many students with physical disabilities have difficulty with writing fluency due to motor limitations. One type of assistive technology that has been developed to improve writing speed and accuracy is word prediction software, although there is a paucity of research supporting its use for individuals with physical disabilities. This study used an alternating treatment design between word prediction versus word pro- cessing to examine fluency, accuracy, and passage length on writing draft papers by individuals who have physical disabilities. Results indicated that word prediction had little to no effectiveness in increasing writing speed for all of the students in this study, but it shows promise in decreasing spelling and typographical errors.},
author = {Mezei, Peter J and {Wolff Heller}, Kathryn},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mezei, Wolff Heller - 2012 - Effects of Word Prediction on Writing Fluency for Students With Physical Disabilities.pdf:pdf},
isbn = {978-1-124-04823-9},
journal = {Physical Disabilities: Education and Related Services},
number = {1},
pages = {3--26},
title = {{Effects of Word Prediction on Writing Fluency for Students With Physical Disabilities}},
url = {http://files.eric.ed.gov/fulltext/EJ986388.pdf},
volume = {31},
year = {2012}
}
@article{Gustavii2003a,
abstract = {Word prediction may be of great help in situations where text entering is cumbersome due to a physical or a cognitive disability, or due to the input device being small. Traditionally, word prediction has solely been based on statistic language modelling, but lately knowledge-based approaches, including the use of grammatical language descriptions, have entered the arena. By making use of grammar rules, the accuracy of the prediction suggestions is expected to increase, and the word predictor to give a more intelligent impression. We have defined and implemented a Swedish grammar for the FASTY word predictor. The gram- mar rules, defined in terms of traditional grammatical functions, are expressed in the procedural UCP- formalism. The grammar functions as a grammar checking filter, reranking the suggestions proposed by a statistic n-gram model on the basis of both confirming and rejecting rules. What structures to cover has been decided in accordance with an investigation on what syntactic errors are most frequently produced by the statistic model. The investigation led us to prioritize rules for handling word order in the main clause, agreement within the noun phrase, verb inflection and prepositional phrases. A preliminary evaluation of the grammar module was carried out, using Keystroke Saving Rate (KSR) estimations. The results showed only a slight improvement in KSR when adding the grammar module to the system, as compared to using only the statistic model based on word form bigrams and part-of-speech tag trigrams. A list length of one suggestion gave a larger improvement, than a list length of five, indicating that the strength of the grammar module lies in the reranking of already displayed suggestions, rather than the addition of new suggestions to a long list of suggestions.},
author = {Gustavii, Ebba and Pettersson, Eva},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gustavii, Pettersson - 2003 - A Swedish Grammar for Word Prediction(3).pdf:pdf},
number = {June},
title = {{A Swedish Grammar for Word Prediction}},
year = {2003}
}
@misc{Benoit2016,
author = {Benoit, Kenneth and Nulty, Paul and Watanabe, Kohei and Lauderdale, Benjamin and Obeng, Adam and Barber{\'{a}}, Pablo and Lowe, Will},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Benoit et al. - 2016 - Package 'quanteda' Quantitative Analysis of Textual Data.pdf:pdf},
title = {{Package 'quanteda': Quantitative Analysis of Textual Data}},
year = {2016}
}
@article{mandelbrot1965information,
author = {Mandelbrot, Beno$\backslash${\^{}}$\backslash$it},
journal = {BB Wolman and E},
title = {{Information theory and psycholinguistics}},
year = {1965}
}
@article{Trnka2008b,
author = {Trnka, Keith},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka - 2008 - Word Prediction Techniques for User Adaptation and Sparse Data Mitigation Ph . D . Thesis Proposal.pdf:pdf},
journal = {Information Sciences},
keywords = {proposal},
title = {{Word Prediction Techniques for User Adaptation and Sparse Data Mitigation Ph . D . Thesis Proposal}},
url = {https://pdfs.semanticscholar.org/e7cd/4e4e31ed6d5b52593c2e434029f3fa1ec50a.pdf},
year = {2008}
}
@article{Raftery1988,
author = {Raftery, Adrian E.},
doi = {10.1093/biomet/75.2.223},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raftery - 1988 - Inference for the Binomial {\$}N{\$} Parameter A Hierarchical Bayes Approach.pdf:pdf},
isbn = {0006-3444},
journal = {Biometrika},
number = {2},
pages = {223--228},
title = {{INFERENCE FOR THE BINOMIAL N-PARAMETER - A HIERARCHICAL BAYES APPROACH}},
url = {https://www.stat.washington.edu/raftery/Research/PDF/bka1988.pdf},
volume = {75},
year = {1988}
}
@book{Barton1994,
author = {Barton, David},
pages = {96},
publisher = {Blackwell Publishing},
title = {{Literacy: An Introduction to the Ecology of Written Language}},
year = {1994}
}
@book{Jeffreys61,
address = {Oxford, England},
author = {Jeffreys, H},
booktitle = {Theory of Probability},
edition = {Third},
publisher = {Oxford},
title = {{Theory of Probability}},
year = {1961}
}
@article{Derczynski2013a,
abstract = {Part-of-speech information is a pre-requisite in many NLP algorithms. However, Twitter text is difficult to part-of-speech tag: it is noisy, with linguistic errors and idiosyncratic style. We present a detailed error analysis of existing taggers, motivating a series of tagger augmentations which are demonstrated to improve performance. We identify and evaluate techniques for improving English part-of-speech tagging performance in this genre. Further, we present a novel approach to system combination for the case where available taggers use different tagsets, based on voteconstrained bootstrapping with unlabeled data. Coupled with assigning prior probabilities to some tokens and handling of unknown words and slang, we reach 88.7{\%} tagging accuracy (90.5{\%} on development data). This is a new high in PTB-compatible tweet part-of-speech tagging, reducing token error by 26.8{\%} and sentence error by 12.2{\%}. The model, training data and tools are made available.},
author = {Derczynski, Leon and Ritter, Alan and Clark, Sam and Bontcheva, Kalina},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Derczynski et al. - 2013 - Twitter part-of-speech tagging for all Overcoming sparse and noisy data(2).pdf:pdf},
issn = {13138502},
journal = {Proceedings of the Recent Advances in Natural Language Processing},
number = {September},
pages = {198--206},
title = {{Twitter part-of-speech tagging for all: Overcoming sparse and noisy data}},
url = {http://www.aclweb.org/website/old{\_}anthology/R/R13/R13-1026.pdf},
year = {2013}
}
@article{DerPhilosophisch-Historischen2004,
author = {der Philosophisch-Historischen, Von and {Evert aus Ludwigsburg Hauptberichter}, Stefan and {Rohrer Mitberichter}, C and {Kahnert Mitberichter}, Apl D and Heid, HD U},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/der Philosophisch-Historischen et al. - 2004 - The Statistics of Word Cooccurrences Word Pairs and Collocations(2).pdf:pdf},
title = {{The Statistics of Word Cooccurrences Word Pairs and Collocations}},
year = {2004}
}
@article{Dowle2016,
author = {Dowle, Matt},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dowle - 2016 - Package ‘data.table'(2).pdf:pdf},
journal = {Cran},
title = {{Package ‘data.table'}},
year = {2016}
}
@article{10.2307/2333344,
abstract = {A random sample is drawn from a population of animals of various species. (The theory may also be applied to studies of literary vocabulary, for example.) If a particular species is represented r times in the sample of size N, then r/N is not a good estimate of the population frequency, p, when r is small. Methods are given for estimating p, assuming virtually nothing about the underlying population. The estimates are expressed in terms of smoothed values of the numbers nr (r = 1, 2, 3...), where nr is the number of distinct species that are each represented r times in the sample. (nr may be described as `the frequency of the frequency r'.) Turing is acknowledged for the most interesting formula in this part of the work. An estimate of the proportion of the population represented by the species occurring in the sample is an immediate corollary. Estimates are made of measures of heterogeneity of the population, including Yule's characteristic' and Shannon's entropy'. Methods are then discussed that do depend on assumptions about the underlying population. It is here that most work has been done by other writers. It is pointed out that a hypothesis can give a good fit to the numbers nr but can give quite the wrong value for Yule's characteristic. An example of this is Fisher's fit to some data of Williams's on Macrolepidoptera.},
author = {Good, I J},
issn = {00063444},
journal = {Biometrika},
number = {3/4},
pages = {237--264},
publisher = {[Oxford University Press, Biometrika Trust]},
title = {{The Population Frequencies of Species and the Estimation of Population Parameters}},
url = {http://www.jstor.org/stable/2333344},
volume = {40},
year = {1953}
}
@article{Biber1993,
author = {Biber, Douglas},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Biber - 1993 - representativeness in corpus design.pdf(2).pdf:pdf},
number = {4},
title = {{Representativeness in Corpus Design.Pdf}},
volume = {8},
year = {1993}
}
@article{Hornik2009,
author = {Hornik, Kurt and Buchta, Christian and Zeileis, Achim},
doi = {10.1007/s00180-008-0119-7},
journal = {Computational Statistics},
number = {2},
pages = {225--232},
title = {{Open-Source Machine Learning: {\{}R{\}} Meets {\{}Weka{\}}}},
volume = {24},
year = {2009}
}
@article{Clarkson1997,
author = {Clarkson, Philip and Rosenfeld, Ronald},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clarkson, Rosenfeld - 1997 - Statistical Language Modeling Using the {\{}CMU{\}}--Cambridge Toolkit.pdf:pdf},
journal = {Proceedings Eurospeech '97},
pages = {2707--2710},
title = {{Statistical Language Modeling Using the {\{}CMU{\}}--Cambridge Toolkit}},
url = {http://repository.cmu.edu/compsci},
year = {1997}
}
@article{Ghayoomi2005,
abstract = {Word prediction is the problem of guessing which words are likely to follow in a given segment of a text to help a user with disabilities. As the user enters each letters of the required word, the system displays a list of the most probable words that could appear in that position. In our research we designed and implemented a word predictor for the Persian language. Three standard performance metrics were used to evaluate the system including keystroke saving, the most important one. The system achieved 57.57{\%} saving in keystrokes.},
author = {Ghayoomi, M and Assi, S M},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Assi - 2005 - Word Prediction in a Running Text A Statistical Language Modeling for the Persian Language(3).pdf:pdf},
journal = {Proceedings of the Australasian Language Technology Workshop},
number = {December},
pages = {57--63},
title = {{Word Prediction in a Running Text: A Statistical Language Modeling for the Persian Language}},
year = {2005}
}
@article{Ling2007a,
author = {Ling, Rich and Baron, Naomi S and R{\&}d, Telenor},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling, Baron, R{\&}d - 2007 - Text Messaging and IM Linguistic Comparison of American College Data(2).pdf:pdf},
title = {{Text Messaging and IM: Linguistic Comparison of American College Data}},
year = {2007}
}
@article{Garay-Vitoria2010a,
abstract = {Text prediction was initially proposed to help people with a low text composition speed to enhance their message composition. After the important advancements obtained in the last years, text prediction methods may nowadays benefit anyone trying to input text messages or commands, if they are adequately integrated within the user interface of the application. Diverse text prediction methods are based in different statistic and linguistic properties of natural languages. Hence, they are very dependent on the language concerned. In order to discuss general issues of text prediction it is necessary to propose abstract descriptions of the methods used. In this paper a number of models applied to text prediction are presented. Some of them are oriented to low-inflected languages while others are for high-inflected languages. All these models have been implemented and their results are compared. Presented models may be useful for future discussion. Finally, some comments related to the comparison of previously published results are also done. ?? 2009 Elsevier Ltd. All rights reserved.},
author = {Garay-Vitoria, Nestor and Abascal, Julio},
doi = {10.1016/j.csl.2009.03.008},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garay-Vitoria, Abascal - 2010 - Modelling text prediction systems in low- and high-inflected languages(3).pdf:pdf},
issn = {08852308},
journal = {Computer Speech and Language},
keywords = {Anticipative interfaces,Communication speed enhancement,Prediction measures,Prediction models,Text prediction},
number = {2},
pages = {117--135},
title = {{Modelling text prediction systems in low- and high-inflected languages}},
volume = {24},
year = {2010}
}
@misc{MacFarlane2006,
author = {MacFarlane, John},
title = {{About Pandoc}},
url = {http://pandoc.org/ http://johnmacfarlane.net/pandoc/},
year = {2006}
}
@misc{Baayen2013,
author = {Baayen, R H},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baayen, Baayen - 2015 - Package 'languageR' Title Data sets and functionswiAnalyzing Linguistic Data A practical introduction to stat(2).pdf:pdf},
title = {{Package 'languageR' : Data sets and functions for Analyzing Linguistic Data: A practical introduction to statistics''}},
year = {2013}
}
@article{Trnka2008,
abstract = {We present the development and tuning of a topic-adapted language model for word prediction, which improves keystroke savings over a comparable baseline. We outline our plans to develop and integrate style adaptations, building on our experience in topic modeling to dynamically tune the model to both topically and stylistically relevant texts.},
author = {Trnka, Keith},
doi = {10.3115/1564154.1564167},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka - 2008 - Adaptive language modeling for word prediction.pdf:pdf},
journal = {HLT-SRWS '08 Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies},
number = {June},
pages = {61--66},
title = {{Adaptive language modeling for word prediction}},
url = {http://dl.acm.org/citation.cfm?id=1564167},
year = {2008}
}
@article{Magnuson2002,
author = {Magnuson, T and Hunnicutt, S},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Magnuson, Hunnicutt - 2002 - Dept. for Speech, Music and Hearing Quarterly Progress and Status Report Measuring the effectiveness of wor.pdf:pdf},
pages = {057--067},
title = {{Dept. for Speech, Music and Hearing Quarterly Progress and Status Report Measuring the effectiveness of word prediction: The advantage of long-term use}},
url = {http://www.speech.kth.se/qpsr},
year = {2002}
}
@article{Kilgarriff2001,
abstract = {Corpus linguistics lacks strategies for describing and compar-ing corpora. Currently most descriptions of corpora are textual, and questions such as 'what sort of a corpus is this?', or 'how does this corpus compare to that?' can only be answered impressionistically. This paper considers various ways in which different corpora can be compared more objectively. First we address the issue, 'which words are particularly characteristic of a corpus?', reviewing and critiquing the statistical methods which have been applied to the question and proposing the use of the Mann-Whitney ranks test. Results of two corpus com-parisons using the ranks test are presented. Then, we consider measures for corpus similarity. After discussing limitations of the idea of corpus similarity, we present a method for evaluat-ing corpus similarity measures. We consider several measures and establish that a $\chi$ 2 -based one performs best. All methods considered in this paper are based on word and ngram fre-quencies; the strategy is defended.},
author = {Kilgarriff, Adam},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kilgarriff - 2001 - 29 COMPARING CORPORA(3).pdf:pdf},
journal = {International Journal of Corpus Linguistics},
number = {1},
pages = {97--133},
title = {{29 COMPARING CORPORA}},
volume = {6},
year = {2001}
}
@unpublished{Feinerer2015,
abstract = {A framework for text mining applications within R.},
author = {Feinerer, Ingo},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feinerer - 2015 - Package 'tm' Text Mining Package.pdf:pdf},
pages = {58},
title = {{Package 'tm' Text Mining Package}},
url = {https://cran.r-project.org/web/packages/tm/index.html},
year = {2015}
}
@misc{Dayan1999,
abstract = {Unsupervised learning studies how systems can learn to represent particular input patterns in a way that reflects the statistical structure of the overall collection of input patterns. By contrast with SUPERVISED LEARNING or REINFORCEMENT LEARNING, there are no explicit target outputs or environmental evaluations associated with each input; rather the unsupervised learner brings to bear prior biases as to what aspects of the structure of the input should be captured in the output.},
author = {Dayan, Peter},
booktitle = {The MIT Encyclopedia of the Cognitive Sciences},
doi = {10.1016/j.visres.2007.07.023},
issn = {0042-6989},
keywords = {Animals,Cats,Learning,Learning: physiology,Models,Neurological,Neuronal Plasticity,Neuronal Plasticity: physiology,Neurons,Neurons: physiology,Orientation,Orientation: physiology,Photic Stimulation,Photic Stimulation: methods,Sensory Deprivation,Sensory Deprivation: physiology,Visual Cortex,Visual Cortex: physiology,Visual Perception,Visual Perception: physiology},
number = {22},
pages = {1--29},
pmid = {17850840},
title = {{Unsupervised Learning}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17850840},
urldate = {2017-09-08},
volume = {47},
year = {1999}
}
@article{Gustavii2003,
abstract = {Word prediction may be of great help in situations where text entering is cumbersome due to a physical or a cognitive disability, or due to the input device being small. Traditionally, word prediction has solely been based on statistic language modelling, but lately knowledge-based approaches, including the use of grammatical language descriptions, have entered the arena. By making use of grammar rules, the accuracy of the prediction suggestions is expected to increase, and the word predictor to give a more intelligent impression. We have defined and implemented a Swedish grammar for the FASTY word predictor. The gram- mar rules, defined in terms of traditional grammatical functions, are expressed in the procedural UCP- formalism. The grammar functions as a grammar checking filter, reranking the suggestions proposed by a statistic n-gram model on the basis of both confirming and rejecting rules. What structures to cover has been decided in accordance with an investigation on what syntactic errors are most frequently produced by the statistic model. The investigation led us to prioritize rules for handling word order in the main clause, agreement within the noun phrase, verb inflection and prepositional phrases. A preliminary evaluation of the grammar module was carried out, using Keystroke Saving Rate (KSR) estimations. The results showed only a slight improvement in KSR when adding the grammar module to the system, as compared to using only the statistic model based on word form bigrams and part-of-speech tag trigrams. A list length of one suggestion gave a larger improvement, than a list length of five, indicating that the strength of the grammar module lies in the reranking of already displayed suggestions, rather than the addition of new suggestions to a long list of suggestions.},
author = {Gustavii, Ebba and Pettersson, Eva},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gustavii, Pettersson - 2003 - A Swedish Grammar for Word Prediction(3).pdf:pdf},
number = {June},
title = {{A Swedish Grammar for Word Prediction}},
year = {2003}
}
@article{Schwarz,
abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Schwarz, Gideon},
doi = {10.1214/aos/1176344136},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwarz - Unknown - Estimating the Dimension of a Model.pdf:pdf},
isbn = {0780394224},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {2},
pages = {461--464},
pmid = {2958889},
title = {{Estimating the Dimension of a Model}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.aos/1176344136 http://projecteuclid.org/euclid.aos/1176344136},
volume = {6},
year = {1978}
}
@misc{Thurlow,
abstract = {The so called 'net generation' is popularly assumed to be naturally media literate and to be necessarily reinventing conventional linguistic and communicative practices. With this in mind, this essay centres around discursive analyses of qualitative data arising from an investigation of 159 older teenagers' use of mobile telephone text-messaging - or SMS (i.e. short-messaging services). In particular, against a backdrop of media commentaries, we examine the linguistic forms and communicative functions in a corpus of 544 participants' actual text-messages. While young people are surely using their mobile phones as a novel, creative means of enhancing and supporting intimate relationships and existing social networks, popular discourses about the linguistic exclusivity and impenetrability of this particular technologically-mediated discourse appear greatly exaggerated. Serving the sociolinguistic 'maxims' of (a) brevity and speed, (b) paralinguistic restitution and (c) phonological approximation, young people's messages are both linguistically unremarkable and communicatively adept.},
author = {Thurlow, Crispin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thurlow - 2003 - Generation Txt The sociolinguistics of young people ' s text-messaging.pdf:pdf},
title = {{Generation Txt?  The socialinguistics of youn people's text-messaging}}
}
@book{Bell:1990:TC:77753,
address = {Upper Saddle River, NJ, USA},
author = {Bell, Timothy C and Cleary, John G and Witten, Ian H},
isbn = {0-13-911991-4},
publisher = {Prentice-Hall, Inc.},
title = {{Text Compression}},
year = {1990}
}
@article{Lidstone1920,
author = {Lidstone, George James},
journal = {Transactions of the Faculty of Actuaries},
keywords = {mathematics},
pages = {182--192},
title = {{Note on the general case of the bayes-laplace formula for inductive or a posteriori probabilities}},
volume = {8},
year = {1920}
}
@article{Lenhart2012,
abstract = {Texting volume is up while the frequency of voice calling is down. About one in four teens say they own smartphones. The volume of texting among teens has risen from 50 texts a day in 2009 to 60 texts for the median teen text user. Older teens, boys, and blacks are leading the increase. Texting is the dominant daily mode of communication between teens and all those with whom they communicate.},
author = {Lenhart, Amanda},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lenhart - 2012 - Teens , Smartphones {\&} Texting.pdf:pdf},
journal = {Pew Research Center},
pages = {34},
title = {{Teens , Smartphones {\&} Texting}},
url = {http://pewinternet.org/Reports/2012/Teens-and-smartphones.aspx},
year = {2012}
}
@misc{LawlerJohn1999,
author = {LawlerJohn},
title = {{English Word List}},
url = {http://www-personal.umich.edu/{~}jlawler/wordlist},
year = {1999}
}
@article{Kurt2016,
author = {Kurt, Author and Karatzoglou, Alexandros and Meyer, David},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kurt, Karatzoglou, Meyer - 2016 - Package ‘ RWeka '.pdf:pdf},
title = {{Package ‘ RWeka '}},
year = {2016}
}
@misc{NORC,
author = {NORC},
title = {{About NORC Menu | NORC.org}},
url = {http://www.norc.org/About/Pages/default.aspx},
urldate = {2017-09-30}
}
@article{Dunlop2000,
abstract = {Mobile phone networks are increasingly supporting the transmission of textual messages between mobile phones and between mobile phones and other services. This paper describes the current text entry method on mobile phones and describes a new text entry method using a single key-press per letter together with a large dictionary of words for disambiguation. This approach, which is similar to technology recently licensed, independently, to several phone companies, is then extended with automatic word completion. The paper reports the results of initial user tests comparing the text entry methods, analysis of word clashes with the dictionary-based methods and keystroke level modelling of the different input methods.},
author = {Dunlop, M.D. and Crossan, Andrew},
doi = {10.1007/BF01324120},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dunlop, Crossan - 2000 - Predictive text entry methods for mobile phones.pdf:pdf},
issn = {0949-2054},
journal = {Personal Technologies},
number = {2-3},
pages = {134--143},
title = {{Predictive text entry methods for mobile phones}},
url = {http://link.springer.com/article/10.1007/BF01324120},
volume = {4},
year = {2000}
}
@misc{wiki:unsupervisedlearning,
annote = {[Online; accessed 
8-September-2017
]},
author = {Wikipedia},
title = {{Unsupervised learning --- Wikipedia{\{},{\}} The Free Encyclopedia}},
url = {https://en.wikipedia.org/w/index.php?title=Unsupervised{\_}learning{\&}oldid=793838440},
year = {2017}
}
@article{Ekstrom2017,
author = {Ekstrom, Claus and Dalgaard, Peter and Gill, Jeffrey and Weibelzahl, Stephan and Ford, Clay and Volcic, Robert},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ekstrom et al. - 2017 - Package ‘ pwr '.pdf:pdf},
pages = {1--22},
title = {{Package ‘ pwr '}},
url = {https://cran.r-project.org/web/packages/pwr/pwr.pdf},
year = {2017}
}
@article{Katz1987,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 2572004 Estimation the recognizer Article Source : CiteSeer CITATIONS 37 READS 663 4 , including : Some : Computers (CHIL) project IARPA Lori French 342 , 023 SEE G . Adda French 161 , 609 SEE All . The . All - text and , letting .Abstract-Thedescriptionofanoveltypeofrn-gramlanguagemodelisgiven.Themodeloffers,viaanonlinearrecursiveprocedure,acom-putationandspaceefficientsolutiontotheproblemofestimatingprob-abilitiesfromsparsedata.Thissolutioncomparesfavorablytootherproposedmethods.Whilethemethodhasbeendevelopedforandsuc-cessfullyimplementedintheIBMRealTimeSpeechRecognizers,itsgeneralitymakesitapplicableinotherareaswheretheproblemofes-timatingprobabilitiesfromsparsedataarises.Sparsenessofdataisaninherentpropertyofanyrealtext,anditisaproblemthatonealwaysencounterswhilecollectingfre-quencystatisticsonwordsandwordsequences(m-grams)fromatextoffinitesize.Thismeansthatevenforaverylargedatacol-lection,themaximumlikelihoodestimationmethoddoesnotallowustoadequatelyestimateprobabilitiesofrarebutneverthelesspos-siblewordsequences-manysequencesoccuronlyonce("single-tons");manymoredonotoccuratall.Inadequacyofthemaximumlikelihoodestimatorandthenecessitytoestimatetheprobabilitiesofm-gramswhichdidnotoccurinthetextconstitutetheessenceoftheproblem.Themainideaoftheproposedsolutiontotheproblemistore-duceunreliableprobabilityestimatesgivenbytheobservedfre-quenciesandredistributethe"freed"probability"mass"amongm-gramswhichneveroccurredinthetext.Thereductionisachievedbyreplacingmaximumlikelihoodestimatesform-gramshavinglowcountswithrenormalizedTuring'sestimates[l],andthere-distributionisdoneviatherecursiveutilizationoflowerlevelcon-ditionaldistributions.WefoundTuring'smethodattractivebe-causeofitssimplicityanditscharacterizationastheoptimalempiricalBayes'estimatorofamultinomialprobability.Robbinsin[2]introducestheempiricalBayes'methodologyandNadasin[3]givesvariousderivationsoftheTuring'sformula.LetNbeasampletextsizeandletn,bethenumberofwords(m-grams)whichoccurredinthetextexactlyrtimes,sothatN=Crn,.},
author = {Katz, Slavam},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz - 1987 - Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer(3).pdf:pdf},
number = {3},
pages = {35--1987},
title = {{Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer}},
url = {https://www.researchgate.net/profile/Lori{\_}Lamel/publication/2572004{\_}Estimation{\_}of{\_}probabilities{\_}from{\_}Sparse{\_}data{\_}for{\_}the{\_}language{\_}model{\_}component{\_}of{\_}a{\_}speech{\_}recognizer/links/5422cdc10cf26120b7a55d60.pdf},
year = {1987}
}
@article{Zellner1980,
abstract = {Bayesian posterior odds ratios for frequently encountered hypotheses about parameters of the normal linear multiple regression model are derived and discussed. For the particular prior distributions utilized, it is found that the posterior odds ratios can be well approximated by functions that are monotonic in usual sampling theoryF statistics. Some implications of this finding and the relation of our work to the pioneering work of Jeffreys and others are considered. Tabulations of odds ratios are provided and discussed.},
author = {Zellner, A and Siow, A},
doi = {10.1007/BF02888369},
issn = {0041-0241},
journal = {Trabajos de Estadistica Y de Investigacion Operativa},
month = {feb},
number = {1},
pages = {585--603},
title = {{Posterior odds ratios for selected regression hypotheses}},
url = {https://doi.org/10.1007/BF02888369},
volume = {31},
year = {1980}
}
@article{Garay-Vitoria2004,
abstract = {Prediction is one of the most extended techniques to enhance the rate of communication for people with motor and speech impairments who use Augmentative and Alternative Communication systems. There is an enormous diversity of prediction methods and techniques mentioned in the literature. Therefore, the designer finds tremendous difficulties in understanding and comparing them in order to decide the most convenient technique for a specific design. This paper presents a survey on prediction techniques applied to communicators with the intention of helping them to understand this field. Prediction applications and related features, such as block size, dictionary structure, prediction method, interface, special features, measurement and results, are detailed. Systems found in the literature are studied and described. Finally, a discussion is carried out on the possible comparison among the different methods. {\textcopyright} Springer-Verlag 2004.},
author = {Garay-Vitoria, N. and Abascal, J.},
doi = {10.1007/978-3-540-30111-0_35},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garay-Vitoria, Abascal - 2004 - A comparison of prediction techniques to enhance the communication rate(3).pdf:pdf},
isbn = {978-3-540-23375-6},
issn = {03029743 16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {October},
pages = {400--417},
title = {{A comparison of prediction techniques to enhance the communication rate}},
volume = {3196},
year = {2004}
}
@article{Baber2012,
abstract = {This communication study explores the relation between commitment, relational maintenance strategies, and text message use in romantic partners. This study examines how these three factors are connected through the use of surveys. It was hypothesized that romantic partners who were more committed to one another would use text messages to communicate about certain relational maintenance strategies. Results showed romantic partners who used more relational maintenance strategies did in fact use text messages to communicate about these issues more often. Also, couples who were more committed to their partner did use more positivity when communicating through text messages with their partner. It was also found that males use the relational maintenance strategy of openness more often than females when communicating through text messages.},
author = {Baber, Vashaun M and Shen, Sam and Tew, Michael and Stacey, Kathleen and Ypsilanti, Michigan},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baber et al. - 2012 - Relational maintenance An examination of how gender, relational maintenance strategies, and commitment affect t(2).pdf:pdf},
title = {{Relational maintenance: An examination of how gender, relational maintenance strategies, and commitment affect the use of text messages in romantic relationships}},
year = {2012}
}
@article{Carmignani2006a,
author = {Carmignani, Nicola},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carmignani - 2006 - Predicting Words and Sentences using Statistical Models(3).pdf:pdf},
title = {{Predicting Words and Sentences using Statistical Models}},
year = {2006}
}
@article{Faulkner2005a,
abstract = {SMS or text messaging is an area of growth in the communications field. The studies described below consisted of a questionnaire and a diary study. The questionnaire was designed to examine texting activities in 565 users of the mobile phone. The diary study was carried out by 24 subjects over a period of 2 weeks. The findings suggest that text messaging is being used by a wide range of people for all kinds of activities and that for some people it is the preferred means of communication. These studies should prove interesting for those examining the use and impact of SMS. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Faulkner, Xristine and Culwin, Fintan},
doi = {10.1016/j.intcom.2004.11.002},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faulkner, Culwin - 2005 - When fingers do the talking A study of text messaging(3).pdf:pdf},
isbn = {0953-5438},
issn = {09535438},
journal = {Interacting with Computers},
keywords = {Communication,E-mail,SMS,Text messaging},
title = {{When fingers do the talking: A study of text messaging}},
year = {2005}
}
@misc{Bekiempis2014a,
author = {Bekiempis, Victoria},
booktitle = {Newsweek},
title = {{Nearly 1 in 5 Americans Sufers from Mental Illness Each Year}},
url = {http://www.newsweek.com/nearly-1-5-americans-suffer-mental-illness-each-year-230608},
year = {2014}
}
@article{Owoputi2013a,
abstract = {We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90{\%} to 93{\%} accuracy (more than 3{\%} absolute). Quali- tative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at: http://www.ark.cs.cmu.edu/TweetNLP This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data},
author = {Owoputi, Olutobi and O'Connor, Brendan and Dyer, Chris and Gimpel, Kevin and Schneider, Nathan and Smith, Noah a},
doi = {10.1177/001316446002000104},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Owoputi et al. - 2013 - Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters(3).pdf:pdf},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT 2013},
number = {June},
pages = {380--390},
title = {{Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters}},
year = {2013}
}
@article{Faulkner2005,
abstract = {SMS or text messaging is an area of growth in the communications field. The studies described below consisted of a questionnaire and a diary study. The questionnaire was designed to examine texting activities in 565 users of the mobile phone. The diary study was carried out by 24 subjects over a period of 2 weeks. The findings suggest that text messaging is being used by a wide range of people for all kinds of activities and that for some people it is the preferred means of communication. These studies should prove interesting for those examining the use and impact of SMS. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Faulkner, Xristine and Culwin, Fintan},
doi = {10.1016/j.intcom.2004.11.002},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faulkner, Culwin - 2005 - When fingers do the talking A study of text messaging(3).pdf:pdf},
isbn = {0953-5438},
issn = {09535438},
journal = {Interacting with Computers},
keywords = {Communication,E-mail,SMS,Text messaging},
number = {2},
pages = {167--185},
title = {{When fingers do the talking: A study of text messaging}},
volume = {17},
year = {2005}
}
@article{Chelba1998,
abstract = {The paper presents a language model that devel-ops syntactic structure and uses it to extract mean-ingful information from the word history, thus en-abling the use of long distance dependencies. The model assigns probability to every joint sequence of words–binary-parse-structure with headword an-notation and operates in a left-to-right manner — therefore usable for automatic speech recognition. The model, its probabilistic parameterization, and a set of experiments meant to evaluate its predictive power are presented; an improvement over standard trigram modeling is achieved.},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/9811022v2},
author = {Chelba, Ciprian and Jelinek, Frederick},
eprint = {9811022v2},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chelba, Jelinek - 1998 - Exploiting Syntactic Structure for Language Modeling.pdf:pdf;:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chelba, Jelinek - Unknown - Exploiting Syntactic Structure for Language Modeling(3).pdf:pdf},
journal = {Proceedings of the Thirty-Sixth Annual Meeting of the {\{}A{\}}ssociation for {\{}C{\}}omputational {\{}L{\}}inguistics and Seventeenth International Conference on Computational Linguistics},
pages = {225--231},
primaryClass = {arXiv:cs},
title = {{Exploiting Syntactic Structure for Language Modeling}},
url = {https://arxiv.org/pdf/cs/9811022.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.556.577{\&}rep=rep1{\&}type=pdf},
year = {1998}
}
@article{Ghayoomia,
abstract = {—The prediction task in national language processing means to guess the missing letter, word, phrase, or sentence that likely follow in a given segment of a text. Since 1980s many systems with different methods were developed for different languages. In this paper an overview of the existing prediction methods that have been used for more than two decades are described and a general classification of the approaches is presented. The three main categories of the classification are statistical modeling, knowledge-based modeling, and heuristic modeling (adaptive).},
author = {Ghayoomi, Masood and Momtazi, Saeedeh},
doi = {10.1109/ICSMC.2009.5346027},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Momtazi - 2009 - An overview on the existing language models for prediction systems as writing assistant tools.pdf:pdf;:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Momtazi - Unknown - An Overview on the Existing Language Models for Prediction Systems as Writing Assistant Tools(3).pdf:pdf},
isbn = {9781424427949},
issn = {1062922X},
journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
keywords = {Assistant technology,Index Terms—Word prediction,Lan-guage modeling,Language modeling,Word prediction},
pages = {5083--5087},
title = {{An Overview on the Existing Language Models for Prediction Systems as Writing Assistant Tools}},
url = {http://hpsg.fu-berlin.de/{~}ghayoomi/Publishedpapers/ghayoomi-2009[3].pdf},
year = {2009}
}
@article{VandenBosch2008a,
abstract = {Word completion is a basic technology for reducing the effort involved in text entry on mobile devices and in augmentative communication devices, where efficiency and ease of use are needed, but where a low memory footprint is also required. Standard solutions compress a lexicon into a suffix tree with a small memory footprint and high retrieval speed. Keystroke savings, a measurable correlate of text entry effort gain, typically improve when the algorithm would also take into account the previous word; however, this comes at the cost of a large footprint. We develop two word completion algorithms that encode the previous word in the input. The first algorithm utilizes a character buffer that includes a fixed number of recent keystrokes, including those belonging to previous words. The second algorithm includes the complete previous word as an extra input feature. In simulation studies, the first algorithm yields marked improvements in keystroke savings, but has a large memory footprint. The second algorithm can be tuned by frequency thresholding to have a small footprint, and be less than one order of magnitude slower than the baseline system, while its keystroke savings improve over the baseline.},
author = {van den Bosch, Antal and Bogers, Toine},
doi = {10.1145/1409240.1409315},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van den Bosch, Bogers - 2008 - Efficient context-sensitive word completion for mobile devices(3).pdf:pdf},
isbn = {9781595939524},
journal = {Proceedings of the 10th international conference on Human computer interaction with mobile devices and services - MobileHCI '08},
keywords = {con-,ergonomics,mobile devices,predictive text processing,scaling,text sensitivity,word completion},
pages = {465},
title = {{Efficient context-sensitive word completion for mobile devices}},
url = {http://portal.acm.org/citation.cfm?doid=1409240.1409315},
year = {2008}
}
@article{Stolcke2002,
abstract = {SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools.},
author = {Stolcke, Andreas},
doi = {10.1.1.157.2429},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stolcke - 2002 - Srilm — an Extensible Language Modeling Toolkit(2).pdf:pdf},
journal = {Interspeech},
number = {Denver, Colorado},
pages = {901--904},
title = {{Srilm — an Extensible Language Modeling Toolkit}},
url = {http://www.speech.sri.com/ http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.7.461{\&}rep=rep1{\&}type=pdf},
volume = {2},
year = {2002}
}
@article{Stolcke2011,
abstract = {The SRI Language Modeling Toolkit (SRILM for short) is an open source software toolkit for statistical language modeling and related tasks. It was first conceived and implemented— with minimal functionality—in 1995, followed by a first public (beta) release in 1999. Since then SRILM has found wide use in the speech and natural language research community. A 2002 paper [1] presented an overview of the toolkit's design and functionality, of which we provide only a brief summary here. This paper takes stock of SRILM developments since then, as well as extensions and applications of SRILM. We also summarize activities in the SRILM user community, give an overall assessment of developments so far, and point out possible future directions.},
author = {Stolcke, Andreas and Zheng, Jing and Wang, Wen and Abrash, Victor},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stolcke et al. - 2011 - SRILM at Sixteen Update and Outlook(2).pdf:pdf},
journal = {Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
pages = {5--9},
title = {{SRILM at Sixteen : Update and Outlook}},
url = {https://www.researchgate.net/profile/Victor{\_}Abrash/publication/255563494{\_}SRILM{\_}at{\_}sixteen{\_}update{\_}and{\_}outlook/links/54b6a3bc0cf2e68eb27ebf14.pdf},
year = {2011}
}
@incollection{Jurafsky2016,
abstract = {CHAPTER 4 Language Modeling with N-grams " You are uniformly charming! " cried he, with a smile of associating and now and then I bowed and they perceived a chaise and four to wish for. Random sentence generated from a Jane Austen trigram model Being able to predict the future is not always a good thing. Cassandra of Troy had the gift of foreseeing but was cursed by Apollo that her predictions would never be believed. Her warnings of the destruction of Troy were ignored and to simplify, let's just say that things just didn't go well for her later. In this chapter we take up the somewhat less fraught topic of predicting words. What word, for example, is likely to follow Please turn your homework ... Hopefully, most of you concluded that a very likely word is in, or possibly over, but probably not refrigerator or the. In the following sections we will formalize this intuition by introducing models that assign a probability to each possible next word. The same models will also serve to assign a probability to an entire sentence. Such a model, for example, could predict that the following sequence has a much higher probability of appearing in a text: all of a sudden I notice three guys standing on the sidewalk},
author = {Jurafsky, Daniel and Martin, James R},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jurafsky, Martin - 2016 - Language Modeling with N- grams.pdf:pdf},
isbn = {0131873210},
title = {{Language Modeling with N- grams}},
year = {2016}
}
@article{Khmaladze1988,
author = {Khmaladze, E. V.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khmaladze - 1988 - The statistical analysis of a large number of rare events(2).pdf:pdf},
journal = {Department of Mathematical Statistics},
language = {en},
month = {jan},
number = {R 8804},
pages = {1--21},
publisher = {CWI},
title = {{The statistical analysis of a large number of rare events}},
url = {http://oai.cwi.nl/oai/asset/5988/5988A.pdf},
year = {1988}
}
@book{Zipf:36,
address = {London},
author = {Zipf, George K},
publisher = {Routledge},
title = {{The Psychobiology of Language: An Introduction to Dynamic Philology}},
year = {1936}
}
@article{Gimpel2011a,
abstract = {We address the problem of part-of-speech tagging for English data from the popular micro-blogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90{\%} accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.},
author = {Gimpel, Kevin and Schneider, Nathan and O'Connor, Brendan and Das, Dipanjan and Mills, Daniel and Eisenstein, Jacob and Heilman, Michael and Yogatama, Dani and Flanigan, Jeffrey and Smith, Noah A.},
doi = {10.1.1.206.3224},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gimpel et al. - 2011 - Part-of-Speech Tagging for Twitter Annotation, Features, and Experiments(2).pdf:pdf},
isbn = {978-1-932432-88-6},
journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Shortpapers},
number = {2},
pages = {42--47},
title = {{Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments}},
year = {2011}
}
@misc{Nga,
abstract = {Machine learning is the science of getting computers to act without being explicitly programmed. In the past decade, machine learning has given us self-driving cars, practical speech recognition, effective web search, and a vastly improved understanding of the human genome. Machine learning is so pervasive today that you probably use it dozens of times a day without knowing it. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself. More importantly, you'll learn about not only the theoretical underpinnings of learning, but also gain the practical know-how needed to quickly and powerfully apply these techniques to new problems. Finally, you'll learn about some of Silicon Valley's best practices in innovation as it pertains to machine learning and AI.},
author = {Ng, Andrew},
title = {{Machine Learning | Coursera}},
url = {https://www.coursera.org/learn/machine-learning/home/info},
urldate = {2017-09-16}
}
@techreport{Buttorff2017,
abstract = {This chartbook updates previous versions with more recent data on the prevalence of multiple chronic conditions (2008–2014) and associated health care utilization and spending. It also analyzes functional and other limitations for those with multiple chronic conditions. In 2014, 60 percent of Americans had at least one chronic condition, and 42 percent had multiple chronic conditions. These proportions have held steady since 2008. Americans with chronic conditions utilize more — and spend more on — health care services and may have reduced physical and social functioning.},
author = {Buttorff, Christine and Ruder, Teague and Bauman, Melissa},
doi = {10.7249/TL221},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buttorff, Ruder, Bauman - 2017 - Multiple Chronic Conditions in the United States.pdf:pdf},
isbn = {9780833097378},
title = {{Multiple Chronic Conditions in the United States}},
url = {http://www.fightchronicdisease.org/sites/default/files/TL221{\_}final.pdf http://www.rand.org/pubs/tools/TL221.html},
year = {2017}
}
@article{Liu2006,
abstract = {Web text has been successfully used as training data for many NLP applications. While most previous work accesses web text through search engine hit counts, we created a Web Corpus by downloading web pages to create a topic-diverse collec- tion of 10 billion words of English. We show that for context-sensitive spelling correction theWeb Corpus results are bet- ter than using a search engine. For the- saurus extraction, it achieved similar over- all results to a corpus of newspaper text. With many more words available on the web, better results can be obtained by col- lecting much larger web corpora.},
author = {Liu, Vinci and Curran, James R},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Curran - 2006 - Web Text Corpus for Natural Language Processing.pdf:pdf},
isbn = {1932432590},
journal = {Eacl},
pages = {233--240},
title = {{Web Text Corpus for Natural Language Processing}},
url = {http://anthology.aclweb.org/E/E06/E06-1030.pdf},
year = {2006}
}
@article{Zhu,
author = {Zhu, Xiaojin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu - Unknown - CS769 Spring 2010 Advanced Natural Language Processing Basic Text Process(2).pdf:pdf},
title = {{CS769 Spring 2010 Advanced Natural Language Processing Basic Text Process}}
}
@article{Edward1996a,
author = {Edward, Matthew and Wood, John},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Edward, Wood - 1996 - Matthew Edward John Wood(2).pdf:pdf},
keywords = {text input speed,word prediction},
number = {June},
title = {{Matthew Edward John Wood}},
year = {1996}
}
@article{Garay-Vitoria2010,
abstract = {Text prediction was initially proposed to help people with a low text composition speed to enhance their message composition. After the important advancements obtained in the last years, text prediction methods may nowadays benefit anyone trying to input text messages or commands, if they are adequately integrated within the user interface of the application. Diverse text prediction methods are based in different statistic and linguistic properties of natural languages. Hence, they are very dependent on the language concerned. In order to discuss general issues of text prediction it is necessary to propose abstract descriptions of the methods used. In this paper a number of models applied to text prediction are presented. Some of them are oriented to low-inflected languages while others are for high-inflected languages. All these models have been implemented and their results are compared. Presented models may be useful for future discussion. Finally, some comments related to the comparison of previously published results are also done. ?? 2009 Elsevier Ltd. All rights reserved.},
author = {Garay-Vitoria, Nestor and Abascal, Julio},
doi = {10.1016/j.csl.2009.03.008},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garay-Vitoria, Abascal - 2010 - Modelling text prediction systems in low- and high-inflected languages(3).pdf:pdf},
issn = {08852308},
journal = {Computer Speech and Language},
keywords = {Anticipative interfaces,Communication speed enhancement,Prediction measures,Prediction models,Text prediction},
number = {2},
pages = {117--135},
title = {{Modelling text prediction systems in low- and high-inflected languages}},
volume = {24},
year = {2010}
}
@article{Link2006,
abstract = {Statistical thinking in wildlife biology and ecology has been profoundly influenced by the introduction of AIC (Akaike's information criterion) as a tool for model selection and as a basis for model averaging. In this paper, we advocate the Bayesian paradigm as a broader framework for multimodel inference, one in which model averaging and model selection are naturally linked, and in which the performance of AIC-based tools is naturally evaluated. Prior model weights implicitly associated with the use of AIC are seen to highly favor complex models: in some cases, all but the most highly parameterized models in the model set are virtually ignored a priori. We suggest the usefulness of the weighted BIC (Bayesian information criterion) as a computationally simple alternative to AIC, based on explicit selection of prior model probabilities rather than acceptance of default priors associated with AIC. We note, however, that both procedures are only approximate to the use of exact Bayes factors. We discuss and illustrate technical difficulties associated with Bayes factors, and suggest approaches to avoiding these difficulties in the context of model selection for a logistic regression. Our example highlights the predisposition of AIC weighting to favor complex models and suggests a need for caution in using the BIC for computing approximate posterior model weights.},
author = {Link, William A and Barker, Richard J},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Link, Barker - 2006 - MODEL WEIGHTS AND THE FOUNDATIONS OF MULTIMODEL INFERENCE.pdf:pdf},
journal = {Ecology},
keywords = {AIC,Akaike's information criterion,BIC,Bayes factors,Bayesian inference,Bayesian information criterion,Salmo trutta,model averaging,model selection},
number = {10},
pages = {2626--2635},
title = {{MODEL WEIGHTS AND THE FOUNDATIONS OF MULTIMODEL INFERENCE}},
url = {https://www.pwrc.usgs.gov/prodabs/ab10060307/6623{\_}Link.pdf},
volume = {87},
year = {2006}
}
@article{Tan2012,
abstract = {This paper presents an attempt at building a large scale distributed composite language model that is formed by seamlessly integrating an n-gram model, a structured language model, and probabilistic latent semantic analysis under a directed Markov random field paradigm to simultaneously account for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm and a follow-up EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over n-grams and achieves significantly better translation quality measured by the Bleu score and "readability" of translations when applied to the task of re-ranking the N-best list from a state-of-the-art parsing-based machine translation system.},
author = {Tan, Ming and Zhou, Wenli and Zheng, Lei and Wang, Shaojun},
doi = {10.1162/COLI_a_00107},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tan et al. - 2012 - A Scalable Distributed Syntactic, Semantic, and Lexical Language Model.pdf:pdf},
isbn = {1530-9312},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {3},
pages = {631--671},
title = {{A Scalable Distributed Syntactic, Semantic, and Lexical Language Model}},
volume = {38},
year = {2012}
}
@article{Hasan2012,
abstract = {-Smoothing techniques is the utmost possibility estimate of probabilities to produce more precise probabilities. Smoothing is one of the most significant techniques while constructing a language model with a limited number of training data. In this paper, our main aim to analyze the performance of different smoothing techniques on n-grams. For language modeling, we considered two most widely-used smoothing algorithms: Witten-Bell smoothing and Kneser-Ney smoothing. For the evaluation we use BLEU (Bilingual Evaluation Understudy) and NIST (National Institute of Standards and Technology) scoring techniques. We have done the evaluation of these models is performed by comparing the automatically produced word alignment. We use Moses Statistical Machine Translation System for our work. Our machine translation approach has been tested on German to English and English to German task. The obtain results are considerably better than those obtained with alternative approaches to machine translation. This paper addresses several aspects of Statistical Machine Translation (SMT).},
author = {Hasan, A S M Mahmudul and Islam, Saria and Rahman, M Arifur},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasan, Islam, Rahman - 2012 - A Comparative Study of Witten Bell and Kneser-Ney Smoothing Methods for Statistical Machine Translation.pdf:pdf},
journal = {JU Journal of Information Technology},
number = {June},
title = {{A Comparative Study of Witten Bell and Kneser-Ney Smoothing Methods for Statistical Machine Translation}},
url = {http://www.juniv.edu/jujit/files/2012/09/1{\_}new.pdf},
volume = {1},
year = {2012}
}
@misc{Google,
author = {Google},
title = {{Google Books Ngram Datasets}},
url = {http://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html},
urldate = {2017-03-17},
year = {2013}
}
@article{Evert2004,
abstract = {This paper describes a population model for word frequency distributions based on the Zipf-Mandelbrot law, corresponding to the word frequency distribution induced by a random character sequence. The model, which has convenient analytical and numerical properties, is shown to be adequate for the description of language data extracted by automatic means from large text corpora. It can thus be used to study the problems faced by the statistical analysis of such data in the field of natural-language processing.},
author = {Evert, Stefan},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evert - 2004 - A simple LNRE model for random character sequences.pdf:pdf},
isbn = {2-930344-49-0},
journal = {Proceedings of JADT},
keywords = {1 introduction to lexical,1 this,area of lexical statistics,cooccurrence statistics,is based on random,lexical statistics,lnre models,model assumes a population,most work in the,of types w 1,probabilities $\pi$ 1,random text,s,sampling with replacement,statistics and lnre models,w s with occurrence,zipf-mandelbrot law,$\pi$ s},
pages = {1--12},
title = {{A simple LNRE model for random character sequences}},
url = {http://www.cogsci.uni-osnabrueck.de/{~}severt/PUB/Evert2004a.pdf},
year = {2004}
}
@article{Matiasek2003,
abstract = {In this paper, we report about some preliminary experiments in which we tried to improve the performance of a state-of-the-art Predictive Typing sys-tem for the German language by adding a collocation-based prediction compo-nent. This component tries to ex-ploit the fact that texts have a topic and are semantically coherent. Thus, the appearance in a text of a certain word can be a cue that other, semanti-cally related words are likely to appear soon. The collocation-based module ex-ploits this kind of topical/semantic re-latedness by relying on statistics about the co-occurrence of words within a large window of text in the training corpus. Our current experimental re-sults indicate that using the collocation-based prediction module has a small but consistent positive effect on the perfor-mance of the system.},
author = {Matiasek, Johannes and Baroni, Marco},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matiasek, Baroni - 2003 - Exploiting Long Distance Collocational Relations in Predictive Typing.pdf:pdf},
journal = {Proceedings of the EACL Workshop on Language Modeling for Text Entry Methods},
keywords = {text completion,word completion},
pages = {1--8},
title = {{Exploiting Long Distance Collocational Relations in Predictive Typing}},
year = {2003}
}
@article{Aliprandi2007a,
author = {Aliprandi, Carlo and Carmignani, Nicola and Mancarella, Paolo},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aliprandi, Carmignani, Mancarella - 2007 - An Inflected-Sensitive Letter and Word Prediction System(3).pdf:pdf},
journal = {System},
keywords = {2006,2007,accepted,alternative and,and particularly difficult to,assistive technology,augmentative communication,august 5,computer aided language learning,december 30,june 10,natural language processing,nlp,received,revised,this is significant,users,word prediction},
number = {2},
pages = {79 -- 85},
title = {{An Inflected-Sensitive Letter and Word Prediction System}},
volume = {5},
year = {2007}
}
@misc{TheRFoundation2016,
author = {{The R Foundation}},
title = {{The Comprehensive R Archive Network}},
url = {https://cran.r-project.org/},
year = {2016}
}
@article{Clyde2017,
abstract = {Depends R ({\textgreater}= 3.0), Imports stats, graphics, utils, grDevices Suggests MASS, knitr, GGally, rmarkdown, roxygen2 Description Package for Bayesian Variable Selection and Model Averaging in linear models and generalized linear models using stochastic or deterministic sampling without replacement from posterior distributions. Prior distributions on coefficients are from Zellner's g-prior or mixtures of g-priors corresponding to the Zellner-Siow Cauchy Priors or the mixture of g-priors from Liang et al (2008) {\textless}DOI:10.1198/016214507000001337{\textgreater} for linear models or mixtures of g-priors in GLMs of Li and Clyde (2015) {\textless}arXiv:1503.06913{\textgreater}. Other model selection criteria include AIC, BIC and Empirical Bayes estimates of g. Sampling probabilities may be updated based on the sampled models using Sampling w/out Replacement or an efficient MCMC algorithm samples models using the BAS tree structure as an efficient hash table. Uniform priors over all models or beta-binomial prior distributions on model size are allowed, and for large p truncated priors on the model space may be used. The user may force variables to always be included. Details behind the sampling algorithm are provided in Clyde, Ghosh and Littman (2010) {\textless}DOI:10.1198/jcgs.},
author = {Clyde, Merlise},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2017 - Bayesian Variable Selection and Model Averaging using Bayesian Adaptive Sampling.pdf:pdf},
title = {{Bayesian Variable Selection and Model Averaging using Bayesian Adaptive Sampling}},
url = {https://github.com/merliseclyde/BAS},
year = {2017}
}
@article{Mann,
author = {Mann, H.B. and D.R., Whitney},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mann, D.R. - Unknown - On a Test of Whether One of Two Random Variables is Stochastically Larger Than The Other.pdf:pdf},
title = {{On a Test of Whether One of Two Random Variables is Stochastically Larger Than The Other}},
url = {http://projecteuclid.org/download/pdf{\_}1/euclid.aoms/1177730491}
}
@book{Cramer1946,
abstract = {In this classic of statistical mathematical theory, Harald Cram{\{}{\'{e}}{\}}r joins the two major lines of development in the field: while British and American statisticians were developing the science of statistical inference, French and Russian probabilitists transformed the classical calculus of probability into a rigorous and pure mathematical theory. The result of Cram{\{}{\'{e}}{\}}r's work is a masterly exposition of the mathematical methods of modern statistics that set the standard that others have since sought to follow. For anyone with a working knowledge of undergraduate mathematics the book is self contained. The first part is an introduction to the fundamental concept of a distribution and of integration with respect to a distribution. The second part contains the general theory of random variables and probability distributions while the third is devoted to the theory of sampling, statistical estimation, and tests of significance.},
author = {Hoel, Paul G. and Wolfowitz, J. and Cramer, Harald},
booktitle = {Journal of the American Statistical Association},
doi = {10.2307/2280199},
isbn = {9780691005478},
issn = {01621459},
number = {237},
pages = {174},
title = {{Mathematical Methods of Statistics.}},
url = {http://www.jstor.org/stable/2280199?origin=crossref},
volume = {42},
year = {1947}
}
@incollection{Mandelbrot61,
address = {New York},
author = {Mandelbrot, B B},
booktitle = {Structures of Language and its Mathematical Aspects},
editor = {Jacobsen, R},
keywords = {bibtex-import},
publisher = {American Mathematical Society},
title = {{On the theory of word frequencies and on related Markovian models of discourse}},
year = {1961}
}
@article{Thurlow2003,
abstract = {Abstract: The so called 'net generation' is popularly assumed to be naturally media literate and to be necessarily reinventing conventional linguistic and communicative practices. With this in mind, this essay centres around discursive analyses of qualitative data arising from an investigation of 159 older teenagers' use of mobile telephone text-messaging - or SMS (i.e. short-messaging services). In particular, against a backdrop of media commentaries, we examine the linguistic forms and communicative functions in a corpus of 544 participants' actual text-messages. While young people are surely using their mobile phones as a novel, creative means of enhancing and supporting intimate relationships and existing social networks, popular discourses about the linguistic exclusivity and impenetrability of this particular technologically-mediated discourse appear greatly exaggerated. Serving the sociolinguistic 'maxims' of (a) brevity and speed, (b) paralinguistic restitution and (c) phonological approximation, young people's messages are both linguistically unremarkable and communicatively adept.},
author = {Thurlow, Crispin},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thurlow - 2003 - Generation Txt The sociolinguistics of young people ' s text-messaging.pdf:pdf},
isbn = {1477-7843},
issn = {14777843},
journal = {Discourse Analysis Online},
keywords = {1,adolescents,all tables,communication technologies,computer-mediated discourse,figures and images are,introduction and background,multimedia,new,presented in pdf format,sms,sociolinguistics,text-messaging},
pages = {1--31},
title = {{Generation Txt ? The sociolinguistics of young people ' s text-messaging}},
url = {http://extra.shu.ac.uk/daol/articles/v1/n1/a3/thurlow2002003-paper.html},
year = {2003}
}
@article{Manning2014,
abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Manning, Christopher D and Bauer, John and Finkel, Jenny and Bethard, Steven J and Surdeanu, Mihai and McClosky, David},
doi = {10.3115/v1/P14-5010},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Manning et al. - 2014 - The Stanford CoreNLP Natural Language Processing Toolkit.pdf:pdf},
isbn = {9781941643006},
issn = {1098-6596},
journal = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
pages = {55--60},
pmid = {25246403},
title = {{The Stanford CoreNLP Natural Language Processing Toolkit}},
url = {http://aclweb.org/anthology/P14-5010},
year = {2014}
}
@article{10.2307/1164823,
author = {Essex-Sorlie, Diane},
issn = {03629791},
journal = {Journal of Educational Statistics},
number = {1},
pages = {72--77},
publisher = {[Sage Publications, Inc., American Educational Research Association, American Statistical Association]},
title = {{No Title}},
url = {http://www.jstor.org/stable/1164823},
volume = {15},
year = {1990}
}
@article{Hasselgren2003,
abstract = {Due to the emergence of SMS messages, the significance of effective text entry on limited-size keyboards has increased. In this paper, we describe and discuss a new method to enter text more efficiently using a mobile telephone keyboard. This method, which we called HMS, predicts words from a sequence of keystrokes using a dictionary and a function combining bigram frequencies and word length. We implemented the HMS text entry method on a software-simulated mobile telephone keyboard and we...},
author = {Hasselgren, Jon and Montnemery, E},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasselgren, Montnemery - 2003 - Hms A predictive text entry method using bigrams(2).pdf:pdf},
journal = {{\ldots} for Text Entry Methods},
pages = {43--50},
title = {{Hms: A predictive text entry method using bigrams}},
url = {http://dl.acm.org/citation.cfm?id=1628201},
year = {2003}
}
@article{Evert2005a,
author = {Evert, Stefan and Baroni, Marco},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Evert, Baroni - 2005 - Testing the extrapolation quality of word frequency models.pdf:pdf},
issn = {1747-939},
journal = {Proceedings of Corpus Linguistics 2005},
pages = {1747--939},
title = {{Testing the extrapolation quality of word frequency models}},
url = {http://www.stefan-evert.de/PUB/EvertBaroni2005.pdf},
year = {2005}
}
@article{Fazly2003,
abstract = {We investigate the effect of incorporat-ing syntactic information into a word-completion algorithm. We introduce lwo new algorithms lhal combine parl-of-speech tag trigrams with word bi-grams, and evaluate them with a test-bench constructed for the purpose. The results show a small but statistically sig-nificant improvement in keystroke sav-ings for one of our algorithms over base-lines that use only word n-grams.},
author = {Fazly, Afsaneh and Hirst, Graeme},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fazly, Hirst - 2003 - Testing the efficacy of part-of-speech information in word completion(2).pdf:pdf},
journal = {Eacl},
number = {1991},
pages = {9--16},
title = {{Testing the efficacy of part-of-speech information in word completion}},
url = {http://www.aclweb.org/anthology/W03-2502},
year = {2003}
}
@book{Goddard2002,
address = {Amsterdam},
author = {Goddard, Cliff},
pages = {5--40},
publisher = {John Benjamins},
title = {{Meaning and Universal Grammar: Theory and Empirical Findings}},
url = {http://www.une.edu.au/lcl/nsm/pdf/Goddard{\_}Ch1{\_}2002.pdf},
year = {2002}
}
@article{Meyer2013,
abstract = {Lista de funciones del package vcd de R.},
author = {Meyer, David and Zeileis, Achim and Hornik, Kurt and Gerber, Florian and Friendly, Michael},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meyer et al. - 2013 - Package ‘ vcd '.pdf:pdf},
keywords = {R STAT,STATISTICAL,Software,Software Manual},
pages = {123},
title = {{Package ‘ vcd '}},
url = {https://cran.r-project.org/web/packages/vcd/vcd.pdf},
year = {2013}
}
@article{Chelba2008,
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0001020v1},
author = {Chelba, Ciprian},
eprint = {0001020v1},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chelba - 2008 - Exploiting Syntactic Structure for Natural Language Modeling(2).pdf:pdf},
number = {December},
primaryClass = {arXiv:cs},
title = {{Exploiting Syntactic Structure for Natural Language Modeling}},
year = {2008}
}
@article{Ripley2014,
author = {Ripley, Brian},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2017 - Package 'MASS' Title Support Functions and Datasets for Venables and Ripley's MASS(2).pdf:pdf},
title = {{Package 'MASS' Title Support Functions and Datasets for Venables and Ripley's MASS}},
url = {http://www.stats.ox.ac.uk/pub/MASS4/},
year = {2014}
}
@article{Goodman2008,
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0108005v1},
author = {Goodman, Joshua T},
eprint = {0108005v1},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodman - 2001 - A Bit of Progress in Language Modeling Extended Version(2).pdf:pdf},
primaryClass = {arXiv:cs},
title = {{A Bit of Progress in Language Modeling Extended Version}},
url = {http://www.research.microsoft.com},
year = {2008}
}
@article{Duggan2013,
abstract = {Fully 91{\%} of American adults own a cell phone and many use the devices for much more than phone calls. In our most recent nationally representative survey, we checked in on some of the most popular activities people perform on their cell phones: Cell phone activities The {\%} of cell phone owners who use their cell phone to{\ldots} 81 send or receive text messages 60 access the internet 52 send or receive email 50 download apps 49 get directions, recommendations, or other location-based information 48 listen to music 21 participate in a video call or video chat 8 “check in” or share your location Source: Pew Research Center's Internet {\&} American Life Project Spring Tracking Survey, April 17 – May 19, 2013. N=2,076 cell phone owners. Interviews were conducted in English and Spanish and on landline and cell phones. The margin of error for results based on all cell phone owners is +/- 2.4 percentage points. Texting, accessing the internet and sending and receiving email remain popular. Some 50{\%} of cell owners download apps—up from 22{\%} in 2009. Many use certain location-based services like getting directions or recommendations. Nearly half of cell owners (48{\%}) use their phones to listen to music. The proportion of cell owners who use video calling has tripled since May 2011. Overall, almost all activities have seen steady upward growth over time},
author = {Duggan, Maeve},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duggan - 2013 - Cell Phone Activities 2013(2).pdf:pdf},
pages = {18},
title = {{Cell Phone Activities 2013}},
url = {http://pewinternet.org/{~}/media//Files/Reports/2013/PIP{\_}Cell Phone Activities May 2013.pdf{\%}5Cnhttp://www.pewinternet.org/files/old-media//Files/Reports/2013/PIP{\_}Cell Phone Activities May 2013.pdf},
year = {2013}
}
@article{Trnka2006,
abstract = {Word prediction can be used for enhancing the communication ability of persons with speech and language impairments. In this work, we explore two methods of adapting a language model to the topic of conversation, and apply these methods to the prediction of fringe words.},
author = {Trnka, Keith and Yarrington, Debra and McCoy, Kathleen and Pennington, Christopher},
doi = {10.1145/1111449.1111509},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka et al. - 2006 - Topic modeling in fringe word prediction for AAC(2).pdf:pdf},
isbn = {1595932879},
journal = {Proceedings of the 11th international conference on Intelligent user interfaces - IUI '06},
keywords = {language mod-,topic modeling,word prediction},
pages = {276},
title = {{Topic modeling in fringe word prediction for AAC}},
url = {http://portal.acm.org/citation.cfm?doid=1111449.1111509},
year = {2006}
}
@article{James2000,
author = {James, Frankie},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/James - 2000 - Modified Kneser-Ney Smoothing of n-gram Models Modified Kneser-Ney Smoothing of n-gram Models(2).pdf:pdf},
journal = {October},
number = {October},
pages = {0--17},
title = {{Modified Kneser-Ney Smoothing of n-gram Models Modified Kneser-Ney Smoothing of n-gram Models}},
year = {2000}
}
@article{Communication2010,
author = {Communication, Alternative and Yarmohammadi, Mahsa A},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Communication, Yarmohammadi - 2010 - Language Modeling and Word Prediction Papers for Today(2).pdf:pdf},
title = {{Language Modeling and Word Prediction Papers for Today}},
year = {2010}
}
@book{Katamba2005,
abstract = {2nd ed. 'English Words' assumes no prior knowledge of linguistics in introducing the vocabulary of modern English usage. It covers meaning, history, pronunciation {\&} the structure of words. This new edition has been extensively updated with new chapters, new exercises, an improved index {\&} links to web resources. 1. Introduction -- 2. What is a word? -- 3. Close encounters of a morphemic kind -- 4. Building words -- 5. lexicon with layers -- 6. Word meaning -- 7. lexical mosaic : sources of English vocabulary -- 8. Words galore : innovation and change -- 9. Should English be spelt as she is spoke? -- 10. Speech recognition -- 11. Speech production.},
author = {Katamba, Francis},
isbn = {0415298938},
pages = {322},
publisher = {Routledge},
title = {{English words : structure, history, usage}},
year = {2005}
}
@book{Wierzbicka1996,
abstract = {Includes previously published material rev. and expanded for this publication. 1. Introduction -- 2. Survey of Semantic Primitives -- 3. Universal Grammar: The Syntax of Universal Semantic Primitives -- 4. Prototypes and Invariant -- 5. Semantic Primitives and Semantic Fields -- 6. Semantics and "Primitive Thought" -- 7. Semantic Complexity and the Role of Ostension in the Acquisition of Concepts -- 8. Against "Against Definitions" -- 9. Semantics and Lexicography -- 10. Meaning of Colour Terms and the Universals of Seeing -- 11. Semantics of Natural Kinds -- 12. Semantics and Ethnobiology -- 13. Semantic Rules in Grammar -- 14. Semantic Basis for Grammatical Description and Typology: Transitivity and Reflexives -- 15. Comparing Grammatical Categories across Languages: The Semantics of Evidentials.},
author = {Wierzbicka, Anna.},
isbn = {0198700024},
pages = {500},
publisher = {Oxford University Press},
title = {{Semantics : primes and universals}},
year = {1996}
}
@article{Neunerdt2013a,
abstract = {—Using social media tools such as blogs and forums have become more and more popular in recent years. Hence, a huge collection of social media texts from different communities is available for accessing user opinions, e.g., for marketing studies or acceptance research. Typically, methods from Natural Language Processing are applied to social media texts to automatically recognize user opinions. A fundamental component of the linguistic pipeline in Natural Language Processing is Part-of-Speech tagging. Most state-of-the-art Part-of-Speech taggers are trained on newspaper corpora, which differ in many ways from non-standardized social media text. Hence, applying common taggers to such texts results in performance degradation. In this paper, we present extensions to a basic Markov model tagger for the annotation of social media texts. Considering the German standard Stuttgart/ ubinger TagSet (STTS), we distinguish 54 tag classes. Applying our approach improves the tagging accuracy for social media texts considerably, when we train our model on a combination of annotated texts from newspapers and Web comments.},
author = {Neunerdt, Melanie and Reyer, Michael and Mathar, Rudolf},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neunerdt, Reyer, Mathar - 2013 - A POS Tagger for Social Media Texts trained on web comments.pdf:pdf},
journal = {Polibits},
keywords = {German,Index Terms—Natural language processing,opinion mining,part-of-speech tagging},
number = {48},
pages = {61--68},
title = {{A POS Tagger for Social Media Texts trained on web comments}},
url = {http://polibits.ojs.gelbukh.com/ojs/index.php/polibits/article/viewFile/1783/1723},
year = {2013}
}
@book{Zipf1935,
abstract = {Frequency counts of phonemes, morphemes, and words in samples of written discourse in diverse languages are presented in support of the generalization that the more complex any speech element, the less frequently does it occur. Thus, the greater the frequency of occurrence of words, the less tends to be their average length, and the smaller also is the number of different words. The relation between frequency and number of different words is said to be expressed by the formula ab2 = k, in which a represents the number of different words of a given frequency and b the frequency. The relationship between the magnitude of speech elements and their frequency is attributed to the operation of a "law" of linguistic change: that as the frequency of phonemes or of linguistic forms increases, their magnitude decreases. There is thus a tendency to "maintain an equilibrium" between length and frequency, and this tendency rests upon an "underlying law of economy." Human beings strive to maintain an "emotional equilibrium" between variety and repetitiveness of environmental factors and behavior. A speaker's discourse must represent a compromise between variety and repetitiveness adapted to the hearer's "tolerable limits of change in maintaining emotional equilibrium." This accounts for the maintenance of the relationship ab2 = k; the exponent of b expresses this "rate of variegation."},
author = {Zipf, George Kingsley},
doi = {10.1097/00005053-193701000-00041},
isbn = {9780415209762},
issn = {0022-3018},
pages = {336},
pmid = {6891221},
publisher = {Houghton Mifflin Company},
title = {{The Psycho-Biology Of Language: AN INTRODUCTION TO DYNAMIC PHILOLOGY}},
url = {http://psycnet.apa.org/psycinfo/1935-04756-000},
volume = {ix},
year = {1935}
}
@book{hastie01statisticallearning,
address = {New York, NY, USA},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
publisher = {Springer New York Inc.},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
year = {2001}
}
@article{Ling2005,
abstract = {Text messaging–or texting–via mobile telephones has become a fixture in many parts of the world. The ability to cheaply send text messages on a mobile asynchronous basis was adopted first by teens and is now spreading to other parts of the population. This said, texting is not an intuitive process. The interface is difficult to master, and the technology is being pressed into areas for which it was not necessarily intended. It is into this arena that systems of predictive texting have been introduced.},
author = {Ling, Rich},
doi = {10.13140/RG.2.1.1922.6089},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ling - 2005 - The Length of Text Messages and Use of Predictive Texting Who Uses it and How Much Do They Have to Say.pdf:pdf},
journal = {Association of Internet Researchers},
number = {l},
pages = {1--18},
title = {{The Length of Text Messages and Use of Predictive Texting: Who Uses it and How Much Do They Have to Say?}},
volume = {4},
year = {2005}
}
@book{R.H.Baayen2008,
abstract = {Statistical analysis is a useful skill for linguists and psycholinguists, allowing them to understand the quantitative structure of their data. This textbook provides a straightforward introduction to the statistical analysis of language. Designed for linguists with a non-mathematical background, it clearly introduces the basic principles and methods of statistical analysis, using 'R', the leading computational statistics programme. The reader is guided step-by-step through a range of real data sets, allowing them to analyse acoustic data, construct grammatical trees for a variety of languages, quantify register variation in corpus linguistics, and measure experimental data using state-of-the-art models. The visualization of data plays a key role, both in the initial stages of data exploration and later on when the reader is encouraged to criticize various models. Containing over 40 exercises with model answers, this book will be welcomed by all linguists wishing to learn more about working with and presenting quantitative data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Baayen, R H and {R. H. Baayen}},
doi = {10.1558/sols.v2i3.471},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baayen - 2008 - Analyzing linguistic data A practical introduction to statistics using R.pdf:pdf},
isbn = {9780521882590},
issn = {17508657},
keywords = {R,linguistics,psycholinguistics,statistics},
number = {3},
pages = {368},
pmid = {25246403},
publisher = {Cambridge University Press},
title = {{Analyzing Linguistic Data: A Practical Introduction to Statistics using R}},
volume = {2},
year = {2008}
}
@incollection{Baayen2001,
address = {Dordrecht},
author = {Baayen, R Harald},
booktitle = {Word Frequency Distributions},
doi = {10.1007/978-94-010-0844-0_6},
isbn = {978-94-010-0844-0},
pages = {195--236},
publisher = {Springer Netherlands},
title = {{Examples of Applications}},
url = {http://dx.doi.org/10.1007/978-94-010-0844-0{\_}6},
year = {2001}
}
@article{VinhNguyen2017,
abstract = {Description Collection of functions to extract inferential values (point estimates, confidence intervals, p-values, etc) of a fitted model object into a matrix-like object that can be used for table/report generation; transform point estimates via the delta method. 1 2 infer,-methods inference-package Extract inferential information from different statistical models. Description Extract inferential information from different statistical models. Details This package provides functions to extract point estimates, standard errors, confidence intervals, p-values, and sample size from a fitted model in a matrix-like object. The purpose is to have all inferential numbers be readily accessible, especially in the construction of summary tables (R -{\textgreater} LaTeX -{\textgreater} html -{\textgreater} Word) for publication and collaboration.},
author = {{Vinh Nguyen}, Author and {Vinh Nguyen}, Maintainer},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vinh Nguyen, Vinh Nguyen - 2017 - Package 'inference' Title Functions to extract inferential values of a fitted model object.pdf:pdf},
title = {{Package 'inference' Title Functions to extract inferential values of a fitted model object}},
url = {http://r-forge.r-project.org/projects/inference/ http://lists.r-forge.r-project.org/mailman/listinfo/inference-devel},
year = {2017}
}
@article{Yihui2017,
author = {Yihui, Xie},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yihui - 2017 - Package printr(2).pdf:pdf},
title = {{Package printr}},
url = {https://cran.r-project.org/web/packages/printr/printr.pdf},
year = {2017}
}
@misc{TheRFoundation2015,
author = {{The R Foundation}},
booktitle = {Mathematical Statistics with Applications in R},
doi = {10.1016/B978-0-12-417113-8.09994-X},
isbn = {0387947256},
issn = {1609-3631},
pages = {745},
pmid = {21196786},
title = {{What is R?}},
url = {https://www.r-project.org/about.html http://www.r-project.org/about.html http://linkinghub.elsevier.com/retrieve/pii/B978012417113809994X},
year = {2015}
}
@misc{IMDbstats2015,
author = {IMDbstats},
booktitle = {IMDb.com},
pages = {14--15},
title = {{IMDb Database Statistics}},
url = {http://www.imdb.com/stats},
urldate = {2017-11-24},
volume = {009},
year = {2015}
}
@article{Cavalieri2015,
abstract = {This paper concentrates on improving a text-based human-machine interface integrated into a robotic wheelchair. Since word prediction is one of the most common methods used in such systems, the goal of this work is to improve the results using this specific module. For this, an expo‐ nential interpolation language model (LM) is considered. First, a model based on partial differential equations is proposed; with the appropriate initial conditions, we are able to design a interpolation language model that merges a word-based n-gram language model and a part-of-speech-based language model. Improvements in keystroke saving (KSS) and perplexity (PP) over the word-based n-gram language model and two other traditional interpola‐ tion models are obtained, considering two different task domains and three different languages. The proposed interpolation model also provides additional improve‐ ments over the hit rate (HR) parameter.},
author = {Cavalieri, DC and Bastos-Filho, Teodiano and Palazuelos-Cagigas, SE},
doi = {10.5772/61753},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cavalieri, Bastos-Filho, Palazuelos-Cagigas - 2015 - On Combining Language Models to Improve a Text-based Human-machine Interface.pdf:pdf},
journal = {International Journal of Advanced Robotic Systems},
keywords = {Communication Aid,Human-machine Interfaces,Language Modelling,Word Prediction Systems},
title = {{On Combining Language Models to Improve a Text-based Human-machine Interface}},
url = {http://journals.sagepub.com/doi/pdf/10.5772/61753 http://www.intechopen.com/journals/international{\_}journal{\_}of{\_}advanced{\_}robotic{\_}systems/on-combining-language-models-to-improve-a-text-based-human-machine-interface},
year = {2015}
}
@article{Brin1998,
abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from 3 years ago. This paper provides an in-depth description of our large-scale web search engine - the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections, where anyone can publish anything they want. ?? 2012 Elsevier B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Brin, S and Page, L},
doi = {10.1.1.109.4049},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brin, Page - 1998 - The anatomy of a large scale hypertextual Web search engine(2).pdf:pdf},
isbn = {0169-7552},
issn = {01697552},
journal = {Computer Networks and ISDN Systems},
keywords = {information retrieval,pagerank,search engines,world wide web},
number = {1/7},
pages = {107--17},
pmid = {726238533241861127},
title = {{The anatomy of a large scale hypertextual Web search engine}},
volume = {30},
year = {1998}
}
@article{Amini,
abstract = {Bayesian model averaging has increasingly witnessed applications across an array of empirical contexts. However, the dearth of available statistical software which allows one to engage in a model averaging exercise is limited. It is common for consumers of these methods to develop their own code, which has obvious appeal. However, canned statistical software can ameliorate one's own analysis if they are not intimately familiar with the nuances of computer coding. Moreover, many researchers would prefer user ready software to mitigate the inevitable time costs that arise when hard coding an econometric estimator. To that end, this paper describes the relative merits and attractiveness of several competing packages in the statistical environment R to implement a Bayesian model averaging exercise.},
author = {Amini, Shahram M and Parmeter, Christopher F},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amini, Parmeter - Unknown - BAYESIAN MODEL AVERAGING IN R.pdf:pdf},
title = {{BAYESIAN MODEL AVERAGING IN R}},
url = {https://core.ac.uk/download/pdf/6494889.pdf}
}
@misc{Wikipedia2016,
author = {Wikipedia},
booktitle = {Wikipedia},
title = {{Wikipedia:List of English contractions}},
url = {http://www.oxforddictionaries.com/us/definition/english/amn't},
urldate = {2016-11-30},
year = {2016}
}
@article{Edward1996,
abstract = {Many disabled persons in the world are unable to communicate verbally with those around them. This may be because they have motor-control diiculties and so cannot operate the muscles which actually produce the speech. Conversely, i t m a y be a neurophysiological problem, either congenital or as a result of trauma, resulting in an inability of the brain to create the speech in the place. Either way the diiculties inherent with such problems are immense, and anything which can be used to help the situation can bring welcome relief. Many people have n o v erbal communication skills at all and as such need some form of prosthetic aid known as an Augmentative and Alternative Communication AAC device. There are many forms of device from very basic pointing boards to complex computer systems and a device will be chosen to suit the needs and abilities of an individual. However, people with such diiculties often have their problems compounded by not being able to use the rest of their bodies properly either. If one cannot use one's voice box because of motor-control diiculties then it is very likely that one will not be able to co-ordinate limbs properly. As such, using a complex device can be very diicult, and needless to say time consuming. For instance, typing on a keyboard can become an almost impossible task. The most common method of operating an AAC device is with a single switch, selecting the letters one-by-one as the system scans through them on a screen or lightboard. This produces prohibitively slow communication rates and as a consequence, even using an ordinary AAC device, users cannot join in with a normal conversation. A method is required which will improve the rate at which users can participate in a conversation using an AAC device.},
author = {Edward, Matthew and Wood, John},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Edward, Wood - 1996 - Matthew Edward John Wood(2).pdf:pdf},
keywords = {text input speed,word prediction},
number = {June},
title = {{Matthew Edward John Wood}},
year = {1996}
}
@article{Kruskal1952,
author = {Kruskal, William H and Wallis, W Allen},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kruskal, Wallis - 1952 - Use of Ranks in One-Criterion Variance Analysis(4).pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {260},
pages = {583--621},
title = {{Use of Ranks in One-Criterion Variance Analysis}},
url = {http://links.jstor.org/sici?sici=0162-1459{\%}28195212{\%}2947{\%}3A260{\%}3C583{\%}3AUORIOV{\%}3E2.0.CO{\%}3B2-A},
volume = {47},
year = {1952}
}
@misc{Norvig2013,
author = {Norvig, Peter},
title = {{English Letter Frequency Counts: Mayzner Revisited or ETAOIN SRHLDCU}},
url = {http://norvig.com/mayzner.html http://norvig.com/mayzner.html (Archived at: http://www.webcitation.org/6b56XqsfK)},
year = {2013}
}
@misc{WikimediaFoundation,
author = {{Wikimedia Foundation}},
booktitle = {Microsoft},
title = {{Wikipedia Corpus}},
url = {https://support.microsoft.com/en-us/help/14200/windows-compress-uncompress-zip-files},
urldate = {2017-03-17}
}
@incollection{DeMori1996,
abstract = {This chapter discusses cache-based language model for speech recognition. Automatic Speech Recognition (ASR) consists of two components: an acoustic component, which matches the acoustic input to words in its vocabulary, producing a set of the most plausible word candidates together with a probability for each, and the second component, which incorporates a language model, estimating the probability for each word in the vocabulary that it will occur, given a list of previously hypothesized words. The chapter focuses on the language model incorporated in the second component. A hypothesis is adopted in the chapter that a word used in the recent past is much more likely to be used sooner, from either its overall frequency in the language or that a 3g-gram model would suggest. The cache component of the combined model estimates the probability of a word from its recent frequency of use. The model uses a weighted average of the 3g-gram and cache components in calculating word probabilities, where the relative weights assigned to each component depend on the Part of Speech (POS). For purpose of comparison, a pure 3g-gram model has been created in the chapter, consisting of only the 3g-gram component of the combined model.},
author = {{De Mori}, Renato and Kuhn, Roland},
booktitle = {Recent Research Towards Advanced Man-Machine Interface Through Spoken Language},
doi = {10.1016/B978-044481607-8/50065-7},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/De Mori, Kuhn - 1996 - – A Cache-Based Natural Language Model for Speech Recognition(2).pdf:pdf},
isbn = {9780444816078},
number = {6},
pages = {219--228},
title = {{– A Cache-Based Natural Language Model for Speech Recognition}},
url = {http://visgraph.cs.ust.hk/biometrics/Papers/Voice/pami1990-06-01.pdf},
volume = {12},
year = {1996}
}
@article{Samuel,
abstract = {Two machine-learning procedures have been investigated in some detail usi!Jg the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to playa better game of checkers than can be played by the person who wrote the program. Further-more, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these 'experiments are, of course, applicable to many other situations.},
author = {Samuel, Arthur L},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samuel - Unknown - 4.3.3 Some Studies in Machine Learning Using the Game of Checkers Some Studies in Machine Learning Using the Game of.pdf:pdf},
title = {{4.3.3 Some Studies in Machine Learning Using the Game of Checkers Some Studies in Machine Learning Using the Game of Checkers}},
url = {https://www.cs.virginia.edu/{~}evans/greatworks/samuel1959.pdf}
}
@article{Wandmacher2008,
abstract = {Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4-gram baseline, and most of them to a simple cache model as well.},
archivePrefix = {arXiv},
arxivId = {0801.4716},
author = {Wandmacher, Tonio and Antoine, Jean-Yves},
eprint = {0801.4716},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wandmacher, Antoine - 2008 - Methods to integrate a language model with semantic information for a word prediction component(2).pdf:pdf},
journal = {Computational Linguistics},
number = {June},
pages = {10},
title = {{Methods to integrate a language model with semantic information for a word prediction component}},
url = {http://arxiv.org/abs/0801.4716},
year = {2008}
}
@article{Trnka2006a,
abstract = {Word prediction can be used for enhancing the communication ability of persons with speech and language impairments. In this work, we explore two methods of adapting a language model to the topic of conversation, and apply these methods to the prediction of fringe words.},
author = {Trnka, Keith and Yarrington, Debra and McCoy, Kathleen and Pennington, Christopher},
doi = {10.1145/1111449.1111509},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka et al. - 2006 - Topic modeling in fringe word prediction for AAC(2).pdf:pdf},
isbn = {1595932879},
journal = {Proceedings of the 11th international conference on Intelligent user interfaces - IUI '06},
keywords = {language mod-,topic modeling,word prediction},
pages = {276},
title = {{Topic modeling in fringe word prediction for AAC}},
url = {http://portal.acm.org/citation.cfm?doid=1111449.1111509},
year = {2006}
}
@article{Suster2011,
author = {Suster, Simoň},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Suster - 2011 - Measuring lexical and syntactic complexity of the language production of the World-of-Warcraft gaming community(3).pdf:pdf},
title = {{Measuring lexical and syntactic complexity of the language production of the World-of-Warcraft gaming community}},
year = {2011}
}
@article{Baron,
abstract = {Mobile telephony in the United States is gaining ground against high adoption rates in other parts of the world as a medium for both talking and sending text messages. While there is research on the use of written forms of computer-mediated communication in the US using full keyboards (e.g., chat, email, instant messaging), we know relatively little about mobile telephony as an American form of electronically-mediated communication. To address this lacuna, we administered questionnaires using convenience sampling to American college students on two campuses regarding their use of mobile phones for both talking and texting. The results suggest that the mobile phone platform is still a medium in transition but that some usage patterns may be gender-driven or economically-based, and that others may be distinctive to American culture.},
author = {Baron, Naomi S and {Ling Telenor}, Rich},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baron, Ling Telenor - Unknown - EMERGING PATTERNS OF AMERICAN MOBILE PHONE USE Electronically-mediated communication in transition(2).pdf:pdf},
keywords = {SMS,cell phones,computer-mediated communication,electronically-mediated communication,mobile phones,texting},
title = {{EMERGING PATTERNS OF AMERICAN MOBILE PHONE USE: Electronically-mediated communication in transition}}
}
@article{Ney1994,
abstract = {We study the problem of stochastic language modelling from the viewpoint of introducing suitable structures into the conditional probability distributions. The task of these distributions is to predict the probability of a new word by looking at M or even all predecessor words. The conventional approach is to limit M to 1 or 2 and to interpolate the resulting bigram and trigram models with a unigram model in a linear fashion. However, there are many other structures that can be used to model the probabilistic dependences between the predecessor word and the word to be predicted. The structures considered in this paper are: nonlinear interpolation as an alternative to linear interpolation; equivalence classes for word histories and single words; cache memory and word associations. For the optimal estimation of nonlinear and linear interpolation parameters, the leaving-one-out method is systematically used. For the determination of word equivalence classes in a bigram model, an automatic clustering procedure has been adapted. To capture long-distance dependences, we consider various models for word-by-word dependences; the cache model may be viewed as a special type of self-association. Experimental results are presented for two text databases, a German database and an English database},
author = {Ney, Hermann and Essen, Ute and Kneser, Reinhard},
doi = {10.1006/csla.1994.1001},
issn = {08852308},
journal = {Computer Speech {\&} Language},
month = {jan},
number = {1},
pages = {1--38},
title = {{On Structuring Probabilistic Dependences in Stochastic Language Modelling}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0885230884710011},
volume = {8},
year = {1994}
}
@incollection{Lopes2011,
abstract = {In statistics, the Kolmogorov–Smirnov test (K–S test) is a nonparametric test for the equality of continuous, one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution (one-sample K–S test), or to compare two samples (two-sample K–S test). The Kolmogorov–Smirnov statistic quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution, or between the empirical distribution functions of two samples. The null distribution of this statistic is calculated under the null hypothesis that the samples are drawn from the same distribution (in the two-sample case) or that the sample is drawn from the reference distribution (in the one-sample case). In each case, the distributions considered under the null hypothesis are continuous distributions but are otherwise unrestricted.},
address = {New York, NY},
author = {Lopes, Raul H. C.},
booktitle = {International Encyclopedia of Statistical Science},
doi = {10.1007/978-3-642-04898-2_326},
isbn = {978-1-4020-6753-2},
issn = {03784754},
pages = {718--720},
publisher = {Springer New York},
title = {{Kolmogorov-Smirnov Test}},
url = {http://link.springer.com/10.1007/978-3-642-04898-2{\_}326},
year = {2011}
}
@article{Luyckx,
abstract = {Current advances in shallow parsing and machine learning allow us to use results from these fields in a methodology for Authorship Attribution. We report on experiments with a corpus that consists of newspaper articles about national current affairs by different journalists from the Belgian newspaper De Standaard. Because the documents are in a similar genre, register, and range of topics, token-based (e.g., sentence length) and lexical features (e.g., vocabulary richness) can be kept roughly constant over the different authors. This allows us to focus on the use of syntax-based features as possible predictors for an author's style, as well as on those token-based features that are predictive to author style more than to topic or register. These style characteristics are not under the author's conscious control and therefore good clues for Authorship Attribution. Machine Learning methods (TiMBL and the WEKA software package) are used to select informative combinations of syntactic, token-based and lexical features and to predict authorship of unseen documents. The combination of these features can be considered an implicit profile that characterizes the style of an author.},
author = {Luyckx, Kim and Daelemans, Walter},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luyckx, Daelemans - Unknown - Shallow Text Analysis and Machine Learning for Authorship Attri- bution(2).pdf:pdf},
title = {{Shallow Text Analysis and Machine Learning for Authorship Attri- bution}}
}
@article{Li2005,
abstract = {We propose an integrated approach to interactive word-completion for users with linguistic disabilities in which semantic knowledge combines with n-gram probabilities to predict semantically more-appropriate words than n-gram methods alone. First, semantic relatives are found for English words, specifically for nouns, and they form the semantic knowledge base. The selection process for these semantically related words is first to rank the pointwise mutual information of co-occurring words in a large corpus and then to identify the semantic relatedness of these words by a Lesk-like filter. Then, the semantic knowledge is used to measure the semantic association of completion candidates with the context. Those that are semantically appropriate to the context are promoted to the top positions in prediction lists due to their high association with context. Experimental results show a performance improvement when using the integrated model for the completion of nouns.},
author = {Li, Jianhua and Hirst, Graeme},
doi = {10.1145/1090785.1090809},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Hirst - 2005 - Semantic Knowledge in Word Completion.pdf:pdf},
isbn = {1-59593-159-7},
journal = {Proceedings of the 7th International ACM SIGACCESS Conference on Computers and Accessibility},
keywords = {For-Motor Disability,Full Paper,Tec-Natural Language Processing,Tec-word prediction,assistive technologies,linguistic semantics,pointwise mutual information,word completion},
pages = {121--128},
title = {{Semantic Knowledge in Word Completion}},
url = {http://doi.acm.org/10.1145/1090785.1090809},
year = {2005}
}
@article{Ghosh2017,
abstract = {In logistic regression, separation occurs when a linear combination of the predictors can perfectly classify part or all of the observations in the sample, and as a result, finite maximum likelihood estimates of the regression coefficients do not exist. Gelman et al. (2008) recommended independent Cauchy distributions as default priors for the regression coefficients in logistic regression, even in the case of separation, and reported posterior modes in their analyses. As the mean does not exist for the Cauchy prior, a natural question is whether the posterior means of the regression coefficients exist under separation. We prove theorems that provide necessary and sufficient conditions for the existence of posterior means under independent Cauchy priors for the logit link and a general family of link functions, including the probit link. We also study the existence of posterior means under multivariate Cauchy priors. For full Bayesian inference, we develop a Gibbs sampler based on P{\'{o}}lya-Gamma data augmentation to sample from the posterior distribution under independent Student-t priors including Cauchy priors, and provide a companion R package in the supplement. We demonstrate empirically that even when the posterior means of the regression coefficients exist under separation, the magnitude of the posterior samples for Cauchy priors may be unusually large, and the corresponding Gibbs sampler shows extremely slow mixing. While alternative algorithms such as the No-U-Turn Sampler in Stan can greatly improve mixing, in order to resolve the issue of extremely heavy tailed posteriors for Cauchy priors under separation, one would need to consider lighter tailed priors such as normal priors or Student-t priors with degrees of freedom larger than one.},
archivePrefix = {arXiv},
arxivId = {arXiv:1507.07170v2},
author = {Ghosh, Joyee and Li, Yingbo and Mitra, Robin},
eprint = {arXiv:1507.07170v2},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghosh, Li, Mitra - 2017 - On the Use of Cauchy Prior Distributions for Bayesian Logistic Regression.pdf:pdf},
title = {{On the Use of Cauchy Prior Distributions for Bayesian Logistic Regression}},
url = {https://arxiv.org/pdf/1507.07170.pdf},
year = {2017}
}
@book{Mitchell1997,
abstract = {Mitchell covers the field of machine learning, the study of algorithms that allow computer programs to automatically improve through experience and that automatically infer general laws from specific data. 1. Introduction -- 2. Concept Learning and the General-to-Specific Ordering -- 3. Decision Tree Learning -- 4. Artificial Neural Networks -- 5. Evaluating Hypotheses -- 6. Bayesian Learning -- 7. Computational Learning Theory -- 8. Instance-Based Learning -- 9. Genetic Algorithms -- 10. Learning Sets of Rules -- 11. Analytical Learning -- 12. Combining Inductive and Analytical Learning -- 13. Reinforcement Learning.},
author = {Mitchell, Tom M. (Tom Michael)},
isbn = {0070428077},
pages = {414},
publisher = {McGraw-Hill},
title = {{Machine Learning}},
url = {http://www.cs.cmu.edu/{~}tom/mlbook.html},
year = {1997}
}
@article{Hodges1987,
author = {Hodges, James S.},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hodges - 1987 - Uncertainty, Policy Analysis and Statistics.pdf:pdf},
journal = {Statistical Science},
number = {3},
pages = {259--291},
title = {{Uncertainty, Policy Analysis and Statisitcs}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.ss/1177013224},
volume = {2},
year = {1987}
}
@article{Palmer2000,
author = {Palmer, David D},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Palmer - 2000 - Tokenisation and Sentence Segmentation.pdf:pdf},
journal = {Handbook of Natural Language Processing},
pages = {11--35},
title = {{Tokenisation and Sentence Segmentation}},
year = {2000}
}
@book{Bauer1983,
abstract = {Interest in word-formation is probably as old as interest in language itself. As Dr. Bauer points out in his Introduction, many of the questions that scholars are asking now were also being asked in the seventeenth, eighteenth and nineteenth centuries. However, there is still little agreement on methodology in the study of word-formation or theoretical approaches to it; even the kind of data relevant to its study is open to debate. Dr. Bauer here provides students and general linguists alike with a new perspective on what is a confused and often controversial field of study, providing a resolution to the terminological confusion which currently reigns in this area. In doing so, he clearly demonstrates the challenge and intrinsic fascination of the study of word-formation. Linguists have recently become increasingly aware of the relevance of word-formation to work in syntax and semantics, phonology and morphology, and Dr Bauer discusses - within a largely synchronic and transformational framework - the theoretical issues involved. He considers topics where word-formation has a contribution to make to other areas of linguistics and, without pretending to provide a fully-fledged theory of word-formation, develops those points which he sees as being central to its study. -- Publisher description. 1. Introduction --- 2. Some basic concepts --- 3. Lexicalization --- 4. Productivity --- 5. Phonological issues in word-formation --- 6. Syntactic and semantic issues in word-formation --- 7. An outline of English word-formation --- 8. Theory and practice --- 9. Conclusion.},
address = {Cambridge},
author = {Bauer, Laurie},
isbn = {0521284929},
pages = {311},
publisher = {Cambridge University Press},
title = {{English word-formation}},
year = {1983}
}
@book{Guiraud1954,
address = {Paris},
author = {Guiraud, Pierre},
publisher = {Presses Universitaires de France},
title = {{Les caracteres statistique du vocabulaire}},
year = {1954}
}
@misc{Benoit2016a,
author = {Benoit, Kenneth},
title = {{Quanteda | Quantitative Text Analysis with R}},
url = {https://kbenoit.github.io/quanteda/intro/overview.html},
year = {2016}
}
@article{Duggan2015,
abstract = {Soil functional stability is the capacity of soil functions to resist and recover from an environmental perturbation and can be used to evaluate soil health. It can be influenced by the presence of xenobiotics such as herbicides. The impact of a fresh 2,4-D contamination (36 mg kg-1 dry soil) on soil functional stability was evaluated by comparing the capacity of soil enzyme activities to resist and recover from a heat perturbation for both a clean and 2,4-D-contaminated soil. The functional stabilities of the soils (uniform sands, pH 6.9, 7{\%} (w/w) organic matter) were calculated using the relative soil stability index (RSSI). The RSSI scores indicate the proportion of potential enzyme activity the soil retains after a perturbation compared to the potential activity of an unperturbed soil. Six extra-cellular enzyme activities (acid and alkaline phosphatases, arylsulfatase, urease, protease and ??-glucosidase) were monitored in soil microcosms during a 15-day period. During this period, a 60 ??C heat perturbation was applied to the soil for 24 h. The activities of arylsulfatase and protease were found to be the most stable following heat perturbation obtaining the highest RSSI scores (87{\%} and 77{\%}, respectively). Urease activity showed the lowest RSSI score (38{\%}). Although all enzyme activities were inhibited by the presence of 2,4-D, the RSSI results indicated that contamination lowered the stability of only three enzyme activities (arylsulfatase, ??-glucosidase and urease). The RSSI adequately described resistance, recovery and recovery rate parameters and enabled differentiation between functional stabilities of clean and contaminated soil and between different soil types. ?? 2006 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {Accessed October 9, 2008},
author = {B{\'{e}}caert, Val{\'{e}}rie and Samson, R{\'{e}}jean and Desch{\^{e}}nes, Louise},
doi = {10.1016/j.chemosphere.2006.01.008},
eprint = {Accessed October 9, 2008},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/B{\'{e}}caert, Samson, Desch{\^{e}}nes - 2006 - Effect of 2,4-D contamination on soil functional stability evaluated using the relative soil stab(2).pdf:pdf},
isbn = {1083610110836},
issn = {00456535},
journal = {Chemosphere},
keywords = {Enzyme activity,Functional recovery,Functional resistance,Heat perturbation,Soil health},
number = {10},
pages = {1713--1721},
pmid = {16476467},
title = {{Effect of 2,4-D contamination on soil functional stability evaluated using the relative soil stability index (RSSI)}},
url = {http://www.pewinternet.org/files/2015/08/Social-Media-Update-2015-FINAL2.pdf},
volume = {64},
year = {2006}
}
@article{Zurhellen,
abstract = {This article is one of a series of short essays, collectively titled " Further Explorations, " published as part of a special issue of Oral Tradition in honor of John Miles Foley's 65 th birthday and 2011 retirement. The surprise Festschrift, guest-edited by Lori and Scott Garner entirely without his knowledge, celebrates John's tremendous impact on studies in oral tradition through a series of essays contributed by his students from the University of Missouri-Columbia (1979-present) and from NEH Summer Seminars that he has directed (1987-1996). http://journal.oraltradition.org/issues/26ii In The Pathways Project, John Miles Foley (2011-) discusses briefly the social role of SMS (Short Message Service), suggesting that " even so-called text messaging, a misnomer of sizeable proportions given that the activity really amounts to a long-distance emergent communication enacted virtually, knits people together into interactive groups and keeps them connected and 'present' to one another. " 1 In this essay, I propose a merger of current research on text messaging and the study of oral traditions in order to shed light on the relationship between this new mode of communication and the workings of consciousness being transformed by the eAgora. Focusing first on the limitations of text messaging as a medium that unexpectedly encouraged language innovation, we can explore how text messaging language merges effective communicative practices from both oral and written technologies in order to generate more efficient communication within a newly-limited, writing-based technology. Moreover, in addition to its efficiency, the kind of linguistic play found in text messaging can be viewed as a source of pleasure for those who engage in texting (" texters "). Thus, by employing the discourse of orality and literacy, we can explain how text messaging, while impossible to imagine without the myriad writing technologies mastered before it, actually encourages its literacy-obsessed users to practice communicative techniques more often found within oral cultures, or more precisely, communicative techniques found in cultures in the incipient stages of literacy. Such cultures are ripe for language innovation precisely because they have begun to record knowledge but have not yet standardized the recording procedure. Coincident with a perspective that sees text messaging as bridging a consciousness gap between oral and literate cultures, then, is the recognition that close study of the ways in which text messaging reworks language could lead to fruitful discoveries about the most current ways in which Computer-Mediated Communication (CMC) directs human life toward ever-emerging horizons of consciousness. When David Crystal (2008) hyperbolized the emergence of text messaging in the following passage, this form of communication was already a well-developed medium. Nevertheless, his humorous figuring of text messaging's inception, while not quite accurate, highlights precisely the form's limits that made it such an unlikely competitor in the tightly-wound market of twenty-first-century technologies (173-74): Oral Tradition, 26/2 (2011): 637-624},
annote = {Not primary research},
author = {Zurhellen, Sarah},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zurhellen - Unknown - A Misnomer of Sizeable Proportions SMS and Oral Tradition(2).pdf:pdf},
title = {{" A Misnomer of Sizeable Proportions " : SMS and Oral Tradition}}
}
@misc{matlab,
title = {{Unsupervised Learning - MATLAB {\&} Simulink}},
url = {https://www.mathworks.com/discovery/unsupervised-learning.html},
urldate = {2017-09-08}
}
@article{Witten:2006:ZPE:2263404.2271157,
address = {Piscataway, NJ, USA},
author = {Witten, I H and Bell, T C},
doi = {10.1109/18.87000},
issn = {0018-9448},
journal = {IEEE Trans. Inf. Theor.},
month = {sep},
number = {4},
pages = {1085--1094},
publisher = {IEEE Press},
title = {{The Zero-frequency Problem: Estimating the Probabilities of Novel Events in Adaptive Text Compression}},
url = {http://dx.doi.org/10.1109/18.87000},
volume = {37},
year = {2006}
}
@misc{Rossenstock,
author = {Rosenstock, Jesse},
title = {{NGramQuickTour {\textless} GRM {\textless} TWiki}},
url = {http://openfst.cs.nyu.edu/twiki/bin/view/GRM/NGramQuickTour{\#}NgramCounting},
urldate = {2017-04-03}
}
@article{GloriaCorpasPastor2010,
author = {{Gloria Corpas Pastor} and {Miriam Seghiri}},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gloria Corpas Pastor, Miriam Seghiri - 2010 - Size matters a quantative approach to corpus representativeness.pdf:pdf},
pages = {111--145},
title = {{Size matters: a quantative approach to corpus representativeness}},
volume = {1},
year = {2010}
}
@misc{Google2012,
author = {Google},
title = {{Offensive Words from Google's 'What Do You Love' Project}},
url = {https://gist.github.com/jamiew/1112488},
year = {2012}
}
@article{Fox2009,
author = {Fox, John},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fox - 2009 - Package ‘ effects '.pdf:pdf},
journal = {Source},
title = {{Package ‘ effects '}},
url = {https://cran.r-project.org/web/packages/effects/effects.pdf},
year = {2009}
}
@article{Even-Zohar2000,
abstract = {The eventual goal of a language model is to accurately predict the value of a missing word given its context. We present an approach to word prediction that is based on learning a representation for each word as a function of words and linguistics predicates in its context. This approach raises a few new questions that we address. First, in order to learn good word representations it is necessary to use an expressive representation of the context. We present a way that uses external knowledge to generate expressive context representations, along with a learning method capable of handling the large number of features generated this way that can, potentially, contribute to each prediction. Second, since the number of words ``competing'' for each prediction is large, there is a need to ``focus the attention'' on a smaller subset of these. We exhibit the contribution of a ``focus of attention'' mechanism to the performance of the word predictor. Finally, we describe a large scale experimental study in which the approach presented is shown to yield significant improvements in word prediction tasks.},
archivePrefix = {arXiv},
arxivId = {cs/0009027},
author = {Even-Zohar, Yair and Roth, Dan},
eprint = {0009027},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Even-Zohar, Roth - 2000 - A Classification Approach to Word Prediction.pdf:pdf},
journal = {Proceedings of the First NorthAmerican Conference on Computational Linguistics},
pages = {8},
primaryClass = {cs},
title = {{A Classification Approach to Word Prediction}},
url = {http://delivery.acm.org/10.1145/980000/974322/p124-even-zohar.pdf?ip=72.241.70.167{\&}id=974322{\&}acc=OPEN{\&}key=4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35.6D218144511F3437{\&}CFID=749091672{\&}CFTOKEN=16749922{\&}{\_}{\_}acm{\_}{\_}=1491795393{\_}e6d409cf9452585155b68a8a43eb83},
year = {2000}
}
@article{Chen1996,
abstract = {We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9606011},
author = {Chen, Stanley F and Goodman, Joshua},
doi = {10.3115/981863.981904},
eprint = {9606011},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Goodman - 1996 - An Empirical Study of Smoothing Techniques for Language Modeling(2).pdf:pdf},
isbn = {0885-2308},
issn = {08852308},
journal = {Proceedings of the 34th Annual Meeting on Association for Computational Linguistics},
pages = {310--318},
pmid = {25246403},
primaryClass = {cmp-lg},
title = {{An Empirical Study of Smoothing Techniques for Language Modeling}},
url = {http://www.idealibrary.com http://arxiv.org/abs/cmp-lg/9606011{\%}5Cnhttp://dx.doi.org/10.3115/981863.981904},
volume = {13},
year = {1996}
}
@article{Johnson2017,
author = {Johnson, Lee N. and Miller, Richard B and Bradford, Angela B. and Anderson, Shayne R.},
doi = {10.1111/jmft.12238},
issn = {0194472X},
journal = {Journal of Marital and Family Therapy},
month = {oct},
number = {4},
pages = {561--572},
pmid = {28426921},
title = {{The Marriage and Family Therapy Practice Research Network (MFT-PRN): Creating a More Perfect Union Between Practice and Research}},
url = {http://doi.wiley.com/10.1111/jmft.12238},
volume = {43},
year = {2017}
}
@article{Garay-Vitoria2004a,
abstract = {Prediction is one of the most extended techniques to enhance the rate of communication for people with motor and speech impairments who use Augmentative and Alternative Communication systems. There is an enormous diversity of prediction methods and techniques mentioned in the literature. Therefore, the designer finds tremendous difficulties in understanding and comparing them in order to decide the most convenient technique for a specific design. This paper presents a survey on prediction techniques applied to communicators with the intention of helping them to understand this field. Prediction applications and related features, such as block size, dictionary structure, prediction method, interface, special features, measurement and results, are detailed. Systems found in the literature are studied and described. Finally, a discussion is carried out on the possible comparison among the different methods. {\textcopyright} Springer-Verlag 2004.},
author = {Garay-Vitoria, Nestor and Abascal, Julio},
doi = {10.1007/978-3-540-30111-0_35},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garay-Vitoria, Abascal - 2004 - A comparison of prediction techniques to enhance the communication rate(3).pdf:pdf},
isbn = {978-3-540-23375-6},
issn = {03029743 16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {400--417},
title = {{A comparison of prediction techniques to enhance the communication rate}},
volume = {3196},
year = {2004}
}
@misc{Wiktionary2016,
author = {Wiktionary},
title = {{Appendix:English internet slang - Wiktionary}},
url = {https://en.wiktionary.org/wiki/Appendix:English{\_}internet{\_}slang},
year = {2016}
}
@article{Moore2010,
abstract = {We address the problem of selecting non- domain-specific language model training data to build auxiliary language models for use in tasks such as machine transla- tion. Our approach is based on comparing the cross-entropy, according to domain- specific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.},
author = {Moore, Robert C and Lewis, William},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moore, Lewis - 2010 - Intelligent Selection of Language Model Training Data.pdf:pdf},
isbn = {9781617388088},
journal = {Proceedings of ACL},
number = {July},
pages = {220--224},
title = {{Intelligent Selection of Language Model Training Data}},
year = {2010}
}
@article{Ghayoomia,
abstract = {—The prediction task in national language processing means to guess the missing letter, word, phrase, or sentence that likely follow in a given segment of a text. Since 1980s many systems with different methods were developed for different languages. In this paper an overview of the existing prediction methods that have been used for more than two decades are described and a general classification of the approaches is presented. The three main categories of the classification are statistical modeling, knowledge-based modeling, and heuristic modeling (adaptive).},
author = {Ghayoomi, Masood and Momtazi, Saeedeh},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghayoomi, Momtazi - Unknown - An Overview on the Existing Language Models for Prediction Systems as Writing Assistant Tools(3).pdf:pdf},
keywords = {Assistant technology,Index Terms—Word prediction,Lan-guage modeling},
title = {{An Overview on the Existing Language Models for Prediction Systems as Writing Assistant Tools}},
url = {http://hpsg.fu-berlin.de/{~}ghayoomi/Publishedpapers/ghayoomi-2009[3].pdf}
}
@article{James2000a,
author = {James, Frankie},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/James - 2000 - Modified Kneser-Ney Smoothing of n-gram Models Modified Kneser-Ney Smoothing of n-gram Models(3).pdf:pdf},
journal = {October},
number = {October},
pages = {0--17},
title = {{Modified Kneser-Ney Smoothing of n-gram Models Modified Kneser-Ney Smoothing of n-gram Models}},
year = {2000}
}
@article{Biber2004,
abstract = {Multi-dimensional (MD) analysis is a methodological approach that applies multivariate statistical techniques (especially factor analysis and cluster analysis) to the investigation of register variation in a language. The approach was originally developed to analyze the full range of spoken and written registers in a language. Early studies focused on English register variation (Biber 1985, 1986 and 1988), while later studies have applied the same approach to Somali, Korean, Tuvaluan, Taiwanese, and Spanish. Surprisingly, these studies have found some striking similarities in the underlying 'dimensions' that distinguish among spoken and written registers in these diverse languages. It is even more surprising that MD studies of restricted discourse domains have also uncovered dimensions that are similar in linguistic form and function to the more general studies of register variation. The present study presents an MD analysis of a single register: conversation. Three primary dimensions of variation are identified, and then cluster analysis is used to distinguish among six conversation text types. The dimensions and text types are interpreted in linguistic and functional terms. The author's expectations were that a unique set of dimensions would emerge to characterize the variation among conversational texts. Instead, the three dimensions identified here turn out to be closely related to dimensions identified in previous analyses of general register variation. Taken together with previous studies, the present study of conversation raises the possibility of universal dimensions of variation.},
author = {Biber, Douglas},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Biber - 2004 - Conversation text types A multi-dimensional analysis(2).pdf:pdf},
journal = {JADT},
title = {{Conversation text types: A multi-dimensional analysis}},
year = {2004}
}
@article{Johnson1944,
author = {Johnson, W},
journal = {Psychological Monographs},
pages = {1--15},
title = {{Studies in Language Behaviour: I. {\{}{\{}{\}}A{\{}{\}}{\}} Program of Research}},
volume = {56},
year = {1944}
}
@techreport{BaptisteAuguie2016,
author = {{Baptiste Auguie}, Anton Antonov},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baptiste Auguie - 2016 - R package gridExtra.pdf:pdf},
pages = {1--10},
title = {{R package gridExtra}},
url = {https://cran.r-project.org/web/packages/gridExtra/gridExtra.pdf ftp://cran.r-project.org/pub/R/web/packages/gridExtra/gridExtra.pdf},
year = {2016}
}
@article{Chen1998,
abstract = {We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods.},
archivePrefix = {arXiv},
arxivId = {cmp-lg/9606011},
author = {Chen, Stanley F. and Goodman, Joshua},
doi = {10.3115/981863.981904},
eprint = {9606011},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Goodman - 1996 - An Empirical Study of Smoothing Techniques for Language Modeling(2).pdf:pdf;:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Goodman - 1998 - An Empirical Study of Smoothing Techniques for Language Modeling.pdf:pdf;:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Goodman - 1996 - An Empirical Study of Smoothing Techniques for Language Modeling(3).pdf:pdf},
isbn = {0885-2308},
issn = {08852308},
journal = {Proceedings of the 34th Annual Meeting on Association for Computational Linguistics},
pages = {310--318},
pmid = {25246403},
primaryClass = {cmp-lg},
title = {{An Empirical Study of Smoothing Techniques for Language Modeling}},
url = {https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1 http://arxiv.org/abs/cmp-lg/9606011{\%}5Cnhttp://dx.doi.org/10.3115/981863.981904 http://www.idealibrary.com https://people.eecs.berkeley.edu/{~}klein/cs294-5/chen{\_}goodman.pdf},
volume = {13},
year = {1996}
}
@article{Wandmacher2008a,
abstract = {Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4-gram baseline, and most of them to a simple cache model as well.},
archivePrefix = {arXiv},
arxivId = {0801.4716},
author = {Wandmacher, Tonio and Antoine, Jean-Yves},
eprint = {0801.4716},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wandmacher, Antoine - 2008 - Methods to integrate a language model with semantic information for a word prediction component(2).pdf:pdf},
journal = {Computational Linguistics},
number = {June},
pages = {10},
title = {{Methods to integrate a language model with semantic information for a word prediction component}},
url = {http://arxiv.org/abs/0801.4716},
year = {2008}
}
@book{Hansen2001,
abstract = {Ch. 1. A Little History -- Ch. 2. Foundations of Active Control. 2.1. Physical Mechanisms. 2.2. Basic Structure of Active Noise Control Systems. 2.3. Control System Optimization -- Ch. 3. The Electronic Control System. 3.1. Introduction. 3.2. Digital Filters (Adaptive Control Filters). 3.3. Adaptation Algorithms for Adaptive Filters. 3.4. Waveform Synthesis. 3.5. Important Controller Implementation Issues -- Ch. 4. Active Noise Control Sources. 4.1. Introduction. 4.2. Acoustic Sources. 4.3. Vibration Sources -- Ch. 5. Reference and Error Sensing. 5.1. Microphones. 5.2. Tachometer Reference Signal. 5.3. Sound Intensity. 5.4. Energy Density. 5.5. Virtual Sensing. 5.6. Vibration Sensing of Sound Radiation.},
author = {Hansen, Colin H.},
isbn = {0415233771},
pages = {162},
publisher = {Spon Press},
title = {{Understanding active noise cancellation}},
year = {2001}
}
@article{Whittaker2001,
abstract = {This paper describes two techniques for reducing the size of statistical back-off -gram language models in computer memory. Language model compression is achieved through a combination of quantizing language model probabilities and back-off weights and the pruning of parameters that are determined to be unnecessary after quantization. The recognition performance of the original and compressed language models is eval-uated across three different language models and two different recognition tasks. The results show that the language models can be compressed by up to 60{\%} of their original size with no significant loss in recognition performance. Moreover, the techniques that are described provide a principled method with which to compress language models further while minimising degradation in recognition performance. Abstract This paper describes two techniques for reducing the size of statistical back-off ¤ -gram language models in computer mem-ory. Language model compression is achieved through a combi-nation of quantizing language model probabilities and back-off weights and the pruning of parameters that are determined to be unnecessary after quantization. The recognition performance of the original and compressed language models is evaluated across three different language models and two different recog-nition tasks. The results show that the language models can be compressed by up to 60{\%} of their original size with no signifi-cant loss in recognition performance. Moreover, the techniques that are described provide a principled method with which to compress language models further while minimising degrada-tion in recognition performance.},
author = {Whittaker, EWD and Raj, Bhiksha},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Whittaker, Raj - 2001 - Quantization-based language model compression.pdf:pdf},
isbn = {8790834100},
journal = {Interspeech},
pages = {2--5},
title = {{Quantization-based language model compression.}},
url = {http://www.merl.com https://merl.com/reports/docs/TR2001-41.pdf},
year = {2001}
}
@misc{TylerWRinker2016,
author = {{Tyler W Rinker}},
title = {{qdap: Quantitative Discourse Analysis Package}},
url = {https://trinker.github.io/qdap/ http://github.com/trinker/qdap},
year = {2016}
}
@article{Trnka2008,
abstract = {We present the development and tuning of a topic-adapted language model for word prediction, which improves keystroke savings over a comparable baseline. We outline our plans to develop and integrate style adaptations, building on our experience in topic modeling to dynamically tune the model to both topically and stylistically relevant texts.},
author = {Trnka, Keith},
doi = {10.3115/1564154.1564167},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka - 2008 - Adaptive language modeling for word prediction.pdf:pdf},
journal = {HLT-SRWS '08 Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies},
number = {June},
pages = {61--66},
title = {{Adaptive language modeling for word prediction}},
url = {http://dl.acm.org/citation.cfm?id=1564167},
year = {2008}
}
@article{Reid2004,
abstract = {Abstract: The increasingly widespread use of text -messaging has led to the questioning of the social and psychological effects of this novel communication medium. A selection of findings from an online questionnaire that was developed by the author to answer this pertinent question are presented. McKennas recent work on the way the Internet can help some people develop relationships is drawn upon and taken a step further by exploring the differences between those who prefer texting (Texters) and those who prefer talking on their mobiles (Talkers). A large sample of 982 respondents completed the questionnaire. Results showed there was a clear distinction between Texters and Talkers in the way they used their mobiles and their underlying motivations. The key finding to emerge in the preliminary analyses was that Texters seemed to form close knit text circles with their own social ecology, interconnecting with a close group of friends in perpetual text contact. Compared to Talkers, Texters were found to be more lonely and socially anxious, and more likely to disclose their real-self through text than via face-toface or voice call exchanges. Structural equation modeling produced a model showing that where respondents located their real-self and whether they were a Texter or a Talker mediated between the loneliness and social anxiety measures and the impact of these on relational outcomes, in line with McKennas theoretical framework. Thus it appears that there is something special about texting that allows some people to translate their loneliness and/or social anxiety into productive relationships whilst for others the mobile does not afford the same effect. .Applications and explorations for future research are discussed.},
author = {Reid, Donna and Reid, Fraser},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reid, Reid - 2004 - The social and psychological effects of Text Insights into the Social and Psychological Effects of SMS Text Messa(2).pdf:pdf},
journal = {The Social and Psychological Effects of Text},
number = {February},
pages = {1--11},
title = {{Insights into the Social and Psychological Effects of SMS Text Messaging}},
url = {http://www.160characters.org/documents/SocialEffectsOfTextMessaging.pdf},
year = {2004}
}
@misc{Norvig2015,
author = {Norvig, Peter},
booktitle = {Quora},
title = {{What is the average number of letters for an English word? - Quora}},
url = {https://www.quora.com/What-is-the-average-number-of-letters-for-an-English-word},
year = {2015}
}
@article{Business2011,
author = {Business, Faculty O F},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Business - 2011 - Marketing Strategy for Medical Devices.pdf:pdf},
title = {{Marketing Strategy for Medical Devices}},
year = {2011}
}
@misc{Christensen2016,
author = {Christensen, Hans},
title = {{HC Corpora}},
url = {http://www.corpora.heliohost.org/},
year = {2016}
}
@article{Montemurro2001,
abstract = {In this paper the Zipf-Mandelbrot law is revisited in the context of linguistics. Despite its widespread popularity the Zipf-Mandelbrot law can only describe the statistical behaviour of a rather restricted fraction of the total number of words contained in some given corpus. In particular, we focus our attention on the important deviations that become statistically relevant as larger corpora are considered and that ultimately could be understood as salient features of the underlying complex process of language generation. Finally, it is shown that all the different observed regimes can be accurately encompassed within a single mathematical framework recently introduced by C. Tsallis. ?? 2001 Elsevier Science B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0104066},
author = {Montemurro, Marcelo A},
doi = {10.1016/S0378-4371(01)00355-7},
eprint = {0104066},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Montemurro - 2001 - Beyond the Zipf-Mandelbrot law in quantitative linguistics.pdf:pdf},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
keywords = {Human language,Zipf-Mandelbrot law},
number = {3-4},
pages = {567--578},
pmid = {172092800018},
primaryClass = {cond-mat},
title = {{Beyond the Zipf-Mandelbrot law in quantitative linguistics}},
url = {www.elsevier.com/locate/physa},
volume = {300},
year = {2001}
}
@misc{RCoreTeam2013a,
abstract = {Maintainer: R Core Team {\textless}R-core@r-project.org{\textgreater} Description: R statistical functions License: Part of R 3.1.1 Built: R 3.1.1; x86{\_}64-apple-darwin10.8.0; 2014-07-11 12:31:14 UTC; unix},
author = {{R Core Team}},
doi = {Version: 3.4.0},
isbn = {3-900051-07-0},
keywords = {anova.glm,anova.lm,confint,glm,lm},
title = {{The R Stats Package}},
url = {https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html},
year = {2013}
}
@misc{WHO2012,
abstract = {WHO fact sheet on depression providing key facts and information on types and symptoms, contributing factors, diagnosis and treatment, WHO response.},
author = {WHO},
booktitle = {World Health Organization},
doi = {/entity/mediacentre/factsheets/fs369/en/index.html},
keywords = {Fact sheet [doctype],Region of the Americas [region],United States of America [country],chronic diseases,communicable disease [subject],depression [subject],depressive symptoms,infectious diseases,mental disorder [subject],mental health [subject],mental health promotion,mental illness,noncommunicable disease [subject],psychiatric illness,suicide [subject],suicide prevention},
publisher = {World Health Organization},
title = {{Fact sheet 369: Depression}},
url = {http://www.who.int/mediacentre/factsheets/fs369/en/ http://www.who.int/mediacentre/factsheets/fs369/en/index.html},
urldate = {2017-06-13},
year = {2012}
}
@article{Trnka2007a,
abstract = {Word prediction can be used to enhance the communication rate of people with disabilities who use Augmentative and Alternative Communication (AAC) devices. We use statistical methods in a word prediction system, which are trained on a corpus, and then measure the efficacy of the resulting system by calculating the theoretical keystroke savings on some held out data. Ideally training and testing should be done on a large corpus of AAC text covering a variety of topics, but no such corpus exists. We discuss training and testing on a wide variety of corpora meant to approximate text from AAC users. We show that training on a combination of in-domain data with out-of-domain data is often more beneficial than either data set alone and that advanced language modeling such as topic modeling is portable even when applied to very different text.},
author = {Trnka, Keith and McCoy, Kathleen F.},
doi = {10.1145/1296843.1296877},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Trnka, McCoy - 2007 - Corpus studies in word prediction(2).pdf:pdf},
isbn = {9781595935731},
journal = {Proceedings of the 9th international ACM SIGACCESS conference on Computers and accessibility - Assets '07},
keywords = {cor-,language modeling,statistical methods,word prediction},
pages = {195},
title = {{Corpus studies in word prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1296843.1296877},
year = {2007}
}
@inproceedings{Akaike1998,
address = {Budapest},
author = {Akaike, H},
booktitle = {Second International Symposium on Information Theory},
editor = {Petrov, B N and Csaki, F},
keywords = {modelselection},
pages = {267--281},
publisher = {Akad{\'{e}}miai Kiado},
title = {{Information theory and an extension of the maximum likelihood principle}},
year = {1973}
}
@article{Heafield2011,
abstract = {We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs. The PROBING data structure uses linear probing hash tables and is de signed for speed. Compared with the widely-used SRILM, our PROBING model is 2.4 times as fast while using 57{\%} of the memory. The TRIE data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption. TRIE simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline. Our code is open-source, thread-safe, and integrated into the Moses, cdec, and Joshua translation systems. This paper describes the several performance techniques used and presents benchmarks against alternative implementations.},
author = {Heafield, Kenneth},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heafield - 2011 - KenLM Faster and Smaller Language Model Queries.pdf:pdf},
isbn = {978-1-937284-12-1},
journal = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
number = {2009},
pages = {187--197},
title = {{KenLM : Faster and Smaller Language Model Queries}},
url = {http://www.kheafield.com/professional/avenue/kenlm.pdf http://www.aclweb.org/anthology/W11-2123{\%}5Cnhttp://kheafield.com/code/kenlm},
year = {2011}
}
@book{Guiraud1954a,
author = {Guiraud, Pierre},
pages = {116},
title = {{Les caract{\`{e}}res statistiques du vocabulaire}},
year = {1954}
}
@article{Drucker,
author = {Drucker, Zach},
journal = {Tufts Daily},
title = {{Trailers tease Hollywood's upcoming blockbusters and Oscar-season favorites}},
url = {http://www.tuftsdaily.com/trailers-tease-hollywood-s-upcoming-blockbusters-and-oscar-season-favorites-1.2383918}
}
@article{Neunerdt2013a,
abstract = {—Using social media tools such as blogs and forums have become more and more popular in recent years. Hence, a huge collection of social media texts from different communities is available for accessing user opinions, e.g., for marketing studies or acceptance research. Typically, methods from Natural Language Processing are applied to social media texts to automatically recognize user opinions. A fundamental component of the linguistic pipeline in Natural Language Processing is Part-of-Speech tagging. Most state-of-the-art Part-of-Speech taggers are trained on newspaper corpora, which differ in many ways from non-standardized social media text. Hence, applying common taggers to such texts results in performance degradation. In this paper, we present extensions to a basic Markov model tagger for the annotation of social media texts. Considering the German standard Stuttgart/ ubinger TagSet (STTS), we distinguish 54 tag classes. Applying our approach improves the tagging accuracy for social media texts considerably, when we train our model on a combination of annotated texts from newspapers and Web comments.},
author = {Neunerdt, Melanie and Reyer, Michael and Mathar, Rudolf},
file = {:C$\backslash$:/Users/John/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Neunerdt, Reyer, Mathar - 2013 - A POS Tagger for Social Media Texts trained on web comments.pdf:pdf},
journal = {Polibits},
keywords = {German,Index Terms—Natural language processing,opinion mining,part-of-speech tagging},
number = {48},
pages = {61--68},
title = {{A POS Tagger for Social Media Texts trained on web comments}},
url = {http://polibits.ojs.gelbukh.com/ojs/index.php/polibits/article/viewFile/1783/1723},
year = {2013}
}
